[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning with Hyperparameter Tuning",
    "section": "",
    "text": "About\nThis book offers a practical, hands-on guide to applying a wide range of machine learning algorithms using the R caret package, with a strong emphasis on hyperparameter tuning and model optimization.\nRather than simply listing algorithms, we focus on how to effectively implement, tune, and evaluate models in a systematic way using a unified workflow. Covering both supervised and unsupervised learning methods, this book walks readers through:\n\nThe theoretical foundations and practical motivations behind each algorithm\n\nModel training and resampling strategies using caret\n\nHyperparameter tuning with grid search and cross-validation\n\nPerformance evaluation and comparison across models\n\nFully reproducible code examples using real-world datasets\n\nWe explore essential machine learning techniques such as K-Nearest Neighbors, Decision Trees, Support Vector Machines, LASSO and Ridge Regression, Elastic Net, Random Forests, Gradient Boosting, AdaBoost, and XGBoost, all within a consistent and structured modeling framework.\nThis book is designed for students, researchers, and data practitioners who want to develop a solid understanding of machine learning workflows in R, especially those who aim to go beyond basic modeling and gain insight into how hyperparameter choices influence model performance.\nBy the end of this book, readers will be able to confidently build, tune, and evaluate predictive models using best practices in machine learning with R.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "",
    "text": "1.1 데이터 불러오기\npacman::p_load(\"data.table\", \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                 # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                  # 사용할 Core 개수 지정                                  \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                       # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#데이터-전처리-i",
    "href": "KNN.html#데이터-전처리-i",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.2 데이터 전처리 I",
    "text": "1.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\n# 4. Convert One-hot Encoding for 범주형 예측 변수\ndummies &lt;- dummyVars(formula = ~ .,                                     # formula : ~ 예측 변수 / \".\" : data에 포함된 모든 변수를 의미\n                     data = titanic1[,-1],                              # Dataset including Only 예측 변수 -&gt; Target 제외\n                     fullRank = FALSE)                                  # fullRank = TRUE : Dummy Variable, fullRank = FALSE : One-hot Encoding\n\ntitanic.Var   &lt;- predict(dummies, newdata = titanic1) %&gt;%               # 범주형 예측 변수에 대한 One-hot Encoding 변환\n  data.frame()                                                          # Data Frame 형태로 변환 \n\nglimpse(titanic.Var)                                                    # 데이터 구조 확인\n\nRows: 891\nColumns: 8\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…\n\n# Combine Target with 변환된 예측 변수\ntitanic.df &lt;- data.frame(Survived = titanic1$Survived, \n                         titanic.Var)\n\ntitanic.df %&gt;%\n  as_tibble\n\n# A tibble: 891 × 9\n   Survived Pclass.1 Pclass.2 Pclass.3 Sex.female Sex.male   Age  Fare FamSize\n   &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 no              0        0        1          0        1    22  7.25       1\n 2 yes             1        0        0          1        0    38 71.3        1\n 3 yes             0        0        1          1        0    26  7.92       0\n 4 yes             1        0        0          1        0    35 53.1        1\n 5 no              0        0        1          0        1    35  8.05       0\n 6 no              0        0        1          0        1    NA  8.46       0\n 7 no              1        0        0          0        1    54 51.9        0\n 8 no              0        0        1          0        1     2 21.1        4\n 9 yes             0        0        1          1        0    27 11.1        2\n10 yes             0        1        0          1        0    14 30.1        1\n# ℹ 881 more rows\n\nglimpse(titanic.df)                                                     # 데이터 구조 확인\n\nRows: 891\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, …\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#데이터-탐색",
    "href": "KNN.html#데이터-탐색",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.3 데이터 탐색",
    "text": "1.3 데이터 탐색\n\nggpairs(titanic.df,                                        \n        aes(colour = Survived)) +                     # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic.df,                                     \n        aes(colour = Survived, alpha = 0.8)) +        # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"purple\",\"cyan4\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"purple\",\"cyan4\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#데이터-분할",
    "href": "KNN.html#데이터-분할",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.4 데이터 분할",
    "text": "1.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic.df$Survived                         # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic.df[ind$Resample1,]             # Training Dataset\ntitanic.ted &lt;- titanic.df[-ind$Resample1,]            # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#데이터-전처리-ii",
    "href": "KNN.html#데이터-전처리-ii",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.5 데이터 전처리 II",
    "text": "1.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n  \nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes,…\n$ Pclass.1   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Pclass.3   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ Age        &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29…\n$ Fare       &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.879…\n$ FamSize    &lt;dbl&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 9\n$ Survived   &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no,…\n$ Pclass.1   &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Pclass.3   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,…\n$ Sex.female &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n$ Age        &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29…\n$ Fare       &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250…\n$ FamSize    &lt;dbl&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#모형-훈련",
    "href": "KNN.html#모형-훈련",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.6 모형 훈련",
    "text": "1.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 k의 최적값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5, # 5-Fold Cross Validation (5-Fold CV)\n                            allowParallel = TRUE)     # 병렬 처리\n             \nset.seed(200)                                         # For CV\nknn.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                 trControl = fitControl ,\n                 method = \"knn\", \n                 preProc = c(\"center\", \"scale\"))      # Standardization for 예측 변수\nknn.fit\n\nk-Nearest Neighbors \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  k  Accuracy  Kappa    \n  5  0.7888    0.5466275\n  7  0.8032    0.5740530\n  9  0.7808    0.5231040\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 7.\n\nplot(knn.fit)                                         # Plot\n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 3개의 k 값에 대한 정확도를 보여주며, k = 7일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 값 7 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(k = seq(5, 10, by = 1))     # k의 탐색 범위 \n\nset.seed(200)                                         # For CV\nknn.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                      trControl = fitControl,\n                      method = \"knn\", \n                      tuneGrid = customGrid,\n                      preProc = c(\"center\", \"scale\")) # Standardization for 예측 변수\n\nknn.tune.fit\n\nk-Nearest Neighbors \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  k   Accuracy  Kappa    \n   5  0.7888    0.5466275\n   6  0.7856    0.5402485\n   7  0.8032    0.5740530\n   8  0.7776    0.5171630\n   9  0.7792    0.5208682\n  10  0.7792    0.5216151\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 7.\n\nplot(knn.tune.fit)                                    # Plot\n\n\n\n\n\n\n\nknn.tune.fit$bestTune                                 # k의 최적값\n\n  k\n3 7\n\n\nResult! k = 7일 때 정확도가 가장 높다는 것을 알 수 있으며, k = 7을 가지는 모형을 최적의 훈련된 모형으로 선택한다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "KNN.html#모형-평가",
    "href": "KNN.html#모형-평가",
    "title": "1  Nearest Neighborhood Algorithm",
    "section": "1.7 모형 평가",
    "text": "1.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\nknn.pred &lt;- predict(knn.tune.fit,                                        \n                    newdata = titanic.ted.Imp[,-1])   # Test Dataset including Only 예측 변수   \n\nknn.pred %&gt;%\n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 no   \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n1.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(knn.pred, titanic.ted.Imp$Survived, \n                               positive = \"yes\")    # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  154  34\n       yes  10  68\n                                          \n               Accuracy : 0.8346          \n                 95% CI : (0.7844, 0.8772)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 7.118e-15       \n                                          \n                  Kappa : 0.6339          \n                                          \n Mcnemar's Test P-Value : 0.0005256       \n                                          \n            Sensitivity : 0.6667          \n            Specificity : 0.9390          \n         Pos Pred Value : 0.8718          \n         Neg Pred Value : 0.8191          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2556          \n   Detection Prevalence : 0.2932          \n      Balanced Accuracy : 0.8028          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n1.7.2 ROC 곡선\n\n# 예측 확률 생성 \ntest.knn.prob &lt;- predict(knn.tune.fit, \n                         newdata = titanic.ted.Imp[,-1],    # Test Dataset including Only 예측 변수   \n                         type = \"prob\")                     # 예측 확률 생성 \n\ntest.knn.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n      no   yes\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1    \n 2 0.714 0.286\n 3 0.857 0.143\n 4 0.286 0.714\n 5 0.857 0.143\n 6 1     0    \n 7 0.571 0.429\n 8 1     0    \n 9 1     0    \n10 0     1    \n# ℹ 256 more rows\n\n\n\ntest.knn.prob &lt;- test.knn.prob[,2]                 # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                    # Test Dataset의 실제 class   \npp  &lt;- as.numeric(test.knn.prob)                   # 예측 확률을 수치형으로 변환\n\n\n1.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nknn.roc  &lt;- roc(ac, pp, plot=T, col=\"gray\")        # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(knn.roc),3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(knn.roc,   \n         col=\"gray\",                               # Line Color\n         print.auc = TRUE,                         # AUC 출력 여부\n         print.auc.col = \"red\",                    # AUC 글씨 색깔\n         print.thres = TRUE,                       # Cutoff Value 출력 여부\n         print.thres.pch = 19,                     # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                  # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                       # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")               # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(knn.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n1.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                           # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n1.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nknn.pred &lt;- prediction(pp, ac)                      # prediction(예측 확률, 실제 class)    \n\nknn.perf &lt;- performance(knn.pred, \"tpr\", \"fpr\")     # performance(, \"민감도\", \"1-특이도\")                      \nplot(knn.perf, col = \"gray\")                        # ROC Curve\n\nperf.auc   &lt;- performance(knn.pred, \"auc\")          # AUC\nauc        &lt;- attributes(perf.auc)$y.values \nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.3 향상 차트\n\n1.7.3.1 Package “ROCR”\n\nknn.perf &lt;- performance(knn.pred, \"lift\", \"rpp\")    # Lift Chart\nplot(knn.perf, main = \"lift curve\", \n     colorize = T,                                  # Coloring according to cutoff\n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nearest Neighborhood Algorithm</span>"
    ]
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "2  Cluster Analysis based on k-means",
    "section": "",
    "text": "2.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\",\n               \"caret\",\n               \"GGally\",                       # For ggpairs\n               \"mlr\",\n               \"clue\",\n               \"parallelMap\",                  # For parallelStartSocket\n               \"parallel\")                     # For detectCores\n\ndata(\"USArrests\")                              # 데이터 불러오기\n\nUSArrests %&gt;%\n  as_tibble\n\n# A tibble: 50 × 4\n   Murder Assault UrbanPop  Rape\n    &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;\n 1   13.2     236       58  21.2\n 2   10       263       48  44.5\n 3    8.1     294       80  31  \n 4    8.8     190       50  19.5\n 5    9       276       91  40.6\n 6    7.9     204       78  38.7\n 7    3.3     110       77  11.1\n 8    5.9     238       72  15.8\n 9   15.4     335       80  31.9\n10   17.4     211       60  25.8\n# ℹ 40 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Clustering.html#데이터-탐색",
    "href": "Clustering.html#데이터-탐색",
    "title": "2  Cluster Analysis based on k-means",
    "section": "2.2 데이터 탐색",
    "text": "2.2 데이터 탐색\n\nggpairs(USArrests,\n        upper = list(continuous = \"density\"),\n        lower = list(continuous = wrap(\"points\", size = 0.5)),\n        diag = list(continuous = \"densityDiag\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# 상관계수 그래프\nggcorr(USArrests,               # 데이터\n       label = TRUE,            # 라벨 명시 여부\n       label_round = 3,         # 상관계수 소숫점 이하 자릿수\n       label_size = 3,          # 상관계수 글자 크기\n       low = \"steelblue\",       # 상관계수가 음수일 때 색깔\n       mid = \"white\",           # 상관계수가 0에 가까울 때 색깔\n       high = \"darkred\")        # 상관계수가 양수일 때 색깔",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Clustering.html#데이터-분할",
    "href": "Clustering.html#데이터-분할",
    "title": "2  Cluster Analysis based on k-means",
    "section": "2.3 데이터 분할",
    "text": "2.3 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 8:2)\nset.seed(200)\nind &lt;- sample(1:nrow(USArrests), 0.8*nrow(USArrests))       # Index를 이용하여 8:2로 분할\n\nUSArrests.trd &lt;- USArrests[ind,]                            # Training Dataset\nUSArrests.ted &lt;- USArrests[-ind,]                           # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Clustering.html#데이터-전처리",
    "href": "Clustering.html#데이터-전처리",
    "title": "2  Cluster Analysis based on k-means",
    "section": "2.4 데이터 전처리",
    "text": "2.4 데이터 전처리\n\n# Standardization\npreProcValues &lt;- preProcess(USArrests.trd,\n                            method = c(\"center\", \"scale\"))  # Standardization 정의 -&gt; Training Dataset에 대한 평균과 표준편차 계산 \n\nUSArrests.trd &lt;- predict(preProcValues, USArrests.trd)      # Standardization for Training Dataset\nUSArrests.ted &lt;- predict(preProcValues, USArrests.ted)      # Standardization for Test Dataset\n\nglimpse(USArrests.trd)                                      # 데이터 구조 확인\n\nRows: 40\nColumns: 4\n$ Murder   &lt;dbl&gt; -0.28335915, -0.82987994, 1.64134450, -1.13878299, 1.11858549, -1.01997413, -0.35464447, 1.87896224, -0.75859462, 1.35620322, 2.04529465, 0.90472953, -0.87740349, -0.37840624, -0.21…\n$ Assault  &lt;dbl&gt; -0.66262772, -0.20320584, 1.37532065, -1.06314937, 1.05725935, -0.49770705, -0.62728758, 1.02191920, -0.70974792, 0.30333625, 1.13971969, 1.62270167, -0.89822869, 0.89233867, -0.132…\n$ UrbanPop &lt;dbl&gt; 0.56553882, 0.63579209, -1.12053964, 0.14401920, 1.19781824, 1.12756497, -0.76927330, 0.14401920, -0.13699387, -0.34775368, -1.40155272, 0.21427247, -1.33129945, 0.56553882, 0.28452…\n$ Rape     &lt;dbl&gt; -0.62276898, 0.61022086, 0.20649852, -0.62276898, 2.77068093, 0.25014418, -0.45909777, 0.17376428, -0.44818635, 0.68660076, -0.38271786, 0.78480349, -0.85190869, -0.52456625, -0.066…\n\nglimpse(USArrests.ted)                                      # 데이터 구조 확인\n\nRows: 10\nColumns: 4\n$ Murder   &lt;dbl&gt; 0.5958265, 1.8789622, 2.3541977, -1.1625448, 0.6908736, -0.3546445, -0.7348328, 0.3582087, 0.8572060, -0.9724506\n$ Assault  &lt;dbl&gt; 1.1868399, 2.0350034, 0.5742774, -0.4977070, 1.0219192, -0.5566073, -0.1560856, 0.1855358, 1.0808194, 0.1384156\n$ UrbanPop &lt;dbl&gt; -1.1205396, 1.1275650, -0.2775004, -0.6990200, 1.3383248, 0.1440192, 1.4788313, 0.4250323, 1.5490846, 1.6193379\n$ Rape     &lt;dbl&gt; 2.6070097, 1.2321715, 0.5665752, -0.6991489, 0.3701697, -0.2845151, -0.4700092, 0.8284491, 0.5993094, -1.3429223",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Clustering.html#모형-훈련",
    "href": "Clustering.html#모형-훈련",
    "title": "2  Cluster Analysis based on k-means",
    "section": "2.5 모형 훈련",
    "text": "2.5 모형 훈련\n\n\n\n\n\n2.5.1 Resampling을 이용한 최적의 초모수 조합 찾기\n\n2.5.1.1 Define Task\n\n문제 유형에 따라 Task를 정의하는 데 사용하는 함수는 다음과 같다.\n\n\n\n\n문제 유형\n함수\n\n\n회귀 문제\nmakeRegrTask()\n\n\n이진 또는 다중 클래스 분류 문제\nmakeClassifTask()\n\n\n생존분석\nmakeSurvTask()\n\n\n군집분석\nmakeClusterTask()\n\n\n다중 라벨 분류 문제\nmakeMultilabelTask()\n\n\n비용 민감 분류 문제\nmakeCostSensTask()\n\n\n\n\n\n\n\n\n## k-means : 군집분석\nUSArrests.Task &lt;- makeClusterTask(data = USArrests.trd)   # Training Dataset\nUSArrests.Task\n\nUnsupervised task: USArrests.trd\nType: cluster\nObservations: 40\nFeatures:\n   numerics     factors     ordered functionals \n          4           0           0           0 \nMissings: FALSE\nHas weights: FALSE\nHas blocking: FALSE\nHas coordinates: FALSE\n\n\n\n\n2.5.1.2 Define Learner\n\n학습자(Learner)는 함수 makeLearner()를 이용하여 정의한다.\n함수 makeLearner()의 첫 번째 인자 cl에는 사용하고자 하는 머신러닝 알고리듬을 문제 유형.알고리듬과 관련된 R 함수 이름 형태로 입력한다.\n\n예를 들어,\n\n회귀 문제 : \"regr.알고리듬과 관련된 R 함수 이름\"\n클래스 분류 문제 : \"classif.알고리듬과 관련된 R 함수 이름\"\n생존분석 : \"surv.알고리듬과 관련된 R 함수 이름\"\n군집분석 : \"cluster.알고리듬과 관련된 R 함수 이름\"\n다중 라벨 문제 : \"multilabel.알고리듬과 관련된 R 함수 이름\"\n\nPackage \"mlr\"에서 사용할 수 있는 알고리듬은 여기를 통해서 확인할 수 있다.\n\n\n\n# 초모수 집합\ngetParamSet(\"cluster.kmeans\")  # cluster.kmeans : Clustering based on \"kmeans\" function\n\n              Type len           Def                             Constr Req Tunable Trafo\ncenters    untyped   -             -                                  -   -    TRUE     -\niter.max   integer   -            10                           1 to Inf   -    TRUE     -\nnstart     integer   -             1                           1 to Inf   -    TRUE     -\nalgorithm discrete   - Hartigan-Wong Hartigan-Wong,Lloyd,Forgy,MacQueen   -    TRUE     -\ntrace      logical   -             -                                  -   -   FALSE     -\n\n\nCaution! 특정 머신러닝 알고리듬이 가지고 있는 초모수는 함수 getParamSet()를 이용하여 확인할 수 있다.\n\n\nUSArrests.Learner &lt;- makeLearner(cl = \"cluster.kmeans\",                # 함수 kmeans를 이용하여 군집분석 수행\n                                 par.vals = list(iter.max = 100,       # 최대 반복 수\n                                                 nstart = 25))         # 수행 횟수\nUSArrests.Learner\n\nLearner cluster.kmeans from package stats,clue\nType: cluster\nName: K-Means; Short name: kmeans\nClass: cluster.kmeans\nProperties: numerics,prob\nPredict-Type: response\nHyperparameters: centers=2,iter.max=100,nstart=25\n\n\nCaution! 함수 makeLearner()의 인자 par.vals에 함수 kmeans()의 옵션을 입력할 수 있다. iter.max에는 최대 반복 수를, nstart에는 k-means 수행 횟수를 지정할 수 있다. k-means는 초기 중심값을 랜덤하게 선택하기 때문에 이 과정에서 다양한 결과가 나타날 수 있다. 그래서 옵션 nstart를 이용하여 수행 횟수를 늘려 최대한 다양한 초기 중심값에 대해 k-means를 수행하고 최적의 결과를 찾을 수 있다.\n\n\n2.5.1.3 Define Search Space\n\n함수 makeDiscreteParam() 또는 makeNumericParam()을 이용하여 초모수의 검색 범위를 정의한다.\n\n함수 makeDiscreteParam() : 검색 범위를 특정값으로 정의하는 경우 사용\n함수 makeNumericParam() : 검색 범위를 구간으로 정의하는 경우 사용\n\n그러고나서, 함수 makeParamSet()를 이용하여 정의한 검색 범위을 ParamSet 객체로 만든다.\n\n\n# 초모수 \"centers\" (군집 수)와 알고리듬의 검색 범위 정의 \ntune.hyper &lt;- makeParamSet( \n  makeDiscreteParam(\"centers\",                                         # 군집 수 \n                    values = 3:7),                                     # 군집 수에 대한 검색 범위\n  makeDiscreteParam(\"algorithm\",                                       # k-means 알고리듬\n                    values = c(\"Lloyd\", \"MacQueen\", \"Hartigan-Wong\"))) # 알고리듬에 대한 검색 범위\ntune.hyper\n\n              Type len Def                       Constr Req Tunable Trafo\ncenters   discrete   -   -                    3,4,5,6,7   -    TRUE     -\nalgorithm discrete   -   - Lloyd,MacQueen,Hartigan-Wong   -    TRUE     -\n\n\nCaution! k-means를 수행하기 위해 “Lloyd”, “MacQueen” 그리고 “Hartigan-Wong” 알고리듬을 검색 범위로 정의하였다. 각 알고리듬의 수행 절차는 다음과 같다.\n\n\n\n\n\n2.5.1.4 Define Tuning Method\n\n정의한 검색 범위 내에서 후보 초모수 조합을 찾는다.\n가장 많이 사용하는 방법은 그리드 검색(Grid Search)과 랜덤 검색(Random Search)이며, 각각 함수 makeTuneControlGrid()와 makeTuneControlRandom()을 이용하여 정의할 수 있다.\n\n\ngridSearch &lt;- makeTuneControlGrid()                                    # 그리드 검색               \ngridSearch\n\nTune control: TuneControlGrid\nSame resampling instance: TRUE\nImputation value: &lt;worst&gt;\nStart: &lt;NULL&gt;\n\nTune threshold: FALSE\nFurther arguments: resolution=10\n\n\nResult! 함수 makeTuneControlGrid()를 이용하여 위에서 정의한 초모수 “centers”와 알고리듬의 검색 범위에 해당하는 모든 조합을 후보 초모수 조합으로 설정한다.\n\n\n2.5.1.5 Define Resampling Strategy\n\nkFold &lt;- makeResampleDesc(\"CV\", iters = 5)                             # 5-Fold Cross Validation\nkFold\n\nResample description: cross-validation with 5 iterations.\nPredict: test\nStratification: FALSE\n\n\n\n\n2.5.1.6 Perform Tuning\n\n최적의 초모수 조합을 찾기 위해 함수 tuneParams()를 이용한다.\n\n\nparallelStartSocket(cpus = detectCores())                              # 병렬 처리\n\nset.seed(100)\ntunedK &lt;- tuneParams(task = USArrests.Task,                            # Defined Task in 5-1-1\n                     learner = USArrests.Learner,                      # Defined Learner in 5-1-2\n                     par.set = tune.hyper,                             # Defined Search Space in 5-1-3\n                     control = gridSearch,                             # Defined Tuning Method in 5-1-4\n                     resampling = kFold,                               # Defined Resampling Strategy in 5-1-5\n                     measures = list(db))                              # Davies-Bouldin Index (군집내 유사성이 높고 군집간 유사성이 낮을수록 값이 낮음)\n\ntunedK\n\nTune result:\nOp. pars: centers=7; algorithm=Hartigan-Wong\ndb.test.mean=0.5285825\n\n\nResult! 최적의 초모수 조합을 찾기 위해 “Silhouette Index”를 사용하였다. “Silhouette Index”는 다른 군집과 비교하여 case가 현재 속한 군집과 얼마나 유사한지를 나타내는 척도로 값이 높을수록 군집화가 잘 되었다는 것을 의미한다.\n\ntunedK$x$centers                                                       # 최적의 군집 수\n\n[1] 7\n\ntunedK$x$algorithm                                                     # 최적의 알고리듬\n\n[1] \"Hartigan-Wong\"\n\n\n\n# 튜닝 과정 시각화\nkMeansTuningData &lt;- generateHyperParsEffectData(tunedK)                # Extract Hyperparameter Effect from Tuning Result\nkMeansTuningData$data\n\n   centers     algorithm db.test.mean iteration exec.time\n1        3         Lloyd    0.7914754         1      0.78\n2        4         Lloyd    0.7204947         2      0.78\n3        5         Lloyd    0.7140210         3      0.89\n4        6         Lloyd    0.6604494         4      0.67\n5        7         Lloyd    0.5892118         5      0.62\n6        3      MacQueen    0.7662724         6      0.57\n7        4      MacQueen    0.7349811         7      0.07\n8        5      MacQueen    0.7286601         8      0.10\n9        6      MacQueen    0.7524750         9      0.14\n10       7      MacQueen    0.5848960        10      0.11\n11       3 Hartigan-Wong    0.7662724        11      0.13\n12       4 Hartigan-Wong    0.7349811        12      0.08\n13       5 Hartigan-Wong    0.7267509        13      0.26\n14       6 Hartigan-Wong    0.7990965        14      0.17\n15       7 Hartigan-Wong    0.5285825        15      0.18\n\n# 데이터 구조 변환\nTuningData &lt;- pivot_longer(kMeansTuningData$data,\n                           cols = -c(centers, iteration, algorithm, exec.time),\n                           names_to = \"Metric\",\n                           values_to = \"Value\")\n\nTuningData %&gt;%\n  as_tibble\n\n# A tibble: 15 × 6\n   centers algorithm     iteration exec.time Metric       Value\n     &lt;int&gt; &lt;chr&gt;             &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1       3 Lloyd                 1    0.78   db.test.mean 0.791\n 2       4 Lloyd                 2    0.78   db.test.mean 0.720\n 3       5 Lloyd                 3    0.89   db.test.mean 0.714\n 4       6 Lloyd                 4    0.67   db.test.mean 0.660\n 5       7 Lloyd                 5    0.62   db.test.mean 0.589\n 6       3 MacQueen              6    0.57   db.test.mean 0.766\n 7       4 MacQueen              7    0.0700 db.test.mean 0.735\n 8       5 MacQueen              8    0.100  db.test.mean 0.729\n 9       6 MacQueen              9    0.140  db.test.mean 0.752\n10       7 MacQueen             10    0.110  db.test.mean 0.585\n11       3 Hartigan-Wong        11    0.130  db.test.mean 0.766\n12       4 Hartigan-Wong        12    0.0800 db.test.mean 0.735\n13       5 Hartigan-Wong        13    0.260  db.test.mean 0.727\n14       6 Hartigan-Wong        14    0.17   db.test.mean 0.799\n15       7 Hartigan-Wong        15    0.180  db.test.mean 0.529\n\nggplot(TuningData, aes(centers, Value, col = algorithm)) +\n  facet_wrap(~ Metric, scales = \"free_y\") +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nResult! 초모수 “centers = 4”이고 알고리듬이 “Lloyd”일 때 “Silhouette Index”값이 가장 높다는 것을 알 수 있다.\n\n\n\n2.5.2 최적의 초모수 조합과 함께 모형 훈련\n\n2.5.2.1 Redefine Learner\n\n# Redefine Learner with 최적의 초모수 조합\ntunedKMeans &lt;- setHyperPars(USArrests.Learner,                               # Defined Learner in 5-1-2\n                            par.vals = list(centers = tunedK$x$centers,      # 최적의 군집 수\n                                            algorithm = tunedK$x$algorithm)) # 최적의 알고리듬\ntunedKMeans\n\nLearner cluster.kmeans from package stats,clue\nType: cluster\nName: K-Means; Short name: kmeans\nClass: cluster.kmeans\nProperties: numerics,prob\nPredict-Type: response\nHyperparameters: centers=7,iter.max=100,nstart=25,algorithm=Hartigan-Wong\n\n\n\n\n2.5.2.2 Train Model\n\ntunedKMeansModel &lt;- train(tunedKMeans,                                       # Defined Learner in 5-2-1\n                          USArrests.Task)                                    # Defined Task in 5-1-1\ntunedKMeansModel\n\nModel for learner.id=cluster.kmeans; learner.class=cluster.kmeans\nTrained on: task.id = USArrests.trd; obs = 40; features = 4\nHyperparameters: centers=7,iter.max=100,nstart=25,algorithm=Hartigan-Wong\n\n\n\nkMeanModel &lt;- getLearnerModel(tunedKMeansModel)                              # 예측 모형 추출\nkMeanModel\n\nK-means clustering with 7 clusters of sizes 5, 6, 6, 7, 7, 6, 3\n\nCluster means:\n       Murder    Assault   UrbanPop       Rape\n1  0.56255999  1.1067356  1.1837676  1.9283197\n2  1.27699731  0.9532023  0.1908547  0.5829423\n3  0.08098804 -0.2660328 -0.4765513 -0.2572366\n4 -1.13878299 -1.1439268 -1.2409738 -1.1698985\n5 -0.44629702 -0.0736253  0.7662624  0.1581765\n6 -0.81007846 -0.9512389  0.4952855 -0.6409547\n7  1.66510628  1.5245346 -1.2844639 -0.2226838\n\nClustering vector:\n  Pennsylvania     Washington South Carolina      Minnesota         Nevada           Utah        Montana      Louisiana       Nebraska      Tennessee    Mississippi       Maryland   South Dakota \n             6              5              7              6              1              5              3              2              6              2              7              2              4 \n      Delaware       Oklahoma     New Mexico       Colorado     New Jersey      Wisconsin       Virginia   North Dakota        Arizona         Hawaii  New Hampshire     California        Alabama \n             5              5              2              1              5              6              3              4              1              6              4              1              2 \n      Michigan           Ohio         Oregon    Connecticut          Texas       Arkansas North Carolina          Maine        Indiana        Wyoming       Kentucky           Iowa        Vermont \n             1              5              5              6              2              3              7              4              3              3              3              4              4 \n West Virginia \n             4 \n\nWithin cluster sum of squares by cluster:\n[1] 3.970393 4.544595 2.689821 3.834536 5.417810 3.459916 1.052706\n (between_SS / total_SS =  84.0 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n# Training Dataset의 각 군집별 특징 시각화\nUSArrests.trd.clus &lt;- mutate(USArrests.trd,\n                             kMeansCluster = as.factor(kMeanModel$cluster))  # 예측 모형에 의한 군집 결과\nUSArrests.trd.clus %&gt;%\n  as_tibble\n\n# A tibble: 40 × 5\n   Murder Assault UrbanPop   Rape kMeansCluster\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;        \n 1 -0.283  -0.663    0.566 -0.623 6            \n 2 -0.830  -0.203    0.636  0.610 5            \n 3  1.64    1.38    -1.12   0.206 7            \n 4 -1.14   -1.06     0.144 -0.623 6            \n 5  1.12    1.06     1.20   2.77  1            \n 6 -1.02   -0.498    1.13   0.250 5            \n 7 -0.355  -0.627   -0.769 -0.459 3            \n 8  1.88    1.02     0.144  0.174 2            \n 9 -0.759  -0.710   -0.137 -0.448 6            \n10  1.36    0.303   -0.348  0.687 2            \n# ℹ 30 more rows\n\nggpairs(USArrests.trd.clus, aes(col = kMeansCluster),\n        upper = list(continuous = \"density\")) +\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Clustering.html#예측",
    "href": "Clustering.html#예측",
    "title": "2  Cluster Analysis based on k-means",
    "section": "2.6 예측",
    "text": "2.6 예측\n\n# 예측 군집 생성\nPred &lt;- predict(tunedKMeansModel, \n                newdata = USArrests.ted)         # predict(Trained Model, Test Dataset)\nPred %&gt;%\n  as_tibble\n\n# A tibble: 10 × 1\n   response\n      &lt;int&gt;\n 1        1\n 2        2\n 3        2\n 4        4\n 5        2\n 6        6\n 7        5\n 8        5\n 9        1\n10        6",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cluster Analysis based on k-means</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html",
    "href": "Decision_tree.html",
    "title": "3  Decision Tree",
    "section": "",
    "text": "3.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"rattle\", \"rpart.plot\",                                  # For fancyRpartPlot\n               \"visNetwork\", \"sparkline\",                               # For visTree\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정\n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#데이터-전처리-i",
    "href": "Decision_tree.html#데이터-전처리-i",
    "title": "3  Decision Tree",
    "section": "3.2 데이터 전처리 I",
    "text": "3.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\nglimpse(titanic1)                                                       # 데이터 구조 확인\n\nRows: 891\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#데이터-탐색",
    "href": "Decision_tree.html#데이터-탐색",
    "title": "3  Decision Tree",
    "section": "3.3 데이터 탐색",
    "text": "3.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#데이터-분할",
    "href": "Decision_tree.html#데이터-분할",
    "title": "3  Decision Tree",
    "section": "3.4 데이터 분할",
    "text": "3.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#데이터-전처리-ii",
    "href": "Decision_tree.html#데이터-전처리-ii",
    "title": "3  Decision Tree",
    "section": "3.5 데이터 전처리 II",
    "text": "3.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#모형-훈련",
    "href": "Decision_tree.html#모형-훈련",
    "title": "3  Decision Tree",
    "section": "3.6 모형 훈련",
    "text": "3.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 cp (Complexity Parameter)의 최적값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,  # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)       # 병렬 처리\n\nset.seed(100)                                          # For CV\nrtree.caret &lt;- train(x = titanic.trd.Imp[,-1],         # Training Dataset including Only 예측 변수\n                     y = titanic.trd.Imp[,\"Survived\"], # Training Dataset including Only Target\n                     method = \"rpart\", \n                     trControl = fitControl)  \n\nCaution! Package \"caret\"에서는 함수 rpart()의 옵션 xval = 0이며, cp의 최적값을 이용하여 최종 모형을 훈련하기 때문에 가지치기를 수행할 필요가 없다. 게다가, Package \"caret\"을 통해 \"rpart\"를 수행하는 경우, 함수 train(Target ~ 예측 변수, data)를 사용하면 범주형 예측 변수는 자동적으로 더미 변환이 된다. 범주형 예측 변수에 대해 더미 변환을 수행하고 싶지 않다면 함수 train(x = 예측 변수만 포함하는 데이터셋, y = Target만 포함하는 데이터셋)를 사용한다.\n\nrtree.caret\n\nCART \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  cp          Accuracy  Kappa    \n  0.03750000  0.8000    0.5702342\n  0.03958333  0.7760    0.5253156\n  0.40833333  0.6944    0.2790591\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.0375.\n\nplot(rtree.caret)                                      # Plot\n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 3개의 cp 값에 대한 정확도를 보여주며, cp = 0.0375일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 값 0.0375 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(cp = seq(0.01, 0.07, by = 0.001))   # cp의 탐색 범위 \n\nset.seed(100)                                                 # For CV\nrtree.grid.caret &lt;- train(x = titanic.trd.Imp[,-1],           # Training Dataset including Only 예측 변수\n                          y = titanic.trd.Imp[,\"Survived\"],   # Training Dataset including Only Target\n                          tuneGrid = customGrid,\n                          method = \"rpart\", \n                          trControl = fitControl)\nrtree.grid.caret\n\nCART \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  cp     Accuracy  Kappa    \n  0.010  0.7936    0.5485790\n  0.011  0.7920    0.5477686\n  0.012  0.7920    0.5477686\n  0.013  0.7920    0.5463259\n  0.014  0.7920    0.5463259\n  0.015  0.7920    0.5463259\n  0.016  0.7904    0.5462842\n  0.017  0.7904    0.5462842\n  0.018  0.7904    0.5462842\n  0.019  0.7904    0.5462842\n  0.020  0.7904    0.5462842\n  0.021  0.8000    0.5702342\n  0.022  0.8000    0.5702342\n  0.023  0.8000    0.5702342\n  0.024  0.8000    0.5702342\n  0.025  0.8000    0.5702342\n  0.026  0.8000    0.5702342\n  0.027  0.8000    0.5702342\n  0.028  0.8000    0.5702342\n  0.029  0.8000    0.5702342\n  0.030  0.8000    0.5702342\n  0.031  0.8000    0.5702342\n  0.032  0.8000    0.5702342\n  0.033  0.8000    0.5702342\n  0.034  0.8000    0.5702342\n  0.035  0.8000    0.5702342\n  0.036  0.8000    0.5702342\n  0.037  0.8000    0.5702342\n  0.038  0.8000    0.5702342\n  0.039  0.8000    0.5702342\n  0.040  0.7760    0.5253156\n  0.041  0.7760    0.5253156\n  0.042  0.7696    0.5105699\n  0.043  0.7696    0.5105699\n  0.044  0.7696    0.5105699\n  0.045  0.7696    0.5105699\n  0.046  0.7696    0.5105699\n  0.047  0.7728    0.5151960\n  0.048  0.7728    0.5151960\n  0.049  0.7728    0.5151960\n  0.050  0.7728    0.5151960\n  0.051  0.7728    0.5151960\n  0.052  0.7728    0.5151960\n  0.053  0.7728    0.5151960\n  0.054  0.7728    0.5151960\n  0.055  0.7728    0.5151960\n  0.056  0.7728    0.5151960\n  0.057  0.7728    0.5151960\n  0.058  0.7728    0.5151960\n  0.059  0.7728    0.5151960\n  0.060  0.7728    0.5151960\n  0.061  0.7728    0.5151960\n  0.062  0.7728    0.5151960\n  0.063  0.7728    0.5151960\n  0.064  0.7728    0.5151960\n  0.065  0.7728    0.5151960\n  0.066  0.7728    0.5151960\n  0.067  0.7728    0.5151960\n  0.068  0.7728    0.5151960\n  0.069  0.7728    0.5151960\n  0.070  0.7728    0.5151960\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.039.\n\n\n\nplot(rtree.grid.caret)                                  # Plot\n\n\n\n\n\n\n\nrtree.grid.caret$bestTune                               # cp의 최적값\n\n      cp\n30 0.039\n\n\nResult! cp = 0.039일 때 정확도가 가장 높다는 것을 알 수 있으며, cp = 0.039를 가지는 모형을 최적의 훈련된 모형으로 선택한다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#tree-plot",
    "href": "Decision_tree.html#tree-plot",
    "title": "3  Decision Tree",
    "section": "3.7 Tree Plot",
    "text": "3.7 Tree Plot\n\n3.7.1 “fancyRpartPlot”\n\nfancyRpartPlot(rtree.grid.caret$finalModel)    # Plot\n\n\n\n\n\n\n\n\n\n\n3.7.2 “visTree”\n\nvisTree(rtree.grid.caret$finalModel)           # Network-based Plot",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "Decision_tree.html#모형-평가",
    "href": "Decision_tree.html#모형-평가",
    "title": "3  Decision Tree",
    "section": "3.8 모형 평가",
    "text": "3.8 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\nrtree.grid.caret.pred &lt;- predict(rtree.grid.caret, \n                                 newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수     \n\nrtree.grid.caret.pred %&gt;%\n  as_tibble                    \n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n3.8.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(rtree.grid.caret.pred, titanic.ted.Imp$Survived, \n                               positive = \"yes\")          # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  153  35\n       yes  11  67\n                                          \n               Accuracy : 0.8271          \n                 95% CI : (0.7762, 0.8705)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 6.692e-14       \n                                          \n                  Kappa : 0.6172          \n                                          \n Mcnemar's Test P-Value : 0.000696        \n                                          \n            Sensitivity : 0.6569          \n            Specificity : 0.9329          \n         Pos Pred Value : 0.8590          \n         Neg Pred Value : 0.8138          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2519          \n   Detection Prevalence : 0.2932          \n      Balanced Accuracy : 0.7949          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n3.8.2 ROC 곡선\n\n# 예측 확률 생성 \ntest.rtree.prob &lt;- predict(rtree.grid.caret,\n                           newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수 \n                           type = \"prob\")                 # 예측 확률 생성 \n\ntest.rtree.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no   yes\n    &lt;dbl&gt; &lt;dbl&gt;\n 1 0.0531 0.947\n 2 0.806  0.194\n 3 0.806  0.194\n 4 0.0531 0.947\n 5 0.806  0.194\n 6 0.806  0.194\n 7 0.417  0.583\n 8 1      0    \n 9 0.806  0.194\n10 0.0531 0.947\n# ℹ 256 more rows\n\n\n\ntest.rtree.prob &lt;- test.rtree.prob[,2]                    # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                           # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.rtree.prob)                        # 예측 확률을 수치형으로 변환\n\n\n3.8.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nrtree.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")         # roc(실제 class, 예측 확률)\nauc        &lt;- round(auc(rtree.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(rtree.roc,   \n         col=\"gray\",                                      # Line Color\n         print.auc = TRUE,                                # AUC 출력 여부\n         print.auc.col = \"red\",                           # AUC 글씨 색깔\n         print.thres = TRUE,                              # Cutoff Value 출력 여부\n         print.thres.pch = 19,                            # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                         # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                              # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                      # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(rtree.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n3.8.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                                 # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n3.8.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nrtree.pred &lt;- prediction(pp, ac)                          # prediction(예측 확률, 실제 class)  \n\nrtree.perf &lt;- performance(rtree.pred, \"tpr\", \"fpr\")       # performance(, \"민감도\", \"1-특이도\")                      \nplot(rtree.perf, col = \"gray\")                            # ROC Curve\n\nperf.auc   &lt;- performance(rtree.pred, \"auc\")              # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.8.3 향상 차트\n\n3.8.3.1 Package “ROCR”\n\nrtree.perf &lt;- performance(rtree.pred, \"lift\", \"rpp\")        # Lift Chart\nplot(rtree.perf, main = \"lift curve\", \n     colorize = T,                                          # Coloring according to cutoff\n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "SVM_li.html",
    "href": "SVM_li.html",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "",
    "text": "4.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정\n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#데이터-전처리-i",
    "href": "SVM_li.html#데이터-전처리-i",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.2 데이터 전처리 I",
    "text": "4.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\n# 4. Convert One-hot Encoding for 범주형 예측 변수\ndummies &lt;- dummyVars(formula = ~ .,                                     # formula : ~ 예측 변수 / \".\" : data에 포함된 모든 변수를 의미\n                     data = titanic1[,-1],                              # Dataset including Only 예측 변수 -&gt; Target 제외\n                     fullRank = FALSE)                                  # fullRank = TRUE : Dummy Variable, fullRank = FALSE : One-hot Encoding\n\ntitanic.Var   &lt;- predict(dummies, newdata = titanic1) %&gt;%               # 범주형 예측 변수에 대한 One-hot Encoding 변환\n  data.frame()                                                          # Data Frame 형태로 변환 \n\nglimpse(titanic.Var)                                                    # 데이터 구조 확인\n\nRows: 891\nColumns: 8\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…\n\n# Combine Target with 변환된 예측 변수\ntitanic.df &lt;- data.frame(Survived = titanic1$Survived, \n                         titanic.Var)\n\ntitanic.df %&gt;%\n  as_tibble\n\n# A tibble: 891 × 9\n   Survived Pclass.1 Pclass.2 Pclass.3 Sex.female Sex.male   Age  Fare FamSize\n   &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 no              0        0        1          0        1    22  7.25       1\n 2 yes             1        0        0          1        0    38 71.3        1\n 3 yes             0        0        1          1        0    26  7.92       0\n 4 yes             1        0        0          1        0    35 53.1        1\n 5 no              0        0        1          0        1    35  8.05       0\n 6 no              0        0        1          0        1    NA  8.46       0\n 7 no              1        0        0          0        1    54 51.9        0\n 8 no              0        0        1          0        1     2 21.1        4\n 9 yes             0        0        1          1        0    27 11.1        2\n10 yes             0        1        0          1        0    14 30.1        1\n# ℹ 881 more rows\n\nglimpse(titanic.df)                                                     # 데이터 구조 확인\n\nRows: 891\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, …\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#데이터-탐색",
    "href": "SVM_li.html#데이터-탐색",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.3 데이터 탐색",
    "text": "4.3 데이터 탐색\n\nggpairs(titanic.df,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic.df,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#데이터-분할",
    "href": "SVM_li.html#데이터-분할",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.4 데이터 분할",
    "text": "4.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic.df$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic.df[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic.df[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#데이터-전처리-ii",
    "href": "SVM_li.html#데이터-전처리-ii",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.5 데이터 전처리 II",
    "text": "4.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes,…\n$ Pclass.1   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Pclass.3   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ Age        &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29…\n$ Fare       &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.879…\n$ FamSize    &lt;dbl&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 9\n$ Survived   &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no,…\n$ Pclass.1   &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Pclass.3   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,…\n$ Sex.female &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n$ Age        &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29…\n$ Fare       &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250…\n$ FamSize    &lt;dbl&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#모형-훈련",
    "href": "SVM_li.html#모형-훈련",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.6 모형 훈련",
    "text": "4.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 C (Cost)의 최적값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,  # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE,       # 병렬 처리\n                           classProbs = TRUE)          # For 예측 확률 생성\n\nset.seed(100)                                          # For CV\nsvm.li.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp,\n                    trControl = fitControl,\n                    method = \"svmLinear\",\n                    preProc = c(\"center\", \"scale\"))    # Standardization for 예측 변수\n\nsvm.li.fit\n\nSupport Vector Machines with Linear Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results:\n\n  Accuracy  Kappa    \n  0.7856    0.5405059\n\nTuning parameter 'C' was held constant at a value of 1\n\n\nResult! 기본값 C = 1에 대한 정확도를 보여준다. 해당 초모수 값 1 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(C = seq(0.5, 2, by = 0.1))     # C의 탐색 범위 \n\nset.seed(100)                                            # For CV\nsvm.li.grid.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp,\n                         trControl = fitControl,\n                         tuneGrid = customGrid,\n                         method = \"svmLinear\",\n                         preProc = c(\"center\", \"scale\")) # Standardization for 예측 변수\nsvm.li.grid.fit\n\nSupport Vector Machines with Linear Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  C    Accuracy  Kappa    \n  0.5  0.7792    0.5275532\n  0.6  0.7776    0.5245070\n  0.7  0.7792    0.5284916\n  0.8  0.7872    0.5426202\n  0.9  0.7792    0.5284916\n  1.0  0.7840    0.5365123\n  1.1  0.7776    0.5245070\n  1.2  0.7856    0.5395739\n  1.3  0.7840    0.5374597\n  1.4  0.7840    0.5365123\n  1.5  0.7840    0.5365123\n  1.6  0.7776    0.5245070\n  1.7  0.7792    0.5284916\n  1.8  0.7776    0.5245070\n  1.9  0.7840    0.5365123\n  2.0  0.7856    0.5395739\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was C = 0.8.\n\n\n\nplot(svm.li.grid.fit)                                    # Plot\n\n\n\n\n\n\n\nsvm.li.grid.fit$bestTune                                 # C의 최적값\n\n    C\n4 0.8\n\n\nResult! C = 0.8일 때 정확도가 가장 높다는 것을 알 수 있으며, C = 0.8를 가지는 모형을 최적의 훈련된 모형으로 선택한다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_li.html#모형-평가",
    "href": "SVM_li.html#모형-평가",
    "title": "4  Support Vector Machine with Linear Kernel",
    "section": "4.7 모형 평가",
    "text": "4.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\nsvm.li.pred &lt;- predict(svm.li.grid.fit,                                        \n                       newdata = titanic.ted.Imp[,-1])     # Test Dataset including Only 예측 변수       \n\nsvm.li.pred %&gt;%\n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n4.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(svm.li.pred, titanic.ted.Imp$Survived, \n                               positive = \"yes\")           # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  151  35\n       yes  13  67\n                                         \n               Accuracy : 0.8195         \n                 95% CI : (0.768, 0.8638)\n    No Information Rate : 0.6165         \n    P-Value [Acc &gt; NIR] : 5.675e-13      \n                                         \n                  Kappa : 0.6021         \n                                         \n Mcnemar's Test P-Value : 0.002437       \n                                         \n            Sensitivity : 0.6569         \n            Specificity : 0.9207         \n         Pos Pred Value : 0.8375         \n         Neg Pred Value : 0.8118         \n             Prevalence : 0.3835         \n         Detection Rate : 0.2519         \n   Detection Prevalence : 0.3008         \n      Balanced Accuracy : 0.7888         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\n\n\n\n4.7.2 ROC 곡선\n\n# 예측 확률 생성 \ntest.svm.prob &lt;- predict(svm.li.grid.fit, \n                         newdata = titanic.ted.Imp[,-1],   # Test Dataset including Only 예측 변수      \n                         type = \"prob\")                    # 예측 확률 생성 \n\ntest.svm.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n      no    yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.213 0.787 \n 2 0.789 0.211 \n 3 0.811 0.189 \n 4 0.276 0.724 \n 5 0.925 0.0748\n 6 0.803 0.197 \n 7 0.228 0.772 \n 8 0.647 0.353 \n 9 0.830 0.170 \n10 0.290 0.710 \n# ℹ 256 more rows\n\n\n\ntest.svm.prob &lt;- test.svm.prob[,2]                         # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                            # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.svm.prob)                           # 예측 확률을 수치형으로 변환\n\n\n4.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nsvm.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")          # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(svm.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(svm.roc,   \n         col=\"gray\",                                     # Line Color\n         print.auc = TRUE,                               # AUC 출력 여부\n         print.auc.col = \"red\",                          # AUC 글씨 색깔\n         print.thres = TRUE,                             # Cutoff Value 출력 여부\n         print.thres.pch = 19,                           # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                        # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                             # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                     # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(svm.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n4.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                                # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n4.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nsvm.pred &lt;- prediction(pp, ac)                           # prediction(예측 확률, 실제 class)    \n\nsvm.perf &lt;- performance(svm.pred, \"tpr\", \"fpr\")          # performance(, \"민감도\", \"1-특이도\")                      \nplot(svm.perf, col = \"gray\")                             # ROC Curve\n\nperf.auc   &lt;- performance(svm.pred, \"auc\")               # AUC\nauc        &lt;- attributes(perf.auc)$y.values \nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.3 향상 차트\n\n4.7.3.1 Package “ROCR”\n\nsvm.perf &lt;- performance(svm.pred, \"lift\", \"rpp\")         # Lift Chart\nplot(svm.perf, main = \"lift curve\", \n     colorize = T,                                       # Coloring according to cutoff\n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Support Vector Machine with Linear Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html",
    "href": "SVM_po.html",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "",
    "text": "5.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정\n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#데이터-전처리-i",
    "href": "SVM_po.html#데이터-전처리-i",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.2 데이터 전처리 I",
    "text": "5.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\n# 4. Convert One-hot Encoding for 범주형 예측 변수\ndummies &lt;- dummyVars(formula = ~ .,                                     # formula : ~ 예측 변수 / \".\" : data에 포함된 모든 변수를 의미\n                     data = titanic1[,-1],                              # Dataset including Only 예측 변수 -&gt; Target 제외\n                     fullRank = FALSE)                                  # fullRank = TRUE : Dummy Variable, fullRank = FALSE : One-hot Encoding\n\ntitanic.Var   &lt;- predict(dummies, newdata = titanic1) %&gt;%               # 범주형 예측 변수에 대한 One-hot Encoding 변환\n  data.frame()                                                          # Data Frame 형태로 변환 \n\nglimpse(titanic.Var)                                                    # 데이터 구조 확인\n\nRows: 891\nColumns: 8\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…\n\n# Combine Target with 변환된 예측 변수\ntitanic.df &lt;- data.frame(Survived = titanic1$Survived, \n                         titanic.Var)\n\ntitanic.df %&gt;%\n  as_tibble\n\n# A tibble: 891 × 9\n   Survived Pclass.1 Pclass.2 Pclass.3 Sex.female Sex.male   Age  Fare FamSize\n   &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 no              0        0        1          0        1    22  7.25       1\n 2 yes             1        0        0          1        0    38 71.3        1\n 3 yes             0        0        1          1        0    26  7.92       0\n 4 yes             1        0        0          1        0    35 53.1        1\n 5 no              0        0        1          0        1    35  8.05       0\n 6 no              0        0        1          0        1    NA  8.46       0\n 7 no              1        0        0          0        1    54 51.9        0\n 8 no              0        0        1          0        1     2 21.1        4\n 9 yes             0        0        1          1        0    27 11.1        2\n10 yes             0        1        0          1        0    14 30.1        1\n# ℹ 881 more rows\n\nglimpse(titanic.df)                                                     # 데이터 구조 확인\n\nRows: 891\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, …\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#데이터-탐색",
    "href": "SVM_po.html#데이터-탐색",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.3 데이터 탐색",
    "text": "5.3 데이터 탐색\n\nggpairs(titanic.df,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic.df,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#데이터-분할",
    "href": "SVM_po.html#데이터-분할",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.4 데이터 분할",
    "text": "5.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic.df$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic.df[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic.df[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#데이터-전처리-ii",
    "href": "SVM_po.html#데이터-전처리-ii",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.5 데이터 전처리 II",
    "text": "5.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes,…\n$ Pclass.1   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Pclass.3   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ Age        &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29…\n$ Fare       &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.879…\n$ FamSize    &lt;dbl&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 9\n$ Survived   &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no,…\n$ Pclass.1   &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Pclass.3   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,…\n$ Sex.female &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n$ Age        &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29…\n$ Fare       &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250…\n$ FamSize    &lt;dbl&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#모형-훈련",
    "href": "SVM_po.html#모형-훈련",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.6 모형 훈련",
    "text": "5.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 degree, scale, C (Cost)의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,  # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE,       # 병렬 처리\n                           classProbs = TRUE)          # For 예측 확률 생성\n\nset.seed(100)                                          # For CV\nsvm.po.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                    trControl = fitControl,\n                    method = \"svmPoly\",\n                    preProc = c(\"center\", \"scale\"))    # Standardization for 예측 변수\n\nsvm.po.fit\n\nSupport Vector Machines with Polynomial Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  degree  scale  C     Accuracy  Kappa    \n  1       0.001  0.25  0.7584    0.4975692\n  1       0.001  0.50  0.7568    0.4945341\n  1       0.001  1.00  0.7632    0.5075464\n  1       0.010  0.25  0.7744    0.5190513\n  1       0.010  0.50  0.7728    0.5151960\n  1       0.010  1.00  0.7728    0.5151960\n  1       0.100  0.25  0.7728    0.5151960\n  1       0.100  0.50  0.7728    0.5151960\n  1       0.100  1.00  0.7808    0.5304620\n  2       0.001  0.25  0.7584    0.4975692\n  2       0.001  0.50  0.7568    0.4945341\n  2       0.001  1.00  0.7680    0.5075973\n  2       0.010  0.25  0.7728    0.5151960\n  2       0.010  0.50  0.7728    0.5151960\n  2       0.010  1.00  0.7776    0.5244049\n  2       0.100  0.25  0.8048    0.5777419\n  2       0.100  0.50  0.8144    0.5997535\n  2       0.100  1.00  0.8128    0.5961417\n  3       0.001  0.25  0.7584    0.4982694\n  3       0.001  0.50  0.7648    0.5058791\n  3       0.001  1.00  0.7744    0.5190513\n  3       0.010  0.25  0.7728    0.5151960\n  3       0.010  0.50  0.7824    0.5337486\n  3       0.010  1.00  0.7856    0.5399515\n  3       0.100  0.25  0.8144    0.6005231\n  3       0.100  0.50  0.8128    0.5968526\n  3       0.100  1.00  0.8096    0.5892714\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were degree = 2, scale = 0.1 and C = 0.5.\n\nplot(svm.po.fit)                                       # plot\n\n\n\n\n\n\n\n\nResult! 각 초모수에 대해 랜덤하게 결정된 3개의 값을 조합하여 만든 27(3x3x3)개의 초모수 조합값 (degree, scale, C)에 대한 정확도를 보여주며, (degree = 2, scale = 0.1, C = 0.5)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (degree = 2, scale = 0.1, C = 0.5) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(degree = 1:3,                  # degree의 탐색 범위\n                          scale = seq(0.05, 0.15, 0.05), # scale의 탐색 범위\n                          C = seq(0.2, 0.8, 0.1))        # C의 탐색 범위\n\nset.seed(100)                                            # For CV\nsvm.po.grid.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp,\n                         trControl = fitControl,\n                         tuneGrid = customGrid,\n                         method = \"svmPoly\",\n                         preProc = c(\"center\", \"scale\")) # Standardization for 예측 변수\nsvm.po.grid.fit\n\nSupport Vector Machines with Polynomial Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  degree  scale  C    Accuracy  Kappa    \n  1       0.05   0.2  0.7728    0.5151960\n  1       0.05   0.3  0.7728    0.5151960\n  1       0.05   0.4  0.7728    0.5151960\n  1       0.05   0.5  0.7728    0.5151960\n  1       0.05   0.6  0.7728    0.5151960\n  1       0.05   0.7  0.7728    0.5151960\n  1       0.05   0.8  0.7728    0.5151960\n  1       0.10   0.2  0.7728    0.5151960\n  1       0.10   0.3  0.7728    0.5151960\n  1       0.10   0.4  0.7728    0.5151960\n  1       0.10   0.5  0.7728    0.5151960\n  1       0.10   0.6  0.7728    0.5151960\n  1       0.10   0.7  0.7760    0.5215808\n  1       0.10   0.8  0.7760    0.5214516\n  1       0.15   0.2  0.7728    0.5151960\n  1       0.15   0.3  0.7728    0.5151960\n  1       0.15   0.4  0.7728    0.5151960\n  1       0.15   0.5  0.7760    0.5214516\n  1       0.15   0.6  0.7792    0.5274727\n  1       0.15   0.7  0.7808    0.5304620\n  1       0.15   0.8  0.7808    0.5304620\n  2       0.05   0.2  0.7904    0.5491287\n  2       0.05   0.3  0.7936    0.5546763\n  2       0.05   0.4  0.8016    0.5697464\n  2       0.05   0.5  0.7984    0.5634940\n  2       0.05   0.6  0.8000    0.5664898\n  2       0.05   0.7  0.8048    0.5780726\n  2       0.05   0.8  0.8048    0.5778862\n  2       0.10   0.2  0.8048    0.5777299\n  2       0.10   0.3  0.8096    0.5887274\n  2       0.10   0.4  0.8144    0.5997535\n  2       0.10   0.5  0.8128    0.5961417\n  2       0.10   0.6  0.8144    0.5997535\n  2       0.10   0.7  0.8128    0.5961417\n  2       0.10   0.8  0.8128    0.5961417\n  2       0.15   0.2  0.8144    0.5997535\n  2       0.15   0.3  0.8128    0.5961417\n  2       0.15   0.4  0.8128    0.5961417\n  2       0.15   0.5  0.8096    0.5895891\n  2       0.15   0.6  0.8096    0.5895891\n  2       0.15   0.7  0.8096    0.5895891\n  2       0.15   0.8  0.8096    0.5895891\n  3       0.05   0.2  0.8016    0.5695023\n  3       0.05   0.3  0.8032    0.5738453\n  3       0.05   0.4  0.8096    0.5887018\n  3       0.05   0.5  0.8112    0.5919890\n  3       0.05   0.6  0.8128    0.5959492\n  3       0.05   0.7  0.8128    0.5959492\n  3       0.05   0.8  0.8128    0.5959492\n  3       0.10   0.2  0.8128    0.5967188\n  3       0.10   0.3  0.8144    0.6008164\n  3       0.10   0.4  0.8112    0.5932322\n  3       0.10   0.5  0.8144    0.6008164\n  3       0.10   0.6  0.8144    0.6008164\n  3       0.10   0.7  0.8144    0.6008164\n  3       0.10   0.8  0.8128    0.5968526\n  3       0.15   0.2  0.8128    0.5970121\n  3       0.15   0.3  0.8080    0.5858446\n  3       0.15   0.4  0.8112    0.5941864\n  3       0.15   0.5  0.8112    0.5938772\n  3       0.15   0.6  0.8112    0.5938772\n  3       0.15   0.7  0.8096    0.5895503\n  3       0.15   0.8  0.8096    0.5895503\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were degree = 2, scale = 0.15 and C = 0.2.\n\n\n\nplot(svm.po.grid.fit)                                    # Plot\n\n\n\n\n\n\n\nsvm.po.grid.fit$bestTune                                 # 최적의 초모수 조합값\n\n   degree scale   C\n36      2  0.15 0.2\n\n\nResult! (degree = 2, scale = 0.15, C = 0.2)일 때 정확도가 가장 높다는 것을 알 수 있으며, (degree = 2, scale = 0.15, C = 0.2)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_po.html#모형-평가",
    "href": "SVM_po.html#모형-평가",
    "title": "5  Support Vector Machine with Polynomial Kernel",
    "section": "5.7 모형 평가",
    "text": "5.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\nsvm.po.pred &lt;- predict(svm.po.grid.fit,                                        \n                       newdata = titanic.ted.Imp[,-1])     # Test Dataset including Only 예측 변수     \n\nsvm.po.pred %&gt;%\n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n5.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(svm.po.pred, titanic.ted.Imp$Survived, \n                               positive = \"yes\")         # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  152  33\n       yes  12  69\n                                          \n               Accuracy : 0.8308          \n                 95% CI : (0.7803, 0.8738)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 2.211e-14       \n                                          \n                  Kappa : 0.6277          \n                                          \n Mcnemar's Test P-Value : 0.002869        \n                                          \n            Sensitivity : 0.6765          \n            Specificity : 0.9268          \n         Pos Pred Value : 0.8519          \n         Neg Pred Value : 0.8216          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2594          \n   Detection Prevalence : 0.3045          \n      Balanced Accuracy : 0.8016          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n5.7.2 ROC 곡선\n\n# 예측 확률 생성 \ntest.svm.prob &lt;- predict(svm.po.grid.fit, \n                         newdata = titanic.ted.Imp[,-1], # Test Dataset including Only 예측 변수     \n                         type = \"prob\")                  # 예측 확률 생성 \n\ntest.svm.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no    yes\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.176  0.824 \n 2 0.830  0.170 \n 3 0.816  0.184 \n 4 0.276  0.724 \n 5 0.786  0.214 \n 6 0.884  0.116 \n 7 0.204  0.796 \n 8 0.960  0.0405\n 9 0.836  0.164 \n10 0.0320 0.968 \n# ℹ 256 more rows\n\n\n\ntest.svm.prob &lt;- test.svm.prob[,2]                       # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                          # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.svm.prob)                         # 예측 확률을 수치형으로 변환\n\n\n5.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nsvm.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")          # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(svm.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(svm.roc,   \n         col=\"gray\",                                     # Line Color\n         print.auc = TRUE,                               # AUC 출력 여부\n         print.auc.col = \"red\",                          # AUC 글씨 색깔\n         print.thres = TRUE,                             # Cutoff Value 출력 여부\n         print.thres.pch = 19,                           # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                        # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                             # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                     # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(svm.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n5.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                                # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n5.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nsvm.pred &lt;- prediction(pp, ac)                           # prediction(예측 확률, 실제 class)    \n\nsvm.perf &lt;- performance(svm.pred, \"tpr\", \"fpr\")          # performance(, \"민감도\", \"1-특이도\")                      \nplot(svm.perf, col = \"gray\")                             # ROC Curve\n\nperf.auc   &lt;- performance(svm.pred, \"auc\")               # AUC\nauc        &lt;- attributes(perf.auc)$y.values \nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 향상 차트\n\n5.7.3.1 Package “ROCR”\n\nsvm.perf &lt;- performance(svm.pred, \"lift\", \"rpp\")         # Lift Chart\nplot(svm.perf, main = \"lift curve\", \n     colorize = T,                                       # Coloring according to cutoff\n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support Vector Machine with Polynomial Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html",
    "href": "SVM_rd.html",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "",
    "text": "6.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정\n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#데이터-전처리-i",
    "href": "SVM_rd.html#데이터-전처리-i",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.2 데이터 전처리 I",
    "text": "6.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\n# 4. Convert One-hot Encoding for 범주형 예측 변수\ndummies &lt;- dummyVars(formula = ~ .,                                     # formula : ~ 예측 변수 / \".\" : data에 포함된 모든 변수를 의미\n                     data = titanic1[,-1],                              # Dataset including Only 예측 변수 -&gt; Target 제외\n                     fullRank = FALSE)                                  # fullRank = TRUE : Dummy Variable, fullRank = FALSE : One-hot Encoding\n\ntitanic.Var   &lt;- predict(dummies, newdata = titanic1) %&gt;%               # 범주형 예측 변수에 대한 One-hot Encoding 변환\n  data.frame()                                                          # Data Frame 형태로 변환 \n\nglimpse(titanic.Var)                                                    # 데이터 구조 확인\n\nRows: 891\nColumns: 8\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…\n\n# Combine Target with 변환된 예측 변수\ntitanic.df &lt;- data.frame(Survived = titanic1$Survived, \n                         titanic.Var)\n\ntitanic.df %&gt;%\n  as_tibble\n\n# A tibble: 891 × 9\n   Survived Pclass.1 Pclass.2 Pclass.3 Sex.female Sex.male   Age  Fare FamSize\n   &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 no              0        0        1          0        1    22  7.25       1\n 2 yes             1        0        0          1        0    38 71.3        1\n 3 yes             0        0        1          1        0    26  7.92       0\n 4 yes             1        0        0          1        0    35 53.1        1\n 5 no              0        0        1          0        1    35  8.05       0\n 6 no              0        0        1          0        1    NA  8.46       0\n 7 no              1        0        0          0        1    54 51.9        0\n 8 no              0        0        1          0        1     2 21.1        4\n 9 yes             0        0        1          1        0    27 11.1        2\n10 yes             0        1        0          1        0    14 30.1        1\n# ℹ 881 more rows\n\nglimpse(titanic.df)                                                     # 데이터 구조 확인\n\nRows: 891\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, …\n$ Pclass.1   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,…\n$ Pclass.3   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,…\n$ Age        &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 6…\n$ Fare       &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.000…\n$ FamSize    &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#데이터-탐색",
    "href": "SVM_rd.html#데이터-탐색",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.3 데이터 탐색",
    "text": "6.3 데이터 탐색\n\nggpairs(titanic.df,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic.df,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#데이터-분할",
    "href": "SVM_rd.html#데이터-분할",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.4 데이터 분할",
    "text": "6.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic.df$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic.df[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic.df[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#데이터-전처리-ii",
    "href": "SVM_rd.html#데이터-전처리-ii",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.5 데이터 전처리 II",
    "text": "6.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 9\n$ Survived   &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes,…\n$ Pclass.1   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Pclass.3   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,…\n$ Sex.female &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,…\n$ Age        &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29…\n$ Fare       &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.879…\n$ FamSize    &lt;dbl&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 9\n$ Survived   &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no,…\n$ Pclass.1   &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ Pclass.2   &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Pclass.3   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,…\n$ Sex.female &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…\n$ Sex.male   &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n$ Age        &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29…\n$ Fare       &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250…\n$ FamSize    &lt;dbl&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0,…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#모형-훈련",
    "href": "SVM_rd.html#모형-훈련",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.6 모형 훈련",
    "text": "6.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 sigma, C (Cost)의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,  # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE,       # 병렬 처리\n                           classProbs = TRUE)          # For 예측 확률 생성\n\nset.seed(100)                                          # For CV\nsvm.rd.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                    trControl = fitControl,\n                    method = \"svmRadial\",\n                    preProc = c(\"center\", \"scale\"))    # Standardization for 예측 변수\n\nsvm.rd.fit\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  C     Accuracy  Kappa    \n  0.25  0.8112    0.5915034\n  0.50  0.8096    0.5866688\n  1.00  0.8096    0.5871914\n\nTuning parameter 'sigma' was held constant at a value of 0.4991246\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.4991246 and C = 0.25.\n\nplot(svm.rd.fit)                                       # plot\n\n\n\n\n\n\n\n\nCaution! 함수 train()에 옵션 method = \"svmRadial\"을 입력하면 Package \"kernlab\"의 함수 ksvm()를 이용하여 Support Vector Machine을 수행한다. 해당 함수는 Kernel 함수가 Radial Basis일 때, 최적의 sigma 값을 자동으로 찾아준다.\nResult! 랜덤하게 결정된 3개의 초모수 C 값과 1개의 sigma 값을 조합하여 만든 3개의 초모수 조합값 (sigma, C)에 대한 정확도를 보여주며, (sigma = 0.4991246, C = 0.25)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (sigma = 0.4991246, C = 0.25) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(sigma = seq(0.3, 0.5, 0.1),    # sigma의 탐색 범위\n                          C = seq(0.1, 0.3, 0.1))        # C의 탐색 범위\n                          \nset.seed(100)                                            # For CV\nsvm.rd.grid.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                         trControl = fitControl,\n                         tuneGrid = customGrid,\n                         method = \"svmRadial\",\n                         preProc = c(\"center\", \"scale\")) # Standardization for 예측 변수\nsvm.rd.grid.fit\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n625 samples\n  8 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  sigma  C    Accuracy  Kappa    \n  0.3    0.1  0.8080    0.5827395\n  0.3    0.2  0.8128    0.5947906\n  0.3    0.3  0.8128    0.5947906\n  0.4    0.1  0.8096    0.5870197\n  0.4    0.2  0.8128    0.5947906\n  0.4    0.3  0.8112    0.5915034\n  0.5    0.1  0.8144    0.5990175\n  0.5    0.2  0.8080    0.5841129\n  0.5    0.3  0.8128    0.5945956\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.5 and C = 0.1.\n\n\n\nplot(svm.rd.grid.fit)                                    # Plot\n\n\n\n\n\n\n\nsvm.rd.grid.fit$bestTune                                 # 최적의 초모수 조합값\n\n  sigma   C\n7   0.5 0.1\n\n\nResult! (sigma = 0.5, C = 0.1)일 때 정확도가 가장 높다는 것을 알 수 있으며, (sigma = 0.5, C = 0.1)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "SVM_rd.html#모형-평가",
    "href": "SVM_rd.html#모형-평가",
    "title": "6  Support Vector Machine with Radial Basis Kernel",
    "section": "6.7 모형 평가",
    "text": "6.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\nsvm.rd.pred &lt;- predict(svm.rd.grid.fit,                                        \n                        newdata = titanic.ted.Imp[,-1])     # Test Dataset including Only 예측 변수     \n\nsvm.rd.pred %&gt;%\n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n6.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(svm.rd.pred, titanic.ted.Imp$Survived, \n                               positive = \"yes\")         # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  154  32\n       yes  10  70\n                                          \n               Accuracy : 0.8421          \n                 95% CI : (0.7926, 0.8838)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 6.804e-16       \n                                          \n                  Kappa : 0.6519          \n                                          \n Mcnemar's Test P-Value : 0.001194        \n                                          \n            Sensitivity : 0.6863          \n            Specificity : 0.9390          \n         Pos Pred Value : 0.8750          \n         Neg Pred Value : 0.8280          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2632          \n   Detection Prevalence : 0.3008          \n      Balanced Accuracy : 0.8126          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n6.7.2 ROC 곡선\n\n# 예측 확률 생성 \ntest.svm.prob &lt;- predict(svm.rd.grid.fit, \n                         newdata = titanic.ted.Imp[,-1], # Test Dataset including Only 예측 변수      \n                         type = \"prob\")                  # 예측 확률 생성 \n\ntest.svm.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no   yes\n    &lt;dbl&gt; &lt;dbl&gt;\n 1 0.0536 0.946\n 2 0.869  0.131\n 3 0.869  0.131\n 4 0.373  0.627\n 5 0.805  0.195\n 6 0.869  0.131\n 7 0.199  0.801\n 8 0.813  0.187\n 9 0.868  0.132\n10 0.263  0.737\n# ℹ 256 more rows\n\n\n\ntest.svm.prob &lt;- test.svm.prob[,2]                       # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                          # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.svm.prob)                         # 예측 확률을 수치형으로 변환\n\n\n6.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nsvm.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")          # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(svm.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(svm.roc,   \n         col=\"gray\",                                     # Line Color\n         print.auc = TRUE,                               # AUC 출력 여부\n         print.auc.col = \"red\",                          # AUC 글씨 색깔\n         print.thres = TRUE,                             # Cutoff Value 출력 여부\n         print.thres.pch = 19,                           # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                        # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                             # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                     # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(svm.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n6.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                                # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n6.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nsvm.pred &lt;- prediction(pp, ac)                           # prediction(예측 확률, 실제 class)    \n\nsvm.perf &lt;- performance(svm.pred, \"tpr\", \"fpr\")          # performance(, \"민감도\", \"1-특이도\")                      \nplot(svm.perf, col = \"gray\")                             # ROC Curve\n\nperf.auc   &lt;- performance(svm.pred, \"auc\")               # AUC\nauc        &lt;- attributes(perf.auc)$y.values \nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.3 향상 차트\n\n6.7.3.1 Package “ROCR”\n\nsvm.perf &lt;- performance(svm.pred, \"lift\", \"rpp\")         # Lift Chart\nplot(svm.perf, main = \"lift curve\", \n     colorize = T,                                       # Coloring according to cutoff\n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine with Radial Basis Kernel</span>"
    ]
  },
  {
    "objectID": "LASSO.html",
    "href": "LASSO.html",
    "title": "7  LASSO Regression",
    "section": "",
    "text": "7.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정     \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#데이터-전처리-i",
    "href": "LASSO.html#데이터-전처리-i",
    "title": "7  LASSO Regression",
    "section": "7.2 데이터 전처리 I",
    "text": "7.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\ntitanic1 %&gt;%\n  as_tibble\n\n# A tibble: 891 × 6\n   Survived Pclass Sex      Age  Fare FamSize\n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 no       3      male      22  7.25       1\n 2 yes      1      female    38 71.3        1\n 3 yes      3      female    26  7.92       0\n 4 yes      1      female    35 53.1        1\n 5 no       3      male      35  8.05       0\n 6 no       3      male      NA  8.46       0\n 7 no       1      male      54 51.9        0\n 8 no       3      male       2 21.1        4\n 9 yes      3      female    27 11.1        2\n10 yes      2      female    14 30.1        1\n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#데이터-탐색",
    "href": "LASSO.html#데이터-탐색",
    "title": "7  LASSO Regression",
    "section": "7.3 데이터 탐색",
    "text": "7.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#데이터-분할",
    "href": "LASSO.html#데이터-분할",
    "title": "7  LASSO Regression",
    "section": "7.4 데이터 분할",
    "text": "7.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                             # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]                 # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]                # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#데이터-전처리-ii",
    "href": "LASSO.html#데이터-전처리-ii",
    "title": "7  LASSO Regression",
    "section": "7.5 데이터 전처리 II",
    "text": "7.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#모형-훈련",
    "href": "LASSO.html#모형-훈련",
    "title": "7  LASSO Regression",
    "section": "7.6 모형 훈련",
    "text": "7.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"를 통해 LASSO Regression을 수행하기 위해 옵션 method에 다양한 방법(Ex: \"lasso\", \"blasso\" 등)을 입력할 수 있지만, 대부분 회귀 문제에 대해서만 분석이 가능하다. 분류와 회귀 문제 모두 가능한 \"glmnet\"을 이용하려면 옵션 tuneGrid = expand.grid()을 통해 탐색하고자 하는 초모수 lambda의 범위를 직접 지정해줘야 한다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,                 # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)                      # 병렬 처리\n\nset.seed(200)                                                         # For CV\nlasso.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                   trControl = fitControl ,\n                   method = \"glmnet\",\n                   tuneGrid = expand.grid(alpha = 1,                  # For LASSO Regression\n                                          lambda = seq(0, 1, 0.001)), # lambda의 탐색 범위\n                   preProc = c(\"center\", \"scale\"))                    # Standardization for 예측 변수\n\nlasso.fit\n\nglmnet \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  lambda  Accuracy  Kappa     \n  0.000   0.7840    0.53787566\n  0.001   0.7840    0.53787566\n  0.002   0.7888    0.54713361\n  0.003   0.7888    0.54587635\n  0.004   0.7840    0.53373303\n  0.005   0.7872    0.53898823\n  0.006   0.7840    0.53197659\n  0.007   0.7840    0.53197400\n  0.008   0.7856    0.53507154\n  0.009   0.7856    0.53507154\n  0.010   0.7840    0.53132186\n  0.011   0.7840    0.53132186\n  0.012   0.7840    0.53067201\n  0.013   0.7840    0.53067201\n  0.014   0.7856    0.53379173\n  0.015   0.7872    0.53754141\n  0.016   0.7872    0.53754141\n  0.017   0.7872    0.53754141\n  0.018   0.7856    0.53447375\n  0.019   0.7856    0.53539055\n  0.020   0.7872    0.53933256\n  0.021   0.7920    0.54927750\n  0.022   0.7936    0.55318733\n  0.023   0.7920    0.55005371\n  0.024   0.7904    0.54630152\n  0.025   0.7904    0.54633807\n  0.026   0.7904    0.54633807\n  0.027   0.7872    0.54007357\n  0.028   0.7872    0.54076382\n  0.029   0.7856    0.53773701\n  0.030   0.7776    0.52231065\n  0.031   0.7776    0.52231065\n  0.032   0.7776    0.52231065\n  0.033   0.7760    0.51930816\n  0.034   0.7760    0.52015488\n  0.035   0.7760    0.52015488\n  0.036   0.7728    0.51413374\n  0.037   0.7712    0.51115833\n  0.038   0.7712    0.51115833\n  0.039   0.7712    0.51115833\n  0.040   0.7712    0.51115833\n  0.041   0.7712    0.51115833\n  0.042   0.7712    0.51115833\n  0.043   0.7712    0.51115833\n  0.044   0.7712    0.51115833\n  0.045   0.7712    0.51115833\n  0.046   0.7728    0.51499913\n  0.047   0.7728    0.51499913\n  0.048   0.7728    0.51499913\n  0.049   0.7728    0.51499913\n  0.050   0.7728    0.51499913\n  0.051   0.7728    0.51499913\n  0.052   0.7728    0.51499913\n  0.053   0.7728    0.51499913\n  0.054   0.7728    0.51499913\n  0.055   0.7728    0.51499913\n  0.056   0.7728    0.51499913\n  0.057   0.7728    0.51499913\n  0.058   0.7728    0.51499913\n  0.059   0.7728    0.51499913\n  0.060   0.7728    0.51499913\n  0.061   0.7728    0.51499913\n  0.062   0.7728    0.51499913\n  0.063   0.7728    0.51499913\n  0.064   0.7728    0.51499913\n  0.065   0.7728    0.51499913\n  0.066   0.7728    0.51499913\n  0.067   0.7728    0.51499913\n  0.068   0.7728    0.51499913\n  0.069   0.7728    0.51499913\n  0.070   0.7728    0.51499913\n  0.071   0.7728    0.51499913\n  0.072   0.7728    0.51499913\n  0.073   0.7728    0.51499913\n  0.074   0.7728    0.51499913\n  0.075   0.7728    0.51499913\n  0.076   0.7728    0.51499913\n  0.077   0.7728    0.51499913\n  0.078   0.7728    0.51499913\n  0.079   0.7728    0.51499913\n  0.080   0.7728    0.51499913\n  0.081   0.7728    0.51499913\n  0.082   0.7728    0.51499913\n  0.083   0.7728    0.51499913\n  0.084   0.7728    0.51499913\n  0.085   0.7728    0.51499913\n  0.086   0.7728    0.51499913\n  0.087   0.7728    0.51499913\n  0.088   0.7728    0.51499913\n  0.089   0.7728    0.51499913\n  0.090   0.7728    0.51499913\n  0.091   0.7728    0.51499913\n  0.092   0.7728    0.51499913\n  0.093   0.7728    0.51499913\n  0.094   0.7728    0.51499913\n  0.095   0.7728    0.51499913\n  0.096   0.7728    0.51499913\n  0.097   0.7728    0.51499913\n  0.098   0.7728    0.51499913\n  0.099   0.7728    0.51499913\n  0.100   0.7728    0.51499913\n  0.101   0.7728    0.51499913\n  0.102   0.7728    0.51499913\n  0.103   0.7728    0.51499913\n  0.104   0.7728    0.51499913\n  0.105   0.7728    0.51499913\n  0.106   0.7728    0.51499913\n  0.107   0.7728    0.51499913\n  0.108   0.7728    0.51499913\n  0.109   0.7728    0.51499913\n  0.110   0.7728    0.51499913\n  0.111   0.7728    0.51499913\n  0.112   0.7728    0.51499913\n  0.113   0.7728    0.51499913\n  0.114   0.7728    0.51499913\n  0.115   0.7728    0.51499913\n  0.116   0.7728    0.51499913\n  0.117   0.7728    0.51499913\n  0.118   0.7728    0.51499913\n  0.119   0.7728    0.51499913\n  0.120   0.7728    0.51499913\n  0.121   0.7728    0.51499913\n  0.122   0.7728    0.51499913\n  0.123   0.7728    0.51499913\n  0.124   0.7728    0.51499913\n  0.125   0.7728    0.51499913\n  0.126   0.7728    0.51499913\n  0.127   0.7728    0.51499913\n  0.128   0.7728    0.51499913\n  0.129   0.7728    0.51499913\n  0.130   0.7728    0.51499913\n  0.131   0.7728    0.51499913\n  0.132   0.7728    0.51499913\n  0.133   0.7680    0.49535288\n  0.134   0.7680    0.49535288\n  0.135   0.7680    0.49535288\n  0.136   0.7680    0.49535288\n  0.137   0.7680    0.49535288\n  0.138   0.7680    0.49535288\n  0.139   0.7680    0.49535288\n  0.140   0.7680    0.49535288\n  0.141   0.7680    0.49535288\n  0.142   0.7616    0.46904664\n  0.143   0.7616    0.46904664\n  0.144   0.7616    0.46904664\n  0.145   0.7616    0.46904664\n  0.146   0.7616    0.46904664\n  0.147   0.7616    0.46904664\n  0.148   0.7616    0.46904664\n  0.149   0.7616    0.46904664\n  0.150   0.7616    0.46904664\n  0.151   0.7616    0.46904664\n  0.152   0.7616    0.46904664\n  0.153   0.7616    0.46904664\n  0.154   0.7616    0.46904664\n  0.155   0.7616    0.46904664\n  0.156   0.7248    0.35460447\n  0.157   0.7248    0.35460447\n  0.158   0.7248    0.35460447\n  0.159   0.7248    0.35460447\n  0.160   0.7248    0.35460447\n  0.161   0.7248    0.35460447\n  0.162   0.6944    0.26430337\n  0.163   0.6944    0.26430337\n  0.164   0.6672    0.18156771\n  0.165   0.6672    0.18156771\n  0.166   0.6672    0.18156771\n  0.167   0.6672    0.18156771\n  0.168   0.6384    0.08223885\n  0.169   0.6384    0.08223885\n  0.170   0.6384    0.08223885\n  0.171   0.6384    0.08223885\n  0.172   0.6384    0.08223885\n  0.173   0.6384    0.08223885\n  0.174   0.6160    0.00000000\n  0.175   0.6160    0.00000000\n  0.176   0.6160    0.00000000\n  0.177   0.6160    0.00000000\n  0.178   0.6160    0.00000000\n  0.179   0.6160    0.00000000\n  0.180   0.6160    0.00000000\n  0.181   0.6160    0.00000000\n  0.182   0.6160    0.00000000\n  0.183   0.6160    0.00000000\n  0.184   0.6160    0.00000000\n  0.185   0.6160    0.00000000\n  0.186   0.6160    0.00000000\n  0.187   0.6160    0.00000000\n  0.188   0.6160    0.00000000\n  0.189   0.6160    0.00000000\n  0.190   0.6160    0.00000000\n  0.191   0.6160    0.00000000\n  0.192   0.6160    0.00000000\n  0.193   0.6160    0.00000000\n  0.194   0.6160    0.00000000\n  0.195   0.6160    0.00000000\n  0.196   0.6160    0.00000000\n  0.197   0.6160    0.00000000\n  0.198   0.6160    0.00000000\n  0.199   0.6160    0.00000000\n  0.200   0.6160    0.00000000\n  0.201   0.6160    0.00000000\n  0.202   0.6160    0.00000000\n  0.203   0.6160    0.00000000\n  0.204   0.6160    0.00000000\n  0.205   0.6160    0.00000000\n  0.206   0.6160    0.00000000\n  0.207   0.6160    0.00000000\n  0.208   0.6160    0.00000000\n  0.209   0.6160    0.00000000\n  0.210   0.6160    0.00000000\n  0.211   0.6160    0.00000000\n  0.212   0.6160    0.00000000\n  0.213   0.6160    0.00000000\n  0.214   0.6160    0.00000000\n  0.215   0.6160    0.00000000\n  0.216   0.6160    0.00000000\n  0.217   0.6160    0.00000000\n  0.218   0.6160    0.00000000\n  0.219   0.6160    0.00000000\n  0.220   0.6160    0.00000000\n  0.221   0.6160    0.00000000\n  0.222   0.6160    0.00000000\n  0.223   0.6160    0.00000000\n  0.224   0.6160    0.00000000\n  0.225   0.6160    0.00000000\n  0.226   0.6160    0.00000000\n  0.227   0.6160    0.00000000\n  0.228   0.6160    0.00000000\n  0.229   0.6160    0.00000000\n  0.230   0.6160    0.00000000\n  0.231   0.6160    0.00000000\n  0.232   0.6160    0.00000000\n  0.233   0.6160    0.00000000\n  0.234   0.6160    0.00000000\n  0.235   0.6160    0.00000000\n  0.236   0.6160    0.00000000\n  0.237   0.6160    0.00000000\n  0.238   0.6160    0.00000000\n  0.239   0.6160    0.00000000\n  0.240   0.6160    0.00000000\n  0.241   0.6160    0.00000000\n  0.242   0.6160    0.00000000\n  0.243   0.6160    0.00000000\n  0.244   0.6160    0.00000000\n  0.245   0.6160    0.00000000\n  0.246   0.6160    0.00000000\n  0.247   0.6160    0.00000000\n  0.248   0.6160    0.00000000\n  0.249   0.6160    0.00000000\n  0.250   0.6160    0.00000000\n  0.251   0.6160    0.00000000\n  0.252   0.6160    0.00000000\n  0.253   0.6160    0.00000000\n  0.254   0.6160    0.00000000\n  0.255   0.6160    0.00000000\n  0.256   0.6160    0.00000000\n  0.257   0.6160    0.00000000\n  0.258   0.6160    0.00000000\n  0.259   0.6160    0.00000000\n  0.260   0.6160    0.00000000\n  0.261   0.6160    0.00000000\n  0.262   0.6160    0.00000000\n  0.263   0.6160    0.00000000\n  0.264   0.6160    0.00000000\n  0.265   0.6160    0.00000000\n  0.266   0.6160    0.00000000\n  0.267   0.6160    0.00000000\n  0.268   0.6160    0.00000000\n  0.269   0.6160    0.00000000\n  0.270   0.6160    0.00000000\n  0.271   0.6160    0.00000000\n  0.272   0.6160    0.00000000\n  0.273   0.6160    0.00000000\n  0.274   0.6160    0.00000000\n  0.275   0.6160    0.00000000\n  0.276   0.6160    0.00000000\n  0.277   0.6160    0.00000000\n  0.278   0.6160    0.00000000\n  0.279   0.6160    0.00000000\n  0.280   0.6160    0.00000000\n  0.281   0.6160    0.00000000\n  0.282   0.6160    0.00000000\n  0.283   0.6160    0.00000000\n  0.284   0.6160    0.00000000\n  0.285   0.6160    0.00000000\n  0.286   0.6160    0.00000000\n  0.287   0.6160    0.00000000\n  0.288   0.6160    0.00000000\n  0.289   0.6160    0.00000000\n  0.290   0.6160    0.00000000\n  0.291   0.6160    0.00000000\n  0.292   0.6160    0.00000000\n  0.293   0.6160    0.00000000\n  0.294   0.6160    0.00000000\n  0.295   0.6160    0.00000000\n  0.296   0.6160    0.00000000\n  0.297   0.6160    0.00000000\n  0.298   0.6160    0.00000000\n  0.299   0.6160    0.00000000\n  0.300   0.6160    0.00000000\n  0.301   0.6160    0.00000000\n  0.302   0.6160    0.00000000\n  0.303   0.6160    0.00000000\n  0.304   0.6160    0.00000000\n  0.305   0.6160    0.00000000\n  0.306   0.6160    0.00000000\n  0.307   0.6160    0.00000000\n  0.308   0.6160    0.00000000\n  0.309   0.6160    0.00000000\n  0.310   0.6160    0.00000000\n  0.311   0.6160    0.00000000\n  0.312   0.6160    0.00000000\n  0.313   0.6160    0.00000000\n  0.314   0.6160    0.00000000\n  0.315   0.6160    0.00000000\n  0.316   0.6160    0.00000000\n  0.317   0.6160    0.00000000\n  0.318   0.6160    0.00000000\n  0.319   0.6160    0.00000000\n  0.320   0.6160    0.00000000\n  0.321   0.6160    0.00000000\n  0.322   0.6160    0.00000000\n  0.323   0.6160    0.00000000\n  0.324   0.6160    0.00000000\n  0.325   0.6160    0.00000000\n  0.326   0.6160    0.00000000\n  0.327   0.6160    0.00000000\n  0.328   0.6160    0.00000000\n  0.329   0.6160    0.00000000\n  0.330   0.6160    0.00000000\n  0.331   0.6160    0.00000000\n  0.332   0.6160    0.00000000\n  0.333   0.6160    0.00000000\n  0.334   0.6160    0.00000000\n  0.335   0.6160    0.00000000\n  0.336   0.6160    0.00000000\n  0.337   0.6160    0.00000000\n  0.338   0.6160    0.00000000\n  0.339   0.6160    0.00000000\n  0.340   0.6160    0.00000000\n  0.341   0.6160    0.00000000\n  0.342   0.6160    0.00000000\n  0.343   0.6160    0.00000000\n  0.344   0.6160    0.00000000\n  0.345   0.6160    0.00000000\n  0.346   0.6160    0.00000000\n  0.347   0.6160    0.00000000\n  0.348   0.6160    0.00000000\n  0.349   0.6160    0.00000000\n  0.350   0.6160    0.00000000\n  0.351   0.6160    0.00000000\n  0.352   0.6160    0.00000000\n  0.353   0.6160    0.00000000\n  0.354   0.6160    0.00000000\n  0.355   0.6160    0.00000000\n  0.356   0.6160    0.00000000\n  0.357   0.6160    0.00000000\n  0.358   0.6160    0.00000000\n  0.359   0.6160    0.00000000\n  0.360   0.6160    0.00000000\n  0.361   0.6160    0.00000000\n  0.362   0.6160    0.00000000\n  0.363   0.6160    0.00000000\n  0.364   0.6160    0.00000000\n  0.365   0.6160    0.00000000\n  0.366   0.6160    0.00000000\n  0.367   0.6160    0.00000000\n  0.368   0.6160    0.00000000\n  0.369   0.6160    0.00000000\n  0.370   0.6160    0.00000000\n  0.371   0.6160    0.00000000\n  0.372   0.6160    0.00000000\n  0.373   0.6160    0.00000000\n  0.374   0.6160    0.00000000\n  0.375   0.6160    0.00000000\n  0.376   0.6160    0.00000000\n  0.377   0.6160    0.00000000\n  0.378   0.6160    0.00000000\n  0.379   0.6160    0.00000000\n  0.380   0.6160    0.00000000\n  0.381   0.6160    0.00000000\n  0.382   0.6160    0.00000000\n  0.383   0.6160    0.00000000\n  0.384   0.6160    0.00000000\n  0.385   0.6160    0.00000000\n  0.386   0.6160    0.00000000\n  0.387   0.6160    0.00000000\n  0.388   0.6160    0.00000000\n  0.389   0.6160    0.00000000\n  0.390   0.6160    0.00000000\n  0.391   0.6160    0.00000000\n  0.392   0.6160    0.00000000\n  0.393   0.6160    0.00000000\n  0.394   0.6160    0.00000000\n  0.395   0.6160    0.00000000\n  0.396   0.6160    0.00000000\n  0.397   0.6160    0.00000000\n  0.398   0.6160    0.00000000\n  0.399   0.6160    0.00000000\n  0.400   0.6160    0.00000000\n  0.401   0.6160    0.00000000\n  0.402   0.6160    0.00000000\n  0.403   0.6160    0.00000000\n  0.404   0.6160    0.00000000\n  0.405   0.6160    0.00000000\n  0.406   0.6160    0.00000000\n  0.407   0.6160    0.00000000\n  0.408   0.6160    0.00000000\n  0.409   0.6160    0.00000000\n  0.410   0.6160    0.00000000\n  0.411   0.6160    0.00000000\n  0.412   0.6160    0.00000000\n  0.413   0.6160    0.00000000\n  0.414   0.6160    0.00000000\n  0.415   0.6160    0.00000000\n  0.416   0.6160    0.00000000\n  0.417   0.6160    0.00000000\n  0.418   0.6160    0.00000000\n  0.419   0.6160    0.00000000\n  0.420   0.6160    0.00000000\n  0.421   0.6160    0.00000000\n  0.422   0.6160    0.00000000\n  0.423   0.6160    0.00000000\n  0.424   0.6160    0.00000000\n  0.425   0.6160    0.00000000\n  0.426   0.6160    0.00000000\n  0.427   0.6160    0.00000000\n  0.428   0.6160    0.00000000\n  0.429   0.6160    0.00000000\n  0.430   0.6160    0.00000000\n  0.431   0.6160    0.00000000\n  0.432   0.6160    0.00000000\n  0.433   0.6160    0.00000000\n  0.434   0.6160    0.00000000\n  0.435   0.6160    0.00000000\n  0.436   0.6160    0.00000000\n  0.437   0.6160    0.00000000\n  0.438   0.6160    0.00000000\n  0.439   0.6160    0.00000000\n  0.440   0.6160    0.00000000\n  0.441   0.6160    0.00000000\n  0.442   0.6160    0.00000000\n  0.443   0.6160    0.00000000\n  0.444   0.6160    0.00000000\n  0.445   0.6160    0.00000000\n  0.446   0.6160    0.00000000\n  0.447   0.6160    0.00000000\n  0.448   0.6160    0.00000000\n  0.449   0.6160    0.00000000\n  0.450   0.6160    0.00000000\n  0.451   0.6160    0.00000000\n  0.452   0.6160    0.00000000\n  0.453   0.6160    0.00000000\n  0.454   0.6160    0.00000000\n  0.455   0.6160    0.00000000\n  0.456   0.6160    0.00000000\n  0.457   0.6160    0.00000000\n  0.458   0.6160    0.00000000\n  0.459   0.6160    0.00000000\n  0.460   0.6160    0.00000000\n  0.461   0.6160    0.00000000\n  0.462   0.6160    0.00000000\n  0.463   0.6160    0.00000000\n  0.464   0.6160    0.00000000\n  0.465   0.6160    0.00000000\n  0.466   0.6160    0.00000000\n  0.467   0.6160    0.00000000\n  0.468   0.6160    0.00000000\n  0.469   0.6160    0.00000000\n  0.470   0.6160    0.00000000\n  0.471   0.6160    0.00000000\n  0.472   0.6160    0.00000000\n  0.473   0.6160    0.00000000\n  0.474   0.6160    0.00000000\n  0.475   0.6160    0.00000000\n  0.476   0.6160    0.00000000\n  0.477   0.6160    0.00000000\n  0.478   0.6160    0.00000000\n  0.479   0.6160    0.00000000\n  0.480   0.6160    0.00000000\n  0.481   0.6160    0.00000000\n  0.482   0.6160    0.00000000\n  0.483   0.6160    0.00000000\n  0.484   0.6160    0.00000000\n  0.485   0.6160    0.00000000\n  0.486   0.6160    0.00000000\n  0.487   0.6160    0.00000000\n  0.488   0.6160    0.00000000\n  0.489   0.6160    0.00000000\n  0.490   0.6160    0.00000000\n  0.491   0.6160    0.00000000\n  0.492   0.6160    0.00000000\n  0.493   0.6160    0.00000000\n  0.494   0.6160    0.00000000\n  0.495   0.6160    0.00000000\n  0.496   0.6160    0.00000000\n  0.497   0.6160    0.00000000\n  0.498   0.6160    0.00000000\n  0.499   0.6160    0.00000000\n  0.500   0.6160    0.00000000\n  0.501   0.6160    0.00000000\n  0.502   0.6160    0.00000000\n  0.503   0.6160    0.00000000\n  0.504   0.6160    0.00000000\n  0.505   0.6160    0.00000000\n  0.506   0.6160    0.00000000\n  0.507   0.6160    0.00000000\n  0.508   0.6160    0.00000000\n  0.509   0.6160    0.00000000\n  0.510   0.6160    0.00000000\n  0.511   0.6160    0.00000000\n  0.512   0.6160    0.00000000\n  0.513   0.6160    0.00000000\n  0.514   0.6160    0.00000000\n  0.515   0.6160    0.00000000\n  0.516   0.6160    0.00000000\n  0.517   0.6160    0.00000000\n  0.518   0.6160    0.00000000\n  0.519   0.6160    0.00000000\n  0.520   0.6160    0.00000000\n  0.521   0.6160    0.00000000\n  0.522   0.6160    0.00000000\n  0.523   0.6160    0.00000000\n  0.524   0.6160    0.00000000\n  0.525   0.6160    0.00000000\n  0.526   0.6160    0.00000000\n  0.527   0.6160    0.00000000\n  0.528   0.6160    0.00000000\n  0.529   0.6160    0.00000000\n  0.530   0.6160    0.00000000\n  0.531   0.6160    0.00000000\n  0.532   0.6160    0.00000000\n  0.533   0.6160    0.00000000\n  0.534   0.6160    0.00000000\n  0.535   0.6160    0.00000000\n  0.536   0.6160    0.00000000\n  0.537   0.6160    0.00000000\n  0.538   0.6160    0.00000000\n  0.539   0.6160    0.00000000\n  0.540   0.6160    0.00000000\n  0.541   0.6160    0.00000000\n  0.542   0.6160    0.00000000\n  0.543   0.6160    0.00000000\n  0.544   0.6160    0.00000000\n  0.545   0.6160    0.00000000\n  0.546   0.6160    0.00000000\n  0.547   0.6160    0.00000000\n  0.548   0.6160    0.00000000\n  0.549   0.6160    0.00000000\n  0.550   0.6160    0.00000000\n  0.551   0.6160    0.00000000\n  0.552   0.6160    0.00000000\n  0.553   0.6160    0.00000000\n  0.554   0.6160    0.00000000\n  0.555   0.6160    0.00000000\n  0.556   0.6160    0.00000000\n  0.557   0.6160    0.00000000\n  0.558   0.6160    0.00000000\n  0.559   0.6160    0.00000000\n  0.560   0.6160    0.00000000\n  0.561   0.6160    0.00000000\n  0.562   0.6160    0.00000000\n  0.563   0.6160    0.00000000\n  0.564   0.6160    0.00000000\n  0.565   0.6160    0.00000000\n  0.566   0.6160    0.00000000\n  0.567   0.6160    0.00000000\n  0.568   0.6160    0.00000000\n  0.569   0.6160    0.00000000\n  0.570   0.6160    0.00000000\n  0.571   0.6160    0.00000000\n  0.572   0.6160    0.00000000\n  0.573   0.6160    0.00000000\n  0.574   0.6160    0.00000000\n  0.575   0.6160    0.00000000\n  0.576   0.6160    0.00000000\n  0.577   0.6160    0.00000000\n  0.578   0.6160    0.00000000\n  0.579   0.6160    0.00000000\n  0.580   0.6160    0.00000000\n  0.581   0.6160    0.00000000\n  0.582   0.6160    0.00000000\n  0.583   0.6160    0.00000000\n  0.584   0.6160    0.00000000\n  0.585   0.6160    0.00000000\n  0.586   0.6160    0.00000000\n  0.587   0.6160    0.00000000\n  0.588   0.6160    0.00000000\n  0.589   0.6160    0.00000000\n  0.590   0.6160    0.00000000\n  0.591   0.6160    0.00000000\n  0.592   0.6160    0.00000000\n  0.593   0.6160    0.00000000\n  0.594   0.6160    0.00000000\n  0.595   0.6160    0.00000000\n  0.596   0.6160    0.00000000\n  0.597   0.6160    0.00000000\n  0.598   0.6160    0.00000000\n  0.599   0.6160    0.00000000\n  0.600   0.6160    0.00000000\n  0.601   0.6160    0.00000000\n  0.602   0.6160    0.00000000\n  0.603   0.6160    0.00000000\n  0.604   0.6160    0.00000000\n  0.605   0.6160    0.00000000\n  0.606   0.6160    0.00000000\n  0.607   0.6160    0.00000000\n  0.608   0.6160    0.00000000\n  0.609   0.6160    0.00000000\n  0.610   0.6160    0.00000000\n  0.611   0.6160    0.00000000\n  0.612   0.6160    0.00000000\n  0.613   0.6160    0.00000000\n  0.614   0.6160    0.00000000\n  0.615   0.6160    0.00000000\n  0.616   0.6160    0.00000000\n  0.617   0.6160    0.00000000\n  0.618   0.6160    0.00000000\n  0.619   0.6160    0.00000000\n  0.620   0.6160    0.00000000\n  0.621   0.6160    0.00000000\n  0.622   0.6160    0.00000000\n  0.623   0.6160    0.00000000\n  0.624   0.6160    0.00000000\n  0.625   0.6160    0.00000000\n  0.626   0.6160    0.00000000\n  0.627   0.6160    0.00000000\n  0.628   0.6160    0.00000000\n  0.629   0.6160    0.00000000\n  0.630   0.6160    0.00000000\n  0.631   0.6160    0.00000000\n  0.632   0.6160    0.00000000\n  0.633   0.6160    0.00000000\n  0.634   0.6160    0.00000000\n  0.635   0.6160    0.00000000\n  0.636   0.6160    0.00000000\n  0.637   0.6160    0.00000000\n  0.638   0.6160    0.00000000\n  0.639   0.6160    0.00000000\n  0.640   0.6160    0.00000000\n  0.641   0.6160    0.00000000\n  0.642   0.6160    0.00000000\n  0.643   0.6160    0.00000000\n  0.644   0.6160    0.00000000\n  0.645   0.6160    0.00000000\n  0.646   0.6160    0.00000000\n  0.647   0.6160    0.00000000\n  0.648   0.6160    0.00000000\n  0.649   0.6160    0.00000000\n  0.650   0.6160    0.00000000\n  0.651   0.6160    0.00000000\n  0.652   0.6160    0.00000000\n  0.653   0.6160    0.00000000\n  0.654   0.6160    0.00000000\n  0.655   0.6160    0.00000000\n  0.656   0.6160    0.00000000\n  0.657   0.6160    0.00000000\n  0.658   0.6160    0.00000000\n  0.659   0.6160    0.00000000\n  0.660   0.6160    0.00000000\n  0.661   0.6160    0.00000000\n  0.662   0.6160    0.00000000\n  0.663   0.6160    0.00000000\n  0.664   0.6160    0.00000000\n  0.665   0.6160    0.00000000\n  0.666   0.6160    0.00000000\n  0.667   0.6160    0.00000000\n  0.668   0.6160    0.00000000\n  0.669   0.6160    0.00000000\n  0.670   0.6160    0.00000000\n  0.671   0.6160    0.00000000\n  0.672   0.6160    0.00000000\n  0.673   0.6160    0.00000000\n  0.674   0.6160    0.00000000\n  0.675   0.6160    0.00000000\n  0.676   0.6160    0.00000000\n  0.677   0.6160    0.00000000\n  0.678   0.6160    0.00000000\n  0.679   0.6160    0.00000000\n  0.680   0.6160    0.00000000\n  0.681   0.6160    0.00000000\n  0.682   0.6160    0.00000000\n  0.683   0.6160    0.00000000\n  0.684   0.6160    0.00000000\n  0.685   0.6160    0.00000000\n  0.686   0.6160    0.00000000\n  0.687   0.6160    0.00000000\n  0.688   0.6160    0.00000000\n  0.689   0.6160    0.00000000\n  0.690   0.6160    0.00000000\n  0.691   0.6160    0.00000000\n  0.692   0.6160    0.00000000\n  0.693   0.6160    0.00000000\n  0.694   0.6160    0.00000000\n  0.695   0.6160    0.00000000\n  0.696   0.6160    0.00000000\n  0.697   0.6160    0.00000000\n  0.698   0.6160    0.00000000\n  0.699   0.6160    0.00000000\n  0.700   0.6160    0.00000000\n  0.701   0.6160    0.00000000\n  0.702   0.6160    0.00000000\n  0.703   0.6160    0.00000000\n  0.704   0.6160    0.00000000\n  0.705   0.6160    0.00000000\n  0.706   0.6160    0.00000000\n  0.707   0.6160    0.00000000\n  0.708   0.6160    0.00000000\n  0.709   0.6160    0.00000000\n  0.710   0.6160    0.00000000\n  0.711   0.6160    0.00000000\n  0.712   0.6160    0.00000000\n  0.713   0.6160    0.00000000\n  0.714   0.6160    0.00000000\n  0.715   0.6160    0.00000000\n  0.716   0.6160    0.00000000\n  0.717   0.6160    0.00000000\n  0.718   0.6160    0.00000000\n  0.719   0.6160    0.00000000\n  0.720   0.6160    0.00000000\n  0.721   0.6160    0.00000000\n  0.722   0.6160    0.00000000\n  0.723   0.6160    0.00000000\n  0.724   0.6160    0.00000000\n  0.725   0.6160    0.00000000\n  0.726   0.6160    0.00000000\n  0.727   0.6160    0.00000000\n  0.728   0.6160    0.00000000\n  0.729   0.6160    0.00000000\n  0.730   0.6160    0.00000000\n  0.731   0.6160    0.00000000\n  0.732   0.6160    0.00000000\n  0.733   0.6160    0.00000000\n  0.734   0.6160    0.00000000\n  0.735   0.6160    0.00000000\n  0.736   0.6160    0.00000000\n  0.737   0.6160    0.00000000\n  0.738   0.6160    0.00000000\n  0.739   0.6160    0.00000000\n  0.740   0.6160    0.00000000\n  0.741   0.6160    0.00000000\n  0.742   0.6160    0.00000000\n  0.743   0.6160    0.00000000\n  0.744   0.6160    0.00000000\n  0.745   0.6160    0.00000000\n  0.746   0.6160    0.00000000\n  0.747   0.6160    0.00000000\n  0.748   0.6160    0.00000000\n  0.749   0.6160    0.00000000\n  0.750   0.6160    0.00000000\n  0.751   0.6160    0.00000000\n  0.752   0.6160    0.00000000\n  0.753   0.6160    0.00000000\n  0.754   0.6160    0.00000000\n  0.755   0.6160    0.00000000\n  0.756   0.6160    0.00000000\n  0.757   0.6160    0.00000000\n  0.758   0.6160    0.00000000\n  0.759   0.6160    0.00000000\n  0.760   0.6160    0.00000000\n  0.761   0.6160    0.00000000\n  0.762   0.6160    0.00000000\n  0.763   0.6160    0.00000000\n  0.764   0.6160    0.00000000\n  0.765   0.6160    0.00000000\n  0.766   0.6160    0.00000000\n  0.767   0.6160    0.00000000\n  0.768   0.6160    0.00000000\n  0.769   0.6160    0.00000000\n  0.770   0.6160    0.00000000\n  0.771   0.6160    0.00000000\n  0.772   0.6160    0.00000000\n  0.773   0.6160    0.00000000\n  0.774   0.6160    0.00000000\n  0.775   0.6160    0.00000000\n  0.776   0.6160    0.00000000\n  0.777   0.6160    0.00000000\n  0.778   0.6160    0.00000000\n  0.779   0.6160    0.00000000\n  0.780   0.6160    0.00000000\n  0.781   0.6160    0.00000000\n  0.782   0.6160    0.00000000\n  0.783   0.6160    0.00000000\n  0.784   0.6160    0.00000000\n  0.785   0.6160    0.00000000\n  0.786   0.6160    0.00000000\n  0.787   0.6160    0.00000000\n  0.788   0.6160    0.00000000\n  0.789   0.6160    0.00000000\n  0.790   0.6160    0.00000000\n  0.791   0.6160    0.00000000\n  0.792   0.6160    0.00000000\n  0.793   0.6160    0.00000000\n  0.794   0.6160    0.00000000\n  0.795   0.6160    0.00000000\n  0.796   0.6160    0.00000000\n  0.797   0.6160    0.00000000\n  0.798   0.6160    0.00000000\n  0.799   0.6160    0.00000000\n  0.800   0.6160    0.00000000\n  0.801   0.6160    0.00000000\n  0.802   0.6160    0.00000000\n  0.803   0.6160    0.00000000\n  0.804   0.6160    0.00000000\n  0.805   0.6160    0.00000000\n  0.806   0.6160    0.00000000\n  0.807   0.6160    0.00000000\n  0.808   0.6160    0.00000000\n  0.809   0.6160    0.00000000\n  0.810   0.6160    0.00000000\n  0.811   0.6160    0.00000000\n  0.812   0.6160    0.00000000\n  0.813   0.6160    0.00000000\n  0.814   0.6160    0.00000000\n  0.815   0.6160    0.00000000\n  0.816   0.6160    0.00000000\n  0.817   0.6160    0.00000000\n  0.818   0.6160    0.00000000\n  0.819   0.6160    0.00000000\n  0.820   0.6160    0.00000000\n  0.821   0.6160    0.00000000\n  0.822   0.6160    0.00000000\n  0.823   0.6160    0.00000000\n  0.824   0.6160    0.00000000\n  0.825   0.6160    0.00000000\n  0.826   0.6160    0.00000000\n  0.827   0.6160    0.00000000\n  0.828   0.6160    0.00000000\n  0.829   0.6160    0.00000000\n  0.830   0.6160    0.00000000\n  0.831   0.6160    0.00000000\n  0.832   0.6160    0.00000000\n  0.833   0.6160    0.00000000\n  0.834   0.6160    0.00000000\n  0.835   0.6160    0.00000000\n  0.836   0.6160    0.00000000\n  0.837   0.6160    0.00000000\n  0.838   0.6160    0.00000000\n  0.839   0.6160    0.00000000\n  0.840   0.6160    0.00000000\n  0.841   0.6160    0.00000000\n  0.842   0.6160    0.00000000\n  0.843   0.6160    0.00000000\n  0.844   0.6160    0.00000000\n  0.845   0.6160    0.00000000\n  0.846   0.6160    0.00000000\n  0.847   0.6160    0.00000000\n  0.848   0.6160    0.00000000\n  0.849   0.6160    0.00000000\n  0.850   0.6160    0.00000000\n  0.851   0.6160    0.00000000\n  0.852   0.6160    0.00000000\n  0.853   0.6160    0.00000000\n  0.854   0.6160    0.00000000\n  0.855   0.6160    0.00000000\n  0.856   0.6160    0.00000000\n  0.857   0.6160    0.00000000\n  0.858   0.6160    0.00000000\n  0.859   0.6160    0.00000000\n  0.860   0.6160    0.00000000\n  0.861   0.6160    0.00000000\n  0.862   0.6160    0.00000000\n  0.863   0.6160    0.00000000\n  0.864   0.6160    0.00000000\n  0.865   0.6160    0.00000000\n  0.866   0.6160    0.00000000\n  0.867   0.6160    0.00000000\n  0.868   0.6160    0.00000000\n  0.869   0.6160    0.00000000\n  0.870   0.6160    0.00000000\n  0.871   0.6160    0.00000000\n  0.872   0.6160    0.00000000\n  0.873   0.6160    0.00000000\n  0.874   0.6160    0.00000000\n  0.875   0.6160    0.00000000\n  0.876   0.6160    0.00000000\n  0.877   0.6160    0.00000000\n  0.878   0.6160    0.00000000\n  0.879   0.6160    0.00000000\n  0.880   0.6160    0.00000000\n  0.881   0.6160    0.00000000\n  0.882   0.6160    0.00000000\n  0.883   0.6160    0.00000000\n  0.884   0.6160    0.00000000\n  0.885   0.6160    0.00000000\n  0.886   0.6160    0.00000000\n  0.887   0.6160    0.00000000\n  0.888   0.6160    0.00000000\n  0.889   0.6160    0.00000000\n  0.890   0.6160    0.00000000\n  0.891   0.6160    0.00000000\n  0.892   0.6160    0.00000000\n  0.893   0.6160    0.00000000\n  0.894   0.6160    0.00000000\n  0.895   0.6160    0.00000000\n  0.896   0.6160    0.00000000\n  0.897   0.6160    0.00000000\n  0.898   0.6160    0.00000000\n  0.899   0.6160    0.00000000\n  0.900   0.6160    0.00000000\n  0.901   0.6160    0.00000000\n  0.902   0.6160    0.00000000\n  0.903   0.6160    0.00000000\n  0.904   0.6160    0.00000000\n  0.905   0.6160    0.00000000\n  0.906   0.6160    0.00000000\n  0.907   0.6160    0.00000000\n  0.908   0.6160    0.00000000\n  0.909   0.6160    0.00000000\n  0.910   0.6160    0.00000000\n  0.911   0.6160    0.00000000\n  0.912   0.6160    0.00000000\n  0.913   0.6160    0.00000000\n  0.914   0.6160    0.00000000\n  0.915   0.6160    0.00000000\n  0.916   0.6160    0.00000000\n  0.917   0.6160    0.00000000\n  0.918   0.6160    0.00000000\n  0.919   0.6160    0.00000000\n  0.920   0.6160    0.00000000\n  0.921   0.6160    0.00000000\n  0.922   0.6160    0.00000000\n  0.923   0.6160    0.00000000\n  0.924   0.6160    0.00000000\n  0.925   0.6160    0.00000000\n  0.926   0.6160    0.00000000\n  0.927   0.6160    0.00000000\n  0.928   0.6160    0.00000000\n  0.929   0.6160    0.00000000\n  0.930   0.6160    0.00000000\n  0.931   0.6160    0.00000000\n  0.932   0.6160    0.00000000\n  0.933   0.6160    0.00000000\n  0.934   0.6160    0.00000000\n  0.935   0.6160    0.00000000\n  0.936   0.6160    0.00000000\n  0.937   0.6160    0.00000000\n  0.938   0.6160    0.00000000\n  0.939   0.6160    0.00000000\n  0.940   0.6160    0.00000000\n  0.941   0.6160    0.00000000\n  0.942   0.6160    0.00000000\n  0.943   0.6160    0.00000000\n  0.944   0.6160    0.00000000\n  0.945   0.6160    0.00000000\n  0.946   0.6160    0.00000000\n  0.947   0.6160    0.00000000\n  0.948   0.6160    0.00000000\n  0.949   0.6160    0.00000000\n  0.950   0.6160    0.00000000\n  0.951   0.6160    0.00000000\n  0.952   0.6160    0.00000000\n  0.953   0.6160    0.00000000\n  0.954   0.6160    0.00000000\n  0.955   0.6160    0.00000000\n  0.956   0.6160    0.00000000\n  0.957   0.6160    0.00000000\n  0.958   0.6160    0.00000000\n  0.959   0.6160    0.00000000\n  0.960   0.6160    0.00000000\n  0.961   0.6160    0.00000000\n  0.962   0.6160    0.00000000\n  0.963   0.6160    0.00000000\n  0.964   0.6160    0.00000000\n  0.965   0.6160    0.00000000\n  0.966   0.6160    0.00000000\n  0.967   0.6160    0.00000000\n  0.968   0.6160    0.00000000\n  0.969   0.6160    0.00000000\n  0.970   0.6160    0.00000000\n  0.971   0.6160    0.00000000\n  0.972   0.6160    0.00000000\n  0.973   0.6160    0.00000000\n  0.974   0.6160    0.00000000\n  0.975   0.6160    0.00000000\n  0.976   0.6160    0.00000000\n  0.977   0.6160    0.00000000\n  0.978   0.6160    0.00000000\n  0.979   0.6160    0.00000000\n  0.980   0.6160    0.00000000\n  0.981   0.6160    0.00000000\n  0.982   0.6160    0.00000000\n  0.983   0.6160    0.00000000\n  0.984   0.6160    0.00000000\n  0.985   0.6160    0.00000000\n  0.986   0.6160    0.00000000\n  0.987   0.6160    0.00000000\n  0.988   0.6160    0.00000000\n  0.989   0.6160    0.00000000\n  0.990   0.6160    0.00000000\n  0.991   0.6160    0.00000000\n  0.992   0.6160    0.00000000\n  0.993   0.6160    0.00000000\n  0.994   0.6160    0.00000000\n  0.995   0.6160    0.00000000\n  0.996   0.6160    0.00000000\n  0.997   0.6160    0.00000000\n  0.998   0.6160    0.00000000\n  0.999   0.6160    0.00000000\n  1.000   0.6160    0.00000000\n\nTuning parameter 'alpha' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 1 and lambda = 0.022.\n\nplot(lasso.fit)                                                       # Plot\n\n\n\n\n\n\n\nlasso.fit$bestTune                                                    # lambda의 최적값\n\n   alpha lambda\n23     1  0.022\n\n\nResult! lambda = 0.022일 때 정확도가 가장 높은 것을 알 수 있으며, lambda = 0.022를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\nround(coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda), 3)       # lambda의 최적값에 대한 회귀계수 추정치\n\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                s1\n(Intercept) -0.601\nPclass2     -0.036\nPclass3     -0.690\nSexmale     -1.052\nAge         -0.193\nFare         0.112\nFamSize     -0.128\n\n\nResult! 데이터 “titanic.trd.Imp”의 Target “Survived”은 “no”와 “yes” 2개의 클래스를 가지며, “Factor” 변환하면 알파벳순으로 수준을 부여하기 때문에 “yes”가 두 번째 클래스가 된다. 즉, “yes”에 속할 확률(= 탑승객이 생존할 확률)을 \\(p\\)라고 할 때, 추정된 회귀계수를 이용하여 다음과 같은 모형식을 얻을 수 있다. \\[\n\\begin{align*}\n\\log{\\frac{p}{1-p}} = &-0.601 - 0.036 Z_{\\text{Pclass2}} - 0.690 Z_{\\text{Pclass3}} -1.052 Z_{\\text{Sexmale}} \\\\\n                      &-0.193 Z_{\\text{Age}} +0.112 Z_{\\text{Fare}} - 0.128 Z_{\\text{FamSize}}\n\\end{align*}\n\\] 여기서, \\(Z_{\\text{예측 변수}}\\)는 표준화한 예측 변수를 의미한다.\n범주형 예측 변수(“Pclass”, “Sex”)는 더미 변환이 수행되었는데, 예를 들어, Pclass2는 탑승객의 티켓 등급이 2등급인 경우 “1”값을 가지고 2등급이 아니면 “0”값을 가진다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "LASSO.html#모형-평가",
    "href": "LASSO.html#모형-평가",
    "title": "7  LASSO Regression",
    "section": "7.7 모형 평가",
    "text": "7.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\ntest.lasso.class &lt;- predict(lasso.fit, \n                            newdata = titanic.ted.Imp[,-1])   # Test Dataset including Only 예측 변수  \n\ntest.lasso.class %&gt;%                                      \n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n7.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.lasso.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")        # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  151  33\n       yes  13  69\n                                          \n               Accuracy : 0.8271          \n                 95% CI : (0.7762, 0.8705)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 6.692e-14       \n                                          \n                  Kappa : 0.6202          \n                                          \n Mcnemar's Test P-Value : 0.005088        \n                                          \n            Sensitivity : 0.6765          \n            Specificity : 0.9207          \n         Pos Pred Value : 0.8415          \n         Neg Pred Value : 0.8207          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2594          \n   Detection Prevalence : 0.3083          \n      Balanced Accuracy : 0.7986          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n7.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.lasso.prob &lt;- predict(lasso.fit, \n                           newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수 \n                           type = \"prob\")                 # 예측 확률 생성\n\ntest.lasso.prob %&gt;%                                                         \n  as_tibble\n\n# A tibble: 266 × 2\n      no   yes\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 0.177 0.823\n 2 0.701 0.299\n 3 0.862 0.138\n 4 0.242 0.758\n 5 0.870 0.130\n 6 0.676 0.324\n 7 0.395 0.605\n 8 0.580 0.420\n 9 0.879 0.121\n10 0.149 0.851\n# ℹ 256 more rows\n\n\n\ntest.lasso.prob &lt;- test.lasso.prob[,2]                 # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                        # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.lasso.prob)                     # 예측 확률을 수치형으로 변환\n\n\n7.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nlasso.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")      # roc(실제 class, 예측 확률)\nauc        &lt;- round(auc(lasso.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(lasso.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(lasso.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n7.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n7.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nlasso.pred &lt;- prediction(pp, ac)                       # prediction(예측 확률, 실제 class) \n\nlasso.perf &lt;- performance(lasso.pred, \"tpr\", \"fpr\")    # performance(, \"민감도\", \"1-특이도\")                      \nplot(lasso.perf, col = \"gray\")                         # ROC Curve\n\nperf.auc   &lt;- performance(lasso.pred, \"auc\")           # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n7.7.3 향상 차트\n\n7.7.3.1 Package “ROCR”\n\nlasso.perf &lt;- performance(lasso.pred, \"lift\", \"rpp\")   # Lift Chart                      \nplot(lasso.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LASSO Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html",
    "href": "Ridge.html",
    "title": "8  Ridge Regression",
    "section": "",
    "text": "8.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정     \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#데이터-전처리-i",
    "href": "Ridge.html#데이터-전처리-i",
    "title": "8  Ridge Regression",
    "section": "8.2 데이터 전처리 I",
    "text": "8.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\ntitanic1 %&gt;%\n  as_tibble\n\n# A tibble: 891 × 6\n   Survived Pclass Sex      Age  Fare FamSize\n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 no       3      male      22  7.25       1\n 2 yes      1      female    38 71.3        1\n 3 yes      3      female    26  7.92       0\n 4 yes      1      female    35 53.1        1\n 5 no       3      male      35  8.05       0\n 6 no       3      male      NA  8.46       0\n 7 no       1      male      54 51.9        0\n 8 no       3      male       2 21.1        4\n 9 yes      3      female    27 11.1        2\n10 yes      2      female    14 30.1        1\n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#데이터-탐색",
    "href": "Ridge.html#데이터-탐색",
    "title": "8  Ridge Regression",
    "section": "8.3 데이터 탐색",
    "text": "8.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#데이터-분할",
    "href": "Ridge.html#데이터-분할",
    "title": "8  Ridge Regression",
    "section": "8.4 데이터 분할",
    "text": "8.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                             # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]                 # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]                # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#데이터-전처리-ii",
    "href": "Ridge.html#데이터-전처리-ii",
    "title": "8  Ridge Regression",
    "section": "8.5 데이터 전처리 II",
    "text": "8.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#모형-훈련",
    "href": "Ridge.html#모형-훈련",
    "title": "8  Ridge Regression",
    "section": "8.6 모형 훈련",
    "text": "8.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"를 통해 Ridge Regression을 수행하기 위해 옵션 method에 다양한 방법(Ex: \"ridge\", \"foba\" 등)을 입력할 수 있지만, 대부분 회귀 문제에 대해서만 분석이 가능하다. 분류와 회귀 문제 모두 가능한 \"glmnet\"을 이용하려면 옵션 tuneGrid = expand.grid()을 통해 탐색하고자 하는 초모수 lambda의 범위를 직접 지정해줘야 한다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,                 # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)                      # 병렬 처리\n\nset.seed(200)                                                         # For CV\nridge.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                   trControl = fitControl ,\n                   method = \"glmnet\",\n                   tuneGrid = expand.grid(alpha = 0,                  # For Ridge Regression\n                                          lambda = seq(0, 1, 0.001)), # lambda의 탐색 범위\n                   preProc = c(\"center\", \"scale\"))                    # Standardization for 예측 변수\n\nridge.fit\n\nglmnet \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  lambda  Accuracy  Kappa    \n  0.000   0.7920    0.5477147\n  0.001   0.7920    0.5477147\n  0.002   0.7920    0.5477147\n  0.003   0.7920    0.5477147\n  0.004   0.7920    0.5477147\n  0.005   0.7920    0.5477147\n  0.006   0.7920    0.5477147\n  0.007   0.7920    0.5477147\n  0.008   0.7920    0.5477147\n  0.009   0.7920    0.5477147\n  0.010   0.7920    0.5477147\n  0.011   0.7920    0.5477147\n  0.012   0.7920    0.5477147\n  0.013   0.7920    0.5477147\n  0.014   0.7920    0.5477147\n  0.015   0.7920    0.5477147\n  0.016   0.7920    0.5477147\n  0.017   0.7920    0.5477147\n  0.018   0.7920    0.5477147\n  0.019   0.7920    0.5477147\n  0.020   0.7920    0.5477147\n  0.021   0.7920    0.5477147\n  0.022   0.7920    0.5477147\n  0.023   0.7920    0.5477147\n  0.024   0.7920    0.5477147\n  0.025   0.7920    0.5477147\n  0.026   0.7920    0.5477147\n  0.027   0.7904    0.5439323\n  0.028   0.7904    0.5439323\n  0.029   0.7904    0.5439323\n  0.030   0.7904    0.5439323\n  0.031   0.7888    0.5408846\n  0.032   0.7888    0.5408846\n  0.033   0.7904    0.5439795\n  0.034   0.7904    0.5439795\n  0.035   0.7904    0.5439795\n  0.036   0.7904    0.5439795\n  0.037   0.7904    0.5439795\n  0.038   0.7904    0.5439795\n  0.039   0.7888    0.5401998\n  0.040   0.7888    0.5401998\n  0.041   0.7888    0.5401998\n  0.042   0.7888    0.5401998\n  0.043   0.7888    0.5401998\n  0.044   0.7888    0.5401998\n  0.045   0.7888    0.5401998\n  0.046   0.7904    0.5432476\n  0.047   0.7904    0.5432476\n  0.048   0.7904    0.5432476\n  0.049   0.7936    0.5495496\n  0.050   0.7952    0.5526223\n  0.051   0.7952    0.5526223\n  0.052   0.7952    0.5526223\n  0.053   0.7984    0.5588816\n  0.054   0.7984    0.5588816\n  0.055   0.7968    0.5550660\n  0.056   0.7968    0.5550660\n  0.057   0.7968    0.5550660\n  0.058   0.7984    0.5582310\n  0.059   0.7984    0.5582310\n  0.060   0.7952    0.5503013\n  0.061   0.7952    0.5494856\n  0.062   0.7952    0.5494856\n  0.063   0.7952    0.5494856\n  0.064   0.7952    0.5494856\n  0.065   0.7952    0.5494856\n  0.066   0.7952    0.5494856\n  0.067   0.7952    0.5494856\n  0.068   0.7936    0.5456362\n  0.069   0.7936    0.5456362\n  0.070   0.7936    0.5456362\n  0.071   0.7920    0.5418538\n  0.072   0.7920    0.5418538\n  0.073   0.7904    0.5380410\n  0.074   0.7904    0.5380410\n  0.075   0.7920    0.5411832\n  0.076   0.7920    0.5411832\n  0.077   0.7920    0.5411832\n  0.078   0.7920    0.5411832\n  0.079   0.7920    0.5411832\n  0.080   0.7920    0.5411832\n  0.081   0.7920    0.5411832\n  0.082   0.7904    0.5373676\n  0.083   0.7904    0.5373676\n  0.084   0.7904    0.5373676\n  0.085   0.7904    0.5373676\n  0.086   0.7920    0.5405558\n  0.087   0.7920    0.5405558\n  0.088   0.7920    0.5405558\n  0.089   0.7920    0.5405558\n  0.090   0.7920    0.5405558\n  0.091   0.7920    0.5392003\n  0.092   0.7936    0.5424629\n  0.093   0.7920    0.5384109\n  0.094   0.7920    0.5384109\n  0.095   0.7920    0.5384109\n  0.096   0.7920    0.5384109\n  0.097   0.7920    0.5384109\n  0.098   0.7920    0.5384109\n  0.099   0.7904    0.5345671\n  0.100   0.7904    0.5345671\n  0.101   0.7920    0.5378571\n  0.102   0.7920    0.5378571\n  0.103   0.7936    0.5410796\n  0.104   0.7936    0.5410796\n  0.105   0.7936    0.5410796\n  0.106   0.7936    0.5410796\n  0.107   0.7936    0.5410796\n  0.108   0.7936    0.5410796\n  0.109   0.7936    0.5410796\n  0.110   0.7904    0.5322904\n  0.111   0.7920    0.5355403\n  0.112   0.7904    0.5316909\n  0.113   0.7904    0.5309466\n  0.114   0.7888    0.5256137\n  0.115   0.7888    0.5256137\n  0.116   0.7904    0.5288914\n  0.117   0.7904    0.5288914\n  0.118   0.7904    0.5288914\n  0.119   0.7904    0.5288914\n  0.120   0.7904    0.5288914\n  0.121   0.7904    0.5288914\n  0.122   0.7904    0.5288914\n  0.123   0.7904    0.5288914\n  0.124   0.7904    0.5288914\n  0.125   0.7888    0.5240297\n  0.126   0.7872    0.5199325\n  0.127   0.7872    0.5199325\n  0.128   0.7872    0.5199325\n  0.129   0.7856    0.5159047\n  0.130   0.7856    0.5159047\n  0.131   0.7856    0.5159047\n  0.132   0.7856    0.5159047\n  0.133   0.7824    0.5078909\n  0.134   0.7824    0.5078909\n  0.135   0.7824    0.5078909\n  0.136   0.7792    0.4998192\n  0.137   0.7808    0.5031216\n  0.138   0.7808    0.5031216\n  0.139   0.7808    0.5031216\n  0.140   0.7808    0.5031216\n  0.141   0.7808    0.5031216\n  0.142   0.7808    0.5031216\n  0.143   0.7792    0.4989892\n  0.144   0.7792    0.4989892\n  0.145   0.7792    0.4989892\n  0.146   0.7792    0.4989892\n  0.147   0.7792    0.4989892\n  0.148   0.7792    0.4989892\n  0.149   0.7792    0.4989892\n  0.150   0.7792    0.4989892\n  0.151   0.7792    0.4989892\n  0.152   0.7792    0.4989892\n  0.153   0.7792    0.4989892\n  0.154   0.7792    0.4989892\n  0.155   0.7792    0.4989892\n  0.156   0.7776    0.4948531\n  0.157   0.7776    0.4948531\n  0.158   0.7760    0.4908685\n  0.159   0.7760    0.4908685\n  0.160   0.7760    0.4908685\n  0.161   0.7760    0.4908685\n  0.162   0.7760    0.4908685\n  0.163   0.7760    0.4908685\n  0.164   0.7760    0.4908685\n  0.165   0.7760    0.4908685\n  0.166   0.7744    0.4867002\n  0.167   0.7712    0.4783236\n  0.168   0.7712    0.4783236\n  0.169   0.7728    0.4814732\n  0.170   0.7744    0.4845893\n  0.171   0.7744    0.4845893\n  0.172   0.7744    0.4845893\n  0.173   0.7744    0.4845893\n  0.174   0.7744    0.4845893\n  0.175   0.7744    0.4845893\n  0.176   0.7744    0.4845893\n  0.177   0.7744    0.4845893\n  0.178   0.7744    0.4845893\n  0.179   0.7744    0.4845893\n  0.180   0.7744    0.4845893\n  0.181   0.7744    0.4845893\n  0.182   0.7728    0.4805782\n  0.183   0.7728    0.4805782\n  0.184   0.7728    0.4795092\n  0.185   0.7728    0.4795092\n  0.186   0.7728    0.4795092\n  0.187   0.7744    0.4827385\n  0.188   0.7760    0.4859375\n  0.189   0.7760    0.4859375\n  0.190   0.7760    0.4848850\n  0.191   0.7760    0.4848850\n  0.192   0.7760    0.4848850\n  0.193   0.7760    0.4848850\n  0.194   0.7760    0.4848850\n  0.195   0.7760    0.4848850\n  0.196   0.7744    0.4808102\n  0.197   0.7728    0.4767011\n  0.198   0.7728    0.4767011\n  0.199   0.7728    0.4767011\n  0.200   0.7728    0.4767011\n  0.201   0.7728    0.4767011\n  0.202   0.7712    0.4726796\n  0.203   0.7712    0.4726796\n  0.204   0.7712    0.4726796\n  0.205   0.7712    0.4726796\n  0.206   0.7728    0.4758139\n  0.207   0.7728    0.4758139\n  0.208   0.7728    0.4758139\n  0.209   0.7728    0.4758139\n  0.210   0.7728    0.4758139\n  0.211   0.7728    0.4758139\n  0.212   0.7728    0.4758139\n  0.213   0.7728    0.4758139\n  0.214   0.7728    0.4750117\n  0.215   0.7728    0.4750117\n  0.216   0.7728    0.4750117\n  0.217   0.7728    0.4750117\n  0.218   0.7744    0.4781725\n  0.219   0.7744    0.4781725\n  0.220   0.7760    0.4814224\n  0.221   0.7760    0.4814224\n  0.222   0.7760    0.4814224\n  0.223   0.7760    0.4814224\n  0.224   0.7760    0.4814224\n  0.225   0.7760    0.4814224\n  0.226   0.7744    0.4773635\n  0.227   0.7744    0.4773635\n  0.228   0.7760    0.4805512\n  0.229   0.7760    0.4805512\n  0.230   0.7760    0.4805512\n  0.231   0.7760    0.4805512\n  0.232   0.7760    0.4805512\n  0.233   0.7776    0.4837662\n  0.234   0.7776    0.4837662\n  0.235   0.7776    0.4837662\n  0.236   0.7776    0.4837662\n  0.237   0.7776    0.4837662\n  0.238   0.7776    0.4837662\n  0.239   0.7776    0.4837662\n  0.240   0.7776    0.4837662\n  0.241   0.7776    0.4837662\n  0.242   0.7760    0.4795577\n  0.243   0.7760    0.4795577\n  0.244   0.7760    0.4795577\n  0.245   0.7760    0.4795577\n  0.246   0.7760    0.4795577\n  0.247   0.7760    0.4795577\n  0.248   0.7776    0.4828354\n  0.249   0.7776    0.4828354\n  0.250   0.7776    0.4828354\n  0.251   0.7776    0.4828354\n  0.252   0.7776    0.4828354\n  0.253   0.7776    0.4828354\n  0.254   0.7776    0.4828354\n  0.255   0.7776    0.4828354\n  0.256   0.7776    0.4828354\n  0.257   0.7776    0.4828354\n  0.258   0.7776    0.4828354\n  0.259   0.7776    0.4828354\n  0.260   0.7776    0.4828354\n  0.261   0.7776    0.4828354\n  0.262   0.7776    0.4828354\n  0.263   0.7776    0.4828354\n  0.264   0.7776    0.4828354\n  0.265   0.7776    0.4828354\n  0.266   0.7792    0.4860780\n  0.267   0.7792    0.4860780\n  0.268   0.7760    0.4778483\n  0.269   0.7760    0.4778483\n  0.270   0.7760    0.4778483\n  0.271   0.7760    0.4778483\n  0.272   0.7760    0.4778483\n  0.273   0.7760    0.4778483\n  0.274   0.7760    0.4778483\n  0.275   0.7760    0.4778483\n  0.276   0.7760    0.4778483\n  0.277   0.7760    0.4778483\n  0.278   0.7760    0.4778483\n  0.279   0.7760    0.4778483\n  0.280   0.7744    0.4737547\n  0.281   0.7744    0.4737547\n  0.282   0.7744    0.4737547\n  0.283   0.7744    0.4737547\n  0.284   0.7744    0.4737547\n  0.285   0.7744    0.4737547\n  0.286   0.7744    0.4737547\n  0.287   0.7744    0.4737547\n  0.288   0.7744    0.4737547\n  0.289   0.7744    0.4737547\n  0.290   0.7744    0.4737547\n  0.291   0.7744    0.4737547\n  0.292   0.7744    0.4737547\n  0.293   0.7744    0.4737547\n  0.294   0.7744    0.4737547\n  0.295   0.7744    0.4737547\n  0.296   0.7744    0.4737547\n  0.297   0.7744    0.4737547\n  0.298   0.7744    0.4737547\n  0.299   0.7744    0.4737547\n  0.300   0.7744    0.4737547\n  0.301   0.7744    0.4737547\n  0.302   0.7744    0.4737547\n  0.303   0.7744    0.4737547\n  0.304   0.7744    0.4737547\n  0.305   0.7744    0.4737547\n  0.306   0.7744    0.4737547\n  0.307   0.7744    0.4737547\n  0.308   0.7744    0.4737547\n  0.309   0.7744    0.4737547\n  0.310   0.7744    0.4737547\n  0.311   0.7728    0.4695864\n  0.312   0.7728    0.4695864\n  0.313   0.7728    0.4695864\n  0.314   0.7728    0.4695864\n  0.315   0.7728    0.4695864\n  0.316   0.7728    0.4695864\n  0.317   0.7728    0.4695864\n  0.318   0.7728    0.4695864\n  0.319   0.7728    0.4695864\n  0.320   0.7728    0.4695864\n  0.321   0.7712    0.4654220\n  0.322   0.7712    0.4654220\n  0.323   0.7712    0.4654220\n  0.324   0.7712    0.4654220\n  0.325   0.7712    0.4654220\n  0.326   0.7712    0.4654220\n  0.327   0.7712    0.4654220\n  0.328   0.7712    0.4654220\n  0.329   0.7712    0.4654220\n  0.330   0.7712    0.4654220\n  0.331   0.7712    0.4654220\n  0.332   0.7712    0.4654220\n  0.333   0.7712    0.4654220\n  0.334   0.7712    0.4654220\n  0.335   0.7712    0.4654220\n  0.336   0.7712    0.4654220\n  0.337   0.7712    0.4654220\n  0.338   0.7712    0.4654220\n  0.339   0.7712    0.4654220\n  0.340   0.7712    0.4654220\n  0.341   0.7712    0.4654220\n  0.342   0.7712    0.4654220\n  0.343   0.7712    0.4654220\n  0.344   0.7712    0.4654220\n  0.345   0.7712    0.4654220\n  0.346   0.7712    0.4654220\n  0.347   0.7712    0.4654220\n  0.348   0.7712    0.4654220\n  0.349   0.7712    0.4654220\n  0.350   0.7712    0.4654220\n  0.351   0.7728    0.4686887\n  0.352   0.7728    0.4686887\n  0.353   0.7728    0.4686887\n  0.354   0.7728    0.4686887\n  0.355   0.7728    0.4686887\n  0.356   0.7728    0.4686887\n  0.357   0.7728    0.4686887\n  0.358   0.7728    0.4686887\n  0.359   0.7728    0.4686887\n  0.360   0.7728    0.4686887\n  0.361   0.7728    0.4686887\n  0.362   0.7728    0.4686887\n  0.363   0.7728    0.4686887\n  0.364   0.7728    0.4686887\n  0.365   0.7728    0.4686887\n  0.366   0.7728    0.4686887\n  0.367   0.7728    0.4686887\n  0.368   0.7728    0.4686887\n  0.369   0.7728    0.4686887\n  0.370   0.7728    0.4686887\n  0.371   0.7728    0.4686887\n  0.372   0.7728    0.4686887\n  0.373   0.7728    0.4686887\n  0.374   0.7728    0.4686887\n  0.375   0.7728    0.4686887\n  0.376   0.7728    0.4686887\n  0.377   0.7728    0.4686887\n  0.378   0.7728    0.4686887\n  0.379   0.7728    0.4686887\n  0.380   0.7728    0.4686887\n  0.381   0.7728    0.4686887\n  0.382   0.7728    0.4686887\n  0.383   0.7728    0.4686887\n  0.384   0.7728    0.4686887\n  0.385   0.7728    0.4686887\n  0.386   0.7728    0.4686887\n  0.387   0.7728    0.4686887\n  0.388   0.7728    0.4686887\n  0.389   0.7728    0.4686887\n  0.390   0.7728    0.4686887\n  0.391   0.7728    0.4686887\n  0.392   0.7728    0.4686887\n  0.393   0.7728    0.4686887\n  0.394   0.7728    0.4686887\n  0.395   0.7728    0.4686887\n  0.396   0.7728    0.4686887\n  0.397   0.7728    0.4686887\n  0.398   0.7728    0.4686887\n  0.399   0.7728    0.4686887\n  0.400   0.7728    0.4686887\n  0.401   0.7728    0.4686887\n  0.402   0.7728    0.4686887\n  0.403   0.7728    0.4686887\n  0.404   0.7728    0.4686887\n  0.405   0.7728    0.4686887\n  0.406   0.7728    0.4686887\n  0.407   0.7728    0.4686887\n  0.408   0.7728    0.4686887\n  0.409   0.7728    0.4686887\n  0.410   0.7728    0.4686887\n  0.411   0.7728    0.4686887\n  0.412   0.7728    0.4686887\n  0.413   0.7728    0.4686887\n  0.414   0.7728    0.4686887\n  0.415   0.7728    0.4686887\n  0.416   0.7728    0.4686887\n  0.417   0.7728    0.4686887\n  0.418   0.7728    0.4686887\n  0.419   0.7728    0.4686887\n  0.420   0.7728    0.4686887\n  0.421   0.7728    0.4686887\n  0.422   0.7728    0.4686887\n  0.423   0.7728    0.4686887\n  0.424   0.7728    0.4686887\n  0.425   0.7728    0.4686887\n  0.426   0.7728    0.4686887\n  0.427   0.7728    0.4686887\n  0.428   0.7728    0.4686887\n  0.429   0.7728    0.4686887\n  0.430   0.7728    0.4686887\n  0.431   0.7728    0.4686887\n  0.432   0.7728    0.4686887\n  0.433   0.7728    0.4686887\n  0.434   0.7728    0.4686887\n  0.435   0.7728    0.4686887\n  0.436   0.7728    0.4686887\n  0.437   0.7728    0.4686887\n  0.438   0.7728    0.4686887\n  0.439   0.7728    0.4686887\n  0.440   0.7728    0.4686887\n  0.441   0.7728    0.4686887\n  0.442   0.7728    0.4686887\n  0.443   0.7728    0.4686887\n  0.444   0.7728    0.4686887\n  0.445   0.7728    0.4686887\n  0.446   0.7728    0.4686887\n  0.447   0.7728    0.4686887\n  0.448   0.7728    0.4686887\n  0.449   0.7728    0.4686887\n  0.450   0.7728    0.4686887\n  0.451   0.7728    0.4686887\n  0.452   0.7728    0.4686887\n  0.453   0.7728    0.4686887\n  0.454   0.7728    0.4686887\n  0.455   0.7728    0.4686887\n  0.456   0.7728    0.4686887\n  0.457   0.7728    0.4686887\n  0.458   0.7728    0.4686887\n  0.459   0.7728    0.4686887\n  0.460   0.7728    0.4686887\n  0.461   0.7728    0.4686887\n  0.462   0.7728    0.4686887\n  0.463   0.7728    0.4686887\n  0.464   0.7728    0.4686887\n  0.465   0.7728    0.4686887\n  0.466   0.7712    0.4644432\n  0.467   0.7712    0.4644432\n  0.468   0.7712    0.4644432\n  0.469   0.7712    0.4644432\n  0.470   0.7712    0.4644432\n  0.471   0.7712    0.4644432\n  0.472   0.7712    0.4644432\n  0.473   0.7712    0.4644432\n  0.474   0.7712    0.4644432\n  0.475   0.7712    0.4644432\n  0.476   0.7712    0.4644432\n  0.477   0.7712    0.4644432\n  0.478   0.7712    0.4644432\n  0.479   0.7712    0.4644432\n  0.480   0.7712    0.4644432\n  0.481   0.7712    0.4644432\n  0.482   0.7712    0.4644432\n  0.483   0.7712    0.4644432\n  0.484   0.7712    0.4644432\n  0.485   0.7712    0.4644432\n  0.486   0.7712    0.4644432\n  0.487   0.7712    0.4644432\n  0.488   0.7712    0.4644432\n  0.489   0.7712    0.4644432\n  0.490   0.7712    0.4644432\n  0.491   0.7712    0.4644432\n  0.492   0.7712    0.4644432\n  0.493   0.7712    0.4644432\n  0.494   0.7712    0.4644432\n  0.495   0.7712    0.4644432\n  0.496   0.7712    0.4644432\n  0.497   0.7712    0.4644432\n  0.498   0.7712    0.4644432\n  0.499   0.7712    0.4644432\n  0.500   0.7712    0.4644432\n  0.501   0.7712    0.4644432\n  0.502   0.7712    0.4644432\n  0.503   0.7712    0.4644432\n  0.504   0.7712    0.4644432\n  0.505   0.7712    0.4644432\n  0.506   0.7712    0.4644432\n  0.507   0.7712    0.4644432\n  0.508   0.7712    0.4644432\n  0.509   0.7728    0.4677383\n  0.510   0.7728    0.4677383\n  0.511   0.7728    0.4677383\n  0.512   0.7728    0.4677383\n  0.513   0.7728    0.4677383\n  0.514   0.7728    0.4677383\n  0.515   0.7728    0.4677383\n  0.516   0.7728    0.4677383\n  0.517   0.7728    0.4677383\n  0.518   0.7744    0.4710334\n  0.519   0.7744    0.4710334\n  0.520   0.7744    0.4710334\n  0.521   0.7744    0.4710334\n  0.522   0.7744    0.4710334\n  0.523   0.7744    0.4710334\n  0.524   0.7744    0.4710334\n  0.525   0.7744    0.4710334\n  0.526   0.7744    0.4710334\n  0.527   0.7744    0.4710334\n  0.528   0.7744    0.4710334\n  0.529   0.7744    0.4710334\n  0.530   0.7744    0.4710334\n  0.531   0.7744    0.4710334\n  0.532   0.7744    0.4710334\n  0.533   0.7744    0.4710334\n  0.534   0.7744    0.4710334\n  0.535   0.7744    0.4710334\n  0.536   0.7744    0.4710334\n  0.537   0.7744    0.4710334\n  0.538   0.7744    0.4710334\n  0.539   0.7744    0.4710334\n  0.540   0.7744    0.4710334\n  0.541   0.7744    0.4710334\n  0.542   0.7744    0.4710334\n  0.543   0.7744    0.4710334\n  0.544   0.7744    0.4710334\n  0.545   0.7744    0.4710334\n  0.546   0.7744    0.4710334\n  0.547   0.7744    0.4710334\n  0.548   0.7744    0.4710334\n  0.549   0.7744    0.4710334\n  0.550   0.7744    0.4710334\n  0.551   0.7744    0.4710334\n  0.552   0.7744    0.4710334\n  0.553   0.7744    0.4710334\n  0.554   0.7728    0.4668249\n  0.555   0.7728    0.4668249\n  0.556   0.7728    0.4668249\n  0.557   0.7728    0.4668249\n  0.558   0.7728    0.4668249\n  0.559   0.7728    0.4668249\n  0.560   0.7728    0.4668249\n  0.561   0.7728    0.4668249\n  0.562   0.7728    0.4668249\n  0.563   0.7744    0.4700991\n  0.564   0.7744    0.4700991\n  0.565   0.7744    0.4700991\n  0.566   0.7744    0.4700991\n  0.567   0.7744    0.4700991\n  0.568   0.7744    0.4700991\n  0.569   0.7744    0.4700991\n  0.570   0.7728    0.4658906\n  0.571   0.7728    0.4658906\n  0.572   0.7728    0.4658906\n  0.573   0.7728    0.4658906\n  0.574   0.7728    0.4658906\n  0.575   0.7728    0.4658906\n  0.576   0.7728    0.4658906\n  0.577   0.7728    0.4658906\n  0.578   0.7728    0.4658906\n  0.579   0.7728    0.4658906\n  0.580   0.7728    0.4658906\n  0.581   0.7728    0.4658906\n  0.582   0.7728    0.4658906\n  0.583   0.7728    0.4658906\n  0.584   0.7728    0.4658906\n  0.585   0.7728    0.4658906\n  0.586   0.7728    0.4658906\n  0.587   0.7728    0.4658906\n  0.588   0.7728    0.4658906\n  0.589   0.7728    0.4658906\n  0.590   0.7728    0.4658906\n  0.591   0.7728    0.4658906\n  0.592   0.7728    0.4658906\n  0.593   0.7728    0.4658906\n  0.594   0.7728    0.4658906\n  0.595   0.7728    0.4658906\n  0.596   0.7728    0.4658906\n  0.597   0.7728    0.4658906\n  0.598   0.7728    0.4658906\n  0.599   0.7728    0.4658906\n  0.600   0.7728    0.4658906\n  0.601   0.7728    0.4658906\n  0.602   0.7728    0.4658906\n  0.603   0.7728    0.4658906\n  0.604   0.7728    0.4658906\n  0.605   0.7712    0.4616452\n  0.606   0.7712    0.4616452\n  0.607   0.7712    0.4616452\n  0.608   0.7712    0.4616452\n  0.609   0.7712    0.4616452\n  0.610   0.7712    0.4616452\n  0.611   0.7712    0.4616452\n  0.612   0.7712    0.4616452\n  0.613   0.7712    0.4616452\n  0.614   0.7712    0.4616452\n  0.615   0.7712    0.4616452\n  0.616   0.7712    0.4616452\n  0.617   0.7712    0.4616452\n  0.618   0.7712    0.4616452\n  0.619   0.7712    0.4616452\n  0.620   0.7712    0.4616452\n  0.621   0.7712    0.4616452\n  0.622   0.7712    0.4616452\n  0.623   0.7712    0.4616452\n  0.624   0.7696    0.4573997\n  0.625   0.7712    0.4607022\n  0.626   0.7712    0.4607022\n  0.627   0.7712    0.4607022\n  0.628   0.7712    0.4607022\n  0.629   0.7712    0.4607022\n  0.630   0.7712    0.4607022\n  0.631   0.7712    0.4607022\n  0.632   0.7712    0.4607022\n  0.633   0.7712    0.4607022\n  0.634   0.7712    0.4607022\n  0.635   0.7712    0.4607022\n  0.636   0.7712    0.4607022\n  0.637   0.7712    0.4607022\n  0.638   0.7712    0.4607022\n  0.639   0.7712    0.4607022\n  0.640   0.7712    0.4607022\n  0.641   0.7712    0.4607022\n  0.642   0.7696    0.4564193\n  0.643   0.7696    0.4564193\n  0.644   0.7696    0.4564193\n  0.645   0.7696    0.4564193\n  0.646   0.7696    0.4564193\n  0.647   0.7696    0.4564193\n  0.648   0.7696    0.4564193\n  0.649   0.7696    0.4564193\n  0.650   0.7696    0.4564193\n  0.651   0.7696    0.4564193\n  0.652   0.7696    0.4564193\n  0.653   0.7696    0.4564193\n  0.654   0.7696    0.4564193\n  0.655   0.7696    0.4564193\n  0.656   0.7696    0.4564193\n  0.657   0.7696    0.4564193\n  0.658   0.7696    0.4564193\n  0.659   0.7696    0.4564193\n  0.660   0.7696    0.4564193\n  0.661   0.7696    0.4564193\n  0.662   0.7680    0.4522832\n  0.663   0.7680    0.4522832\n  0.664   0.7680    0.4522832\n  0.665   0.7680    0.4522832\n  0.666   0.7680    0.4522832\n  0.667   0.7680    0.4522832\n  0.668   0.7680    0.4522832\n  0.669   0.7680    0.4522832\n  0.670   0.7680    0.4522832\n  0.671   0.7680    0.4522832\n  0.672   0.7680    0.4522832\n  0.673   0.7680    0.4522832\n  0.674   0.7680    0.4522832\n  0.675   0.7680    0.4522832\n  0.676   0.7680    0.4522832\n  0.677   0.7680    0.4522832\n  0.678   0.7680    0.4522832\n  0.679   0.7680    0.4522832\n  0.680   0.7680    0.4522832\n  0.681   0.7680    0.4522832\n  0.682   0.7680    0.4522832\n  0.683   0.7680    0.4522832\n  0.684   0.7680    0.4522832\n  0.685   0.7680    0.4522832\n  0.686   0.7680    0.4522832\n  0.687   0.7680    0.4522832\n  0.688   0.7680    0.4522832\n  0.689   0.7680    0.4522832\n  0.690   0.7664    0.4480004\n  0.691   0.7664    0.4480004\n  0.692   0.7664    0.4480004\n  0.693   0.7648    0.4438283\n  0.694   0.7648    0.4438283\n  0.695   0.7648    0.4438283\n  0.696   0.7648    0.4438283\n  0.697   0.7648    0.4438283\n  0.698   0.7648    0.4438283\n  0.699   0.7632    0.4395075\n  0.700   0.7632    0.4395075\n  0.701   0.7632    0.4395075\n  0.702   0.7632    0.4395075\n  0.703   0.7632    0.4395075\n  0.704   0.7616    0.4351483\n  0.705   0.7616    0.4351483\n  0.706   0.7616    0.4351483\n  0.707   0.7616    0.4351483\n  0.708   0.7600    0.4307502\n  0.709   0.7600    0.4307502\n  0.710   0.7600    0.4307502\n  0.711   0.7600    0.4307502\n  0.712   0.7584    0.4263125\n  0.713   0.7584    0.4263125\n  0.714   0.7584    0.4263125\n  0.715   0.7584    0.4263125\n  0.716   0.7584    0.4263125\n  0.717   0.7584    0.4263125\n  0.718   0.7584    0.4263125\n  0.719   0.7568    0.4221040\n  0.720   0.7568    0.4221040\n  0.721   0.7568    0.4221040\n  0.722   0.7568    0.4221040\n  0.723   0.7568    0.4221040\n  0.724   0.7568    0.4221040\n  0.725   0.7568    0.4221040\n  0.726   0.7568    0.4221040\n  0.727   0.7568    0.4221040\n  0.728   0.7568    0.4221040\n  0.729   0.7552    0.4177832\n  0.730   0.7552    0.4177832\n  0.731   0.7568    0.4210756\n  0.732   0.7568    0.4210756\n  0.733   0.7568    0.4210756\n  0.734   0.7568    0.4210756\n  0.735   0.7568    0.4210756\n  0.736   0.7568    0.4210756\n  0.737   0.7568    0.4210756\n  0.738   0.7568    0.4210756\n  0.739   0.7568    0.4210756\n  0.740   0.7568    0.4210756\n  0.741   0.7568    0.4210756\n  0.742   0.7568    0.4210756\n  0.743   0.7568    0.4210756\n  0.744   0.7568    0.4210756\n  0.745   0.7568    0.4210756\n  0.746   0.7568    0.4210756\n  0.747   0.7552    0.4167164\n  0.748   0.7552    0.4167164\n  0.749   0.7552    0.4167164\n  0.750   0.7552    0.4167164\n  0.751   0.7536    0.4122335\n  0.752   0.7536    0.4122335\n  0.753   0.7536    0.4122335\n  0.754   0.7536    0.4122335\n  0.755   0.7520    0.4078353\n  0.756   0.7520    0.4078353\n  0.757   0.7520    0.4078353\n  0.758   0.7520    0.4078353\n  0.759   0.7520    0.4078353\n  0.760   0.7520    0.4078353\n  0.761   0.7488    0.3991522\n  0.762   0.7488    0.3991522\n  0.763   0.7488    0.3991522\n  0.764   0.7472    0.3946285\n  0.765   0.7472    0.3946285\n  0.766   0.7456    0.3903123\n  0.767   0.7440    0.3860294\n  0.768   0.7440    0.3860294\n  0.769   0.7440    0.3860294\n  0.770   0.7440    0.3860294\n  0.771   0.7440    0.3860294\n  0.772   0.7408    0.3768573\n  0.773   0.7408    0.3768573\n  0.774   0.7392    0.3725366\n  0.775   0.7392    0.3725366\n  0.776   0.7392    0.3725366\n  0.777   0.7392    0.3725366\n  0.778   0.7376    0.3678871\n  0.779   0.7376    0.3678871\n  0.780   0.7376    0.3678871\n  0.781   0.7376    0.3678871\n  0.782   0.7376    0.3678871\n  0.783   0.7360    0.3635279\n  0.784   0.7360    0.3635279\n  0.785   0.7360    0.3635279\n  0.786   0.7360    0.3635279\n  0.787   0.7360    0.3635279\n  0.788   0.7360    0.3635279\n  0.789   0.7360    0.3635279\n  0.790   0.7344    0.3591735\n  0.791   0.7344    0.3591735\n  0.792   0.7344    0.3591735\n  0.793   0.7344    0.3591735\n  0.794   0.7344    0.3591735\n  0.795   0.7344    0.3591735\n  0.796   0.7328    0.3546959\n  0.797   0.7328    0.3546959\n  0.798   0.7344    0.3580118\n  0.799   0.7344    0.3580118\n  0.800   0.7328    0.3536136\n  0.801   0.7328    0.3536136\n  0.802   0.7312    0.3492205\n  0.803   0.7312    0.3492205\n  0.804   0.7312    0.3492205\n  0.805   0.7312    0.3492205\n  0.806   0.7312    0.3492205\n  0.807   0.7312    0.3492205\n  0.808   0.7312    0.3492205\n  0.809   0.7312    0.3492205\n  0.810   0.7296    0.3447829\n  0.811   0.7296    0.3447829\n  0.812   0.7296    0.3447829\n  0.813   0.7280    0.3400903\n  0.814   0.7280    0.3400903\n  0.815   0.7280    0.3400903\n  0.816   0.7280    0.3400903\n  0.817   0.7264    0.3358032\n  0.818   0.7264    0.3358032\n  0.819   0.7264    0.3358032\n  0.820   0.7264    0.3358032\n  0.821   0.7264    0.3358032\n  0.822   0.7264    0.3358032\n  0.823   0.7248    0.3312850\n  0.824   0.7248    0.3312850\n  0.825   0.7248    0.3312850\n  0.826   0.7232    0.3267258\n  0.827   0.7232    0.3267258\n  0.828   0.7232    0.3267258\n  0.829   0.7232    0.3267258\n  0.830   0.7232    0.3267258\n  0.831   0.7216    0.3221249\n  0.832   0.7216    0.3221249\n  0.833   0.7216    0.3221249\n  0.834   0.7216    0.3221249\n  0.835   0.7216    0.3221249\n  0.836   0.7216    0.3221249\n  0.837   0.7216    0.3221249\n  0.838   0.7216    0.3221249\n  0.839   0.7216    0.3221249\n  0.840   0.7216    0.3221249\n  0.841   0.7216    0.3221249\n  0.842   0.7216    0.3221249\n  0.843   0.7184    0.3131291\n  0.844   0.7184    0.3131291\n  0.845   0.7184    0.3131291\n  0.846   0.7184    0.3131291\n  0.847   0.7152    0.3038337\n  0.848   0.7152    0.3038337\n  0.849   0.7136    0.2994013\n  0.850   0.7120    0.2950760\n  0.851   0.7120    0.2950760\n  0.852   0.7120    0.2950760\n  0.853   0.7120    0.2950760\n  0.854   0.7120    0.2950760\n  0.855   0.7120    0.2950760\n  0.856   0.7120    0.2950760\n  0.857   0.7120    0.2950760\n  0.858   0.7104    0.2906038\n  0.859   0.7104    0.2906038\n  0.860   0.7104    0.2906038\n  0.861   0.7104    0.2906038\n  0.862   0.7104    0.2906038\n  0.863   0.7088    0.2858233\n  0.864   0.7088    0.2858233\n  0.865   0.7088    0.2858233\n  0.866   0.7072    0.2813109\n  0.867   0.7072    0.2813109\n  0.868   0.7072    0.2813109\n  0.869   0.7072    0.2813109\n  0.870   0.7072    0.2813109\n  0.871   0.7072    0.2813109\n  0.872   0.7072    0.2813109\n  0.873   0.7072    0.2813109\n  0.874   0.7072    0.2813109\n  0.875   0.7072    0.2813109\n  0.876   0.7072    0.2813109\n  0.877   0.7072    0.2813109\n  0.878   0.7056    0.2769470\n  0.879   0.7056    0.2769470\n  0.880   0.7056    0.2769470\n  0.881   0.7056    0.2769470\n  0.882   0.7056    0.2769470\n  0.883   0.7056    0.2769470\n  0.884   0.7040    0.2723038\n  0.885   0.7040    0.2723038\n  0.886   0.7040    0.2723038\n  0.887   0.7040    0.2723038\n  0.888   0.7040    0.2723038\n  0.889   0.7040    0.2723038\n  0.890   0.7040    0.2723038\n  0.891   0.7040    0.2723038\n  0.892   0.7040    0.2723038\n  0.893   0.7056    0.2755557\n  0.894   0.7072    0.2788371\n  0.895   0.7040    0.2695861\n  0.896   0.7040    0.2695861\n  0.897   0.7040    0.2695861\n  0.898   0.7024    0.2651830\n  0.899   0.7024    0.2651830\n  0.900   0.7024    0.2651830\n  0.901   0.7024    0.2651830\n  0.902   0.7024    0.2651830\n  0.903   0.7024    0.2651830\n  0.904   0.7024    0.2651830\n  0.905   0.7008    0.2607403\n  0.906   0.7008    0.2607403\n  0.907   0.7008    0.2607403\n  0.908   0.6992    0.2561394\n  0.909   0.6992    0.2561394\n  0.910   0.6976    0.2515324\n  0.911   0.6976    0.2515324\n  0.912   0.6976    0.2515324\n  0.913   0.6976    0.2515324\n  0.914   0.6960    0.2468030\n  0.915   0.6960    0.2468030\n  0.916   0.6960    0.2468030\n  0.917   0.6960    0.2468030\n  0.918   0.6960    0.2468030\n  0.919   0.6960    0.2468030\n  0.920   0.6960    0.2468030\n  0.921   0.6960    0.2468030\n  0.922   0.6944    0.2419776\n  0.923   0.6944    0.2419776\n  0.924   0.6944    0.2419776\n  0.925   0.6944    0.2419776\n  0.926   0.6944    0.2419776\n  0.927   0.6944    0.2419776\n  0.928   0.6928    0.2374946\n  0.929   0.6912    0.2328515\n  0.930   0.6896    0.2281656\n  0.931   0.6896    0.2281656\n  0.932   0.6896    0.2281656\n  0.933   0.6896    0.2281656\n  0.934   0.6896    0.2281656\n  0.935   0.6896    0.2281656\n  0.936   0.6896    0.2281656\n  0.937   0.6896    0.2281656\n  0.938   0.6896    0.2281656\n  0.939   0.6896    0.2281656\n  0.940   0.6896    0.2281656\n  0.941   0.6880    0.2233922\n  0.942   0.6880    0.2233922\n  0.943   0.6880    0.2233922\n  0.944   0.6880    0.2233922\n  0.945   0.6880    0.2233922\n  0.946   0.6880    0.2233922\n  0.947   0.6880    0.2233922\n  0.948   0.6864    0.2186628\n  0.949   0.6864    0.2186628\n  0.950   0.6848    0.2141391\n  0.951   0.6848    0.2141391\n  0.952   0.6832    0.2093211\n  0.953   0.6832    0.2093211\n  0.954   0.6832    0.2093211\n  0.955   0.6832    0.2093211\n  0.956   0.6832    0.2093211\n  0.957   0.6832    0.2093211\n  0.958   0.6832    0.2093211\n  0.959   0.6832    0.2093211\n  0.960   0.6832    0.2093211\n  0.961   0.6832    0.2093211\n  0.962   0.6832    0.2093211\n  0.963   0.6832    0.2093211\n  0.964   0.6816    0.2046716\n  0.965   0.6816    0.2046716\n  0.966   0.6816    0.2046716\n  0.967   0.6816    0.2046716\n  0.968   0.6816    0.2046716\n  0.969   0.6816    0.2046716\n  0.970   0.6816    0.2046716\n  0.971   0.6816    0.2046716\n  0.972   0.6816    0.2046716\n  0.973   0.6800    0.1998983\n  0.974   0.6784    0.1953332\n  0.975   0.6784    0.1953332\n  0.976   0.6768    0.1906406\n  0.977   0.6768    0.1906406\n  0.978   0.6768    0.1906406\n  0.979   0.6768    0.1906406\n  0.980   0.6768    0.1906406\n  0.981   0.6768    0.1906406\n  0.982   0.6768    0.1906406\n  0.983   0.6768    0.1906406\n  0.984   0.6768    0.1906406\n  0.985   0.6752    0.1857774\n  0.986   0.6752    0.1857774\n  0.987   0.6752    0.1857774\n  0.988   0.6752    0.1857774\n  0.989   0.6752    0.1857774\n  0.990   0.6752    0.1857774\n  0.991   0.6720    0.1762612\n  0.992   0.6720    0.1762612\n  0.993   0.6720    0.1762612\n  0.994   0.6720    0.1762612\n  0.995   0.6720    0.1762612\n  0.996   0.6720    0.1762612\n  0.997   0.6720    0.1762612\n  0.998   0.6704    0.1715250\n  0.999   0.6688    0.1668755\n  1.000   0.6688    0.1668755\n\nTuning parameter 'alpha' was held constant at a value of 0\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0 and lambda = 0.059.\n\nplot(ridge.fit)                                                       # Plot\n\n\n\n\n\n\n\nridge.fit$bestTune                                                    # lambda의 최적값\n\n   alpha lambda\n60     0  0.059\n\n\nResult! lambda = 0.059일 때 정확도가 가장 높은 것을 알 수 있으며, lambda = 0.059를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\nround(coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda), 3)       # lambda의 최적값에 대한 회귀계수 추정치\n\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                s1\n(Intercept) -0.566\nPclass2     -0.072\nPclass3     -0.569\nSexmale     -0.871\nAge         -0.251\nFare         0.247\nFamSize     -0.204\n\n\nResult! 데이터 “titanic.trd.Imp”의 Target “Survived”은 “no”와 “yes” 2개의 클래스를 가지며, “Factor” 변환하면 알파벳순으로 수준을 부여하기 때문에 “yes”가 두 번째 클래스가 된다. 즉, “yes”에 속할 확률(= 탑승객이 생존할 확률)을 \\(p\\)라고 할 때, 추정된 회귀계수를 이용하여 다음과 같은 모형식을 얻을 수 있다. \\[\n\\begin{align*}\n\\log{\\frac{p}{1-p}} = &-0.566 - 0.072 Z_{\\text{Pclass2}} - 0.569 Z_{\\text{Pclass3}} -0.871 Z_{\\text{Sexmale}} \\\\\n                      &-0.251 Z_{\\text{Age}} +0.247 Z_{\\text{Fare}} - 0.204 Z_{\\text{FamSize}}\n\\end{align*}\n\\] 여기서, \\(Z_{\\text{예측 변수}}\\)는 표준화한 예측 변수를 의미한다.\n범주형 예측 변수(“Pclass”, “Sex”)는 더미 변환이 수행되었는데, 예를 들어, Pclass2는 탑승객의 티켓 등급이 2등급인 경우 “1”값을 가지고 2등급이 아니면 “0”값을 가진다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Ridge.html#모형-평가",
    "href": "Ridge.html#모형-평가",
    "title": "8  Ridge Regression",
    "section": "8.7 모형 평가",
    "text": "8.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\ntest.ridge.class &lt;- predict(ridge.fit, \n                            newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수   \n\ntest.ridge.class %&gt;%                                      \n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n8.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.ridge.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")       # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  152  36\n       yes  12  66\n                                         \n               Accuracy : 0.8195         \n                 95% CI : (0.768, 0.8638)\n    No Information Rate : 0.6165         \n    P-Value [Acc &gt; NIR] : 5.675e-13      \n                                         \n                  Kappa : 0.6006         \n                                         \n Mcnemar's Test P-Value : 0.0009009      \n                                         \n            Sensitivity : 0.6471         \n            Specificity : 0.9268         \n         Pos Pred Value : 0.8462         \n         Neg Pred Value : 0.8085         \n             Prevalence : 0.3835         \n         Detection Rate : 0.2481         \n   Detection Prevalence : 0.2932         \n      Balanced Accuracy : 0.7869         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\n\n\n\n8.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.ridge.prob &lt;- predict(ridge.fit, \n                           newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수 \n                           type = \"prob\")                 # 예측 확률 생성\n\ntest.ridge.prob %&gt;%                                                     \n  as_tibble\n\n# A tibble: 266 × 2\n      no   yes\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 0.219 0.781\n 2 0.694 0.306\n 3 0.820 0.180\n 4 0.352 0.648\n 5 0.842 0.158\n 6 0.691 0.309\n 7 0.404 0.596\n 8 0.663 0.337\n 9 0.847 0.153\n10 0.202 0.798\n# ℹ 256 more rows\n\n\n\ntest.ridge.prob &lt;- test.ridge.prob[,2]                 # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                        # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.ridge.prob)                     # 예측 확률을 수치형으로 변환\n\n\n8.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nridge.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")      # roc(실제 class, 예측 확률)\nauc        &lt;- round(auc(ridge.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(ridge.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(ridge.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n8.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n8.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nridge.pred &lt;- prediction(pp, ac)                       # prediction(예측 확률, 실제 class) \n\nridge.perf &lt;- performance(ridge.pred, \"tpr\", \"fpr\")    # performance(, \"민감도\", \"1-특이도\")                      \nplot(ridge.perf, col = \"gray\")                         # ROC Curve\n\nperf.auc   &lt;- performance(ridge.pred, \"auc\")           # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n8.7.3 향상 차트\n\n8.7.3.1 Package “ROCR”\n\nridge.perf &lt;- performance(ridge.pred, \"lift\", \"rpp\")   # Lift Chart                      \nplot(ridge.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ridge Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html",
    "href": "Elasticnet.html",
    "title": "9  Elastic Net Regression",
    "section": "",
    "text": "9.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정     \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#데이터-전처리",
    "href": "Elasticnet.html#데이터-전처리",
    "title": "9  Elastic Net Regression",
    "section": "9.2 데이터 전처리",
    "text": "9.2 데이터 전처리\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\ntitanic1 %&gt;%\n  as_tibble\n\n# A tibble: 891 × 6\n   Survived Pclass Sex      Age  Fare FamSize\n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 no       3      male      22  7.25       1\n 2 yes      1      female    38 71.3        1\n 3 yes      3      female    26  7.92       0\n 4 yes      1      female    35 53.1        1\n 5 no       3      male      35  8.05       0\n 6 no       3      male      NA  8.46       0\n 7 no       1      male      54 51.9        0\n 8 no       3      male       2 21.1        4\n 9 yes      3      female    27 11.1        2\n10 yes      2      female    14 30.1        1\n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#데이터-탐색",
    "href": "Elasticnet.html#데이터-탐색",
    "title": "9  Elastic Net Regression",
    "section": "9.3 데이터 탐색",
    "text": "9.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#데이터-분할",
    "href": "Elasticnet.html#데이터-분할",
    "title": "9  Elastic Net Regression",
    "section": "9.4 데이터 분할",
    "text": "9.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                             # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)     # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]                 # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]                # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#데이터-전처리-ii",
    "href": "Elasticnet.html#데이터-전처리-ii",
    "title": "9  Elastic Net Regression",
    "section": "9.5 데이터 전처리 II",
    "text": "9.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#모형-훈련",
    "href": "Elasticnet.html#모형-훈련",
    "title": "9  Elastic Net Regression",
    "section": "9.6 모형 훈련",
    "text": "9.6 모형 훈련\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 alpha와 lambda의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5,                 # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)                      # 병렬 처리\n\n\nset.seed(200)                                                         # For CV\nelast.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                   trControl = fitControl ,\n                   method = \"glmnet\",\n                   preProc = c(\"center\", \"scale\"))                    # Standardization for 예측 변수\n\nelast.fit\n\nglmnet \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  alpha  lambda        Accuracy  Kappa    \n  0.10   0.0005015509  0.7840    0.5378757\n  0.10   0.0050155094  0.7840    0.5351176\n  0.10   0.0501550942  0.7888    0.5401858\n  0.55   0.0005015509  0.7840    0.5378757\n  0.55   0.0050155094  0.7840    0.5344038\n  0.55   0.0501550942  0.7872    0.5392899\n  1.00   0.0005015509  0.7840    0.5378757\n  1.00   0.0050155094  0.7856    0.5359368\n  1.00   0.0501550942  0.7728    0.5149991\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0.1 and lambda = 0.05015509.\n\nplot(elast.fit)                                                       # Plot\n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 3개의 초모수 alpha, lambda 값을 조합하여 만든 9개의 초모수 조합값 (alpha, lambda)에 대한 정확도를 보여주며, (alpha = 0.1, lambda = 0.05015509)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (alpha = 0.1, lambda = 0.05015509) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(alpha = seq(0.05, 0.15, by = 0.01),         # alpha의 탐색 범위\n                          lambda = seq(0.03, 0.07, by = 0.01))        # lambda의 탐색 범위\n\nset.seed(200)                                                         # For CV\nelast.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                        trControl = fitControl ,\n                        method = \"glmnet\",\n                        tuneGrid = customGrid,\n                        preProc = c(\"center\", \"scale\"))               # Standardization for 예측 변수\n\nelast.tune.fit\n\nglmnet \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  alpha  lambda  Accuracy  Kappa    \n  0.05   0.03    0.7888    0.5408846\n  0.05   0.04    0.7888    0.5401998\n  0.05   0.05    0.7888    0.5401998\n  0.05   0.06    0.7936    0.5463561\n  0.05   0.07    0.7920    0.5425067\n  0.06   0.03    0.7888    0.5408846\n  0.06   0.04    0.7888    0.5401998\n  0.06   0.05    0.7888    0.5401998\n  0.06   0.06    0.7936    0.5463561\n  0.06   0.07    0.7920    0.5425067\n  0.07   0.03    0.7872    0.5377284\n  0.07   0.04    0.7888    0.5401998\n  0.07   0.05    0.7888    0.5401998\n  0.07   0.06    0.7920    0.5433084\n  0.07   0.07    0.7920    0.5425067\n  0.08   0.03    0.7872    0.5377284\n  0.08   0.04    0.7888    0.5401998\n  0.08   0.05    0.7888    0.5401998\n  0.08   0.06    0.7920    0.5433084\n  0.08   0.07    0.7920    0.5425067\n  0.09   0.03    0.7856    0.5339514\n  0.09   0.04    0.7872    0.5370436\n  0.09   0.05    0.7888    0.5401998\n  0.09   0.06    0.7920    0.5433084\n  0.09   0.07    0.7920    0.5425067\n  0.10   0.03    0.7856    0.5339514\n  0.10   0.04    0.7872    0.5370436\n  0.10   0.05    0.7888    0.5401858\n  0.10   0.06    0.7904    0.5401360\n  0.10   0.07    0.7904    0.5394590\n  0.11   0.03    0.7856    0.5339514\n  0.11   0.04    0.7872    0.5370436\n  0.11   0.05    0.7888    0.5401858\n  0.11   0.06    0.7904    0.5401360\n  0.11   0.07    0.7888    0.5362866\n  0.12   0.03    0.7856    0.5339514\n  0.12   0.04    0.7872    0.5370436\n  0.12   0.05    0.7888    0.5401858\n  0.12   0.06    0.7904    0.5401360\n  0.12   0.07    0.7888    0.5362866\n  0.13   0.03    0.7856    0.5339514\n  0.13   0.04    0.7872    0.5370436\n  0.13   0.05    0.7888    0.5401858\n  0.13   0.06    0.7904    0.5401360\n  0.13   0.07    0.7888    0.5362866\n  0.14   0.03    0.7872    0.5377337\n  0.14   0.04    0.7872    0.5370436\n  0.14   0.05    0.7888    0.5401858\n  0.14   0.06    0.7904    0.5401360\n  0.14   0.07    0.7888    0.5362866\n  0.15   0.03    0.7872    0.5377337\n  0.15   0.04    0.7872    0.5370436\n  0.15   0.05    0.7888    0.5401858\n  0.15   0.06    0.7920    0.5439542\n  0.15   0.07    0.7888    0.5362866\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0.05 and lambda = 0.06.\n\nplot(elast.tune.fit)                                                 # Plot\n\n\n\n\n\n\n\nelast.tune.fit$bestTune                                              # 최적의 초모수 조합값\n\n  alpha lambda\n4  0.05   0.06\n\n\nResult! (alpha = 0.05, lambda = 0.06)일 때 정확도가 가장 높은 것을 알 수 있으며, (alpha = 0.05, lambda = 0.06)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\nround(coef(elast.tune.fit$finalModel, elast.tune.fit$bestTune$lambda), 3)  # 최적의 초모수 조합값에 대한 회귀계수 추정치 \n\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                s1\n(Intercept) -0.565\nPclass2     -0.055\nPclass3     -0.553\nSexmale     -0.864\nAge         -0.233\nFare         0.237\nFamSize     -0.187\n\n\nResult! 데이터 “titanic.trd.Imp”의 Target “Survived”은 “no”와 “yes” 2개의 클래스를 가지며, “Factor” 변환하면 알파벳순으로 수준을 부여하기 때문에 “yes”가 두 번째 클래스가 된다. 즉, “yes”에 속할 확률(= 탑승객이 생존할 확률)을 \\(p\\)라고 할 때, 추정된 회귀계수를 이용하여 다음과 같은 모형식을 얻을 수 있다. \\[\n\\begin{align*}\n\\log{\\frac{p}{1-p}} = &-0.565 - 0.055 Z_{\\text{Pclass2}} - 0.553 Z_{\\text{Pclass3}} -0.864 Z_{\\text{Sexmale}} \\\\\n                      &-0.233 Z_{\\text{Age}} +0.237 Z_{\\text{Fare}} - 0.187 Z_{\\text{FamSize}}\n\\end{align*}\n\\] 여기서, \\(Z_{\\text{예측 변수}}\\)는 표준화한 예측 변수를 의미한다.\n범주형 예측 변수(“Pclass”, “Sex”)는 더미 변환이 수행되었는데, 예를 들어, Pclass2는 탑승객의 티켓 등급이 2등급인 경우 “1”값을 가지고 2등급이 아니면 “0”값을 가진다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "Elasticnet.html#모형-평가",
    "href": "Elasticnet.html#모형-평가",
    "title": "9  Elastic Net Regression",
    "section": "9.7 모형 평가",
    "text": "9.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성\ntest.elast.class &lt;- predict(elast.tune.fit, \n                            newdata = titanic.ted.Imp[,-1])   # Test Dataset including Only 예측 변수 \n\ntest.elast.class %&gt;%                                      \n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 yes  \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n9.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.elast.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")       # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  152  36\n       yes  12  66\n                                         \n               Accuracy : 0.8195         \n                 95% CI : (0.768, 0.8638)\n    No Information Rate : 0.6165         \n    P-Value [Acc &gt; NIR] : 5.675e-13      \n                                         \n                  Kappa : 0.6006         \n                                         \n Mcnemar's Test P-Value : 0.0009009      \n                                         \n            Sensitivity : 0.6471         \n            Specificity : 0.9268         \n         Pos Pred Value : 0.8462         \n         Neg Pred Value : 0.8085         \n             Prevalence : 0.3835         \n         Detection Rate : 0.2481         \n   Detection Prevalence : 0.2932         \n      Balanced Accuracy : 0.7869         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\n\n\n\n9.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.elast.prob &lt;- predict(elast.tune.fit, \n                           newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수 \n                           type = \"prob\")                 # 예측 확률 생성\n\ntest.elast.prob %&gt;%                                                          \n  as_tibble\n\n# A tibble: 266 × 2\n      no   yes\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 0.224 0.776\n 2 0.694 0.306\n 3 0.821 0.179\n 4 0.343 0.657\n 5 0.840 0.160\n 6 0.686 0.314\n 7 0.410 0.590\n 8 0.649 0.351\n 9 0.846 0.154\n10 0.203 0.797\n# ℹ 256 more rows\n\n\n\ntest.elast.prob &lt;- test.elast.prob[,2]                 # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                        # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.elast.prob)                     # 예측 확률을 수치형으로 변환\n\n\n9.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nelast.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")      # roc(실제 class, 예측 확률)\nauc        &lt;- round(auc(elast.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(elast.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(elast.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n9.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n9.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nelast.pred &lt;- prediction(pp, ac)                       # prediction(예측 확률, 실제 class) \n\nelast.perf &lt;- performance(elast.pred, \"tpr\", \"fpr\")    # performance(, \"민감도\", \"1-특이도\")                      \nplot(elast.perf, col = \"gray\")                         # ROC Curve\n\nperf.auc   &lt;- performance(elast.pred, \"auc\")           # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n9.7.3 향상 차트\n\n9.7.3.1 Package “ROCR”\n\nelast.perf &lt;- performance(elast.pred, \"lift\", \"rpp\")   # Lift Chart                      \nplot(elast.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net Regression</span>"
    ]
  },
  {
    "objectID": "RF.html",
    "href": "RF.html",
    "title": "10  Random Forest",
    "section": "",
    "text": "10.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                 # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                  # 사용할 Core 개수 지정     \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                       # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#데이터-전처리-i",
    "href": "RF.html#데이터-전처리-i",
    "title": "10  Random Forest",
    "section": "10.2 데이터 전처리 I",
    "text": "10.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  select(Survived, Pclass, Sex, Age, Fare, FamSize)                     # 분석에 사용할 변수 선택\n\nglimpse(titanic1)                                                       # 데이터 구조 확인\n\nRows: 891\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#데이터-탐색",
    "href": "RF.html#데이터-탐색",
    "title": "10  Random Forest",
    "section": "10.3 데이터 탐색",
    "text": "10.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"#00798c\", \"#d1495b\")) + # 특정 색깔 지정\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\")) +   # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#데이터-분할",
    "href": "RF.html#데이터-분할",
    "title": "10  Random Forest",
    "section": "10.4 데이터 분할",
    "text": "10.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#데이터-전처리-ii",
    "href": "RF.html#데이터-전처리-ii",
    "title": "10  Random Forest",
    "section": "10.5 데이터 전처리 II",
    "text": "10.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#모형-훈련",
    "href": "RF.html#모형-훈련",
    "title": "10  Random Forest",
    "section": "10.6 모형 훈련",
    "text": "10.6 모형 훈련\nBagging은 “Bootstrap Aggregation”의 약어로써 Original Dataset으로부터 크기가 동일한 Bootstrap Dataset을 생성한 후 각 Dataset에 독립적으로 예측 모형을 적용하고, 예측 결과를 집계하여 최종 예측을 도출한다. Bagging은 여러 모형의 예측 결과를 집계함으로써 예측 성능을 향상시키는 앙상블 기법이다.\n\n\n\n\nRandom Forest는 Bagging 기법을 사용하는 대표적인 머신러닝 알고리듬으로 Original Dataset으로부터 크기가 동일한 Bootstrap Dataset을 생성한 후 각 Dataset에 독립적으로 의사결정나무(Decision Tree)를 적용한다. Random Forest의 가장 큰 특징은 노드를 분할할 때마다 \\(m\\)개의 예측 변수(Feature)를 랜덤하게 추출하고 그중 최적의 변수의 선택한다. 이러한 랜덤성은 생성된 트리들의 상관성을 낮춤으로써 성능을 더욱 향상시키는 역할을 한다.\n\n\n\n\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 mtry의 최적값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5, # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)      # 병렬 처리\n\nset.seed(200)                                         # For CV\nrf.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                trControl = fitControl ,\n                method = \"parRF\",\n                ntree = 100,                          # 생성할 트리 개수\n                importance = TRUE)                    # 예측 변수의 중요도 저장\n\nCaution! Package \"caret\"을 통해 Random Forest를 수행하기 위해서 함수 train()의 옵션 method = \"parRF\" 또는 method = \"rf\"를 입력할 수 있다. 전자의 경우 병렬 처리를 통해 더 빠르게 모형 훈련을 수행할 수 있지만 OBB 오차는 계산할 수 없다. 게다가, 함수 train(Target ~ 예측 변수, data)를 사용하면 범주형 예측 변수는 자동적으로 더미 변환을 수행한다. 범주형 예측 변수에 대해 더미 변환을 수행하고 싶지 않다면 함수 train(x = 예측 변수만 포함하는 데이터셋, y = Target만 포함하는 데이터셋)를 사용한다.\n\nrf.fit\n\nParallel Random Forest \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa    \n  2     0.8016    0.5699679\n  4     0.7968    0.5690283\n  6     0.7984    0.5734107\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\nplot(rf.fit)                                          # Plot\n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 3개의 mtry 값에 대한 정확도를 보여주며, mtry = 2일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 값 2 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(mtry = seq(1, 5, by = 1))   # mtry의 탐색 범위 \n\nset.seed(200)                                         # For CV\nrf.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                     trControl = fitControl,\n                     method = \"parRF\", \n                     tuneGrid = customGrid,\n                     ntree = 100,                     # 생성할 트리 개수\n                     importance = TRUE)               # 예측 변수의 중요도 저장\n\nrf.tune.fit\n\nParallel Random Forest \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa    \n  1     0.7920    0.5206514\n  2     0.8192    0.6074700\n  3     0.8128    0.5982802\n  4     0.8000    0.5758978\n  5     0.7968    0.5694918\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\nplot(rf.tune.fit)                                     # Plot\n\n\n\n\n\n\n\nrf.tune.fit$bestTune                                  # mtry의 최적값\n\n  mtry\n2    2\n\n\nResult! mtry = 2일 때 정확도가 가장 높다는 것을 알 수 있으며, mtry = 2를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\n# 변수 중요도\nrandomForest::varImpPlot(rf.tune.fit$finalModel)\n\n\n\n\n\n\n\n\nResult! 정확도와 지니계수 측면에서 Sexmale이 Target Survived을 분류하는 데 있어 중요하다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "RF.html#모형-평가",
    "href": "RF.html#모형-평가",
    "title": "10  Random Forest",
    "section": "10.7 모형 평가",
    "text": "10.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성 \ntest.rf.class &lt;- predict(rf.tune.fit,\n                         newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수     \n\ntest.rf.class %&gt;%\n  as_tibble\n\n# A tibble: 266 × 1\n   value\n   &lt;fct&gt;\n 1 yes  \n 2 no   \n 3 no   \n 4 yes  \n 5 no   \n 6 no   \n 7 no   \n 8 no   \n 9 no   \n10 yes  \n# ℹ 256 more rows\n\n\n\n\n10.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.rf.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")        # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  157  36\n       yes   7  66\n                                          \n               Accuracy : 0.8383          \n                 95% CI : (0.7885, 0.8805)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 2.231e-15       \n                                          \n                  Kappa : 0.6387          \n                                          \n Mcnemar's Test P-Value : 1.955e-05       \n                                          \n            Sensitivity : 0.6471          \n            Specificity : 0.9573          \n         Pos Pred Value : 0.9041          \n         Neg Pred Value : 0.8135          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2481          \n   Detection Prevalence : 0.2744          \n      Balanced Accuracy : 0.8022          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n10.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.rf.prob &lt;- predict(rf.tune.fit, \n                        newdata = titanic.ted.Imp[,-1], # Test Dataset including Only 예측 변수  \n                        type = \"prob\")                  # 예측 확률 생성     \n\ntest.rf.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no    yes\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.0588 0.941 \n 2 0.765  0.235 \n 3 1      0     \n 4 0.412  0.588 \n 5 0.824  0.176 \n 6 1      0     \n 7 0.765  0.235 \n 8 0.941  0.0588\n 9 1      0     \n10 0.118  0.882 \n# ℹ 256 more rows\n\n\n\ntest.rf.prob &lt;- test.rf.prob[,2]                        # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                         # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.rf.prob)                         # 예측 확률을 수치형으로 변환\n\n\n10.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nrf.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")         # roc(실제 class, 예측 확률)\nauc     &lt;- round(auc(rf.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(rf.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(rf.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n10.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n10.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nrf.pred &lt;- prediction(pp, ac)                          # prediction(예측 확률, 실제 class) \n\nrf.perf &lt;- performance(rf.pred, \"tpr\", \"fpr\")          # performance(, \"민감도\", \"1-특이도\")                      \nplot(rf.perf, col = \"gray\")                            # ROC Curve\n\nperf.auc   &lt;- performance(rf.pred, \"auc\")              # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n10.7.3 향상 차트\n\n10.7.3.1 Package “ROCR”\n\nrf.perf &lt;- performance(rf.pred, \"lift\", \"rpp\")         # Lift Chart                      \nplot(rf.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html",
    "href": "AdaBoost.html",
    "title": "11  AdaBoost",
    "section": "",
    "text": "11.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정     \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#데이터-전처리-i",
    "href": "AdaBoost.html#데이터-전처리-i",
    "title": "11  AdaBoost",
    "section": "11.2 데이터 전처리 I",
    "text": "11.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  dplyr::select(Survived, Pclass, Sex, Age, Fare, FamSize)              # 분석에 사용할 변수 선택\n\nglimpse(titanic1)                                                       # 데이터 구조 확인\n\nRows: 891\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#데이터-탐색",
    "href": "AdaBoost.html#데이터-탐색",
    "title": "11  AdaBoost",
    "section": "11.3 데이터 탐색",
    "text": "11.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"purple\",\"cyan4\")) +     # 특정 색깔 지정\n  scale_fill_manual(values = c(\"purple\",\"cyan4\")) +       # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#데이터-분할",
    "href": "AdaBoost.html#데이터-분할",
    "title": "11  AdaBoost",
    "section": "11.4 데이터 분할",
    "text": "11.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#데이터-전처리-ii",
    "href": "AdaBoost.html#데이터-전처리-ii",
    "title": "11  AdaBoost",
    "section": "11.5 데이터 전처리 II",
    "text": "11.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#모형-훈련",
    "href": "AdaBoost.html#모형-훈련",
    "title": "11  AdaBoost",
    "section": "11.6 모형 훈련",
    "text": "11.6 모형 훈련\nBoosting은 다수의 약한 학습자(간단하면서 성능이 낮은 예측 모형)을 순차적으로 학습하는 앙상블 기법이다. Boosting의 특징은 이전 모형의 오차를 반영하여 다음 모형을 생성하며, 오차를 개선하는 방향으로 학습을 수행한다.\n\n\n\n\nAdaBoost는 최초로 Boosting 기법을 사용한 머신러닝 알고리듬으로 잘못 분류한 case에 대해 높은 Sample Weight를 부여하여 오차를 개선해 나가는 학습 방식이다.\n\n\n\n\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 coeflearn(모형 가중치 계산 방법), maxdepth(트리 최대 깊이), mfinal(트리 개수)의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5, # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)      # 병렬 처리\n\nset.seed(200)                                         # For CV\nada.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                 trControl = fitControl ,\n                 method = \"AdaBoost.M1\")                \n\nCaution! Package \"caret\"을 통해 \"AdaBoost.M1\"를 수행하는 경우, 함수 train(Target ~ 예측 변수, data)를 사용하면 범주형 예측 변수는 자동적으로 더미 변환이 된다. 범주형 예측 변수에 대해 더미 변환을 수행하고 싶지 않다면 함수 train(x = 예측 변수만 포함하는 데이터셋, y = Target만 포함하는 데이터셋)를 사용한다.\n\nada.fit\n\nAdaBoost.M1 \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  coeflearn  maxdepth  mfinal  Accuracy  Kappa    \n  Breiman    1          50     0.8064    0.5800334\n  Breiman    1         100     0.8048    0.5800353\n  Breiman    1         150     0.7968    0.5614759\n  Breiman    2          50     0.7936    0.5556642\n  Breiman    2         100     0.8080    0.5825767\n  Breiman    2         150     0.7840    0.5305024\n  Breiman    3          50     0.8016    0.5703638\n  Breiman    3         100     0.7968    0.5645317\n  Breiman    3         150     0.8096    0.5930586\n  Freund     1          50     0.8128    0.5966309\n  Freund     1         100     0.8096    0.5912254\n  Freund     1         150     0.8096    0.5912254\n  Freund     2          50     0.7824    0.5389308\n  Freund     2         100     0.7808    0.5323576\n  Freund     2         150     0.7872    0.5437294\n  Freund     3          50     0.8304    0.6359596\n  Freund     3         100     0.8192    0.6122056\n  Freund     3         150     0.8128    0.5995340\n  Zhu        1          50     0.8096    0.5913549\n  Zhu        1         100     0.8096    0.5920816\n  Zhu        1         150     0.8128    0.5981642\n  Zhu        2          50     0.7888    0.5430021\n  Zhu        2         100     0.7936    0.5572231\n  Zhu        2         150     0.7808    0.5285023\n  Zhu        3          50     0.8016    0.5769049\n  Zhu        3         100     0.7984    0.5714938\n  Zhu        3         150     0.8128    0.6016995\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mfinal = 50, maxdepth = 3 and coeflearn = Freund.\n\nplot(ada.fit)                                         # Plot \n\n\n\n\n\n\n\n\nResult! 각 초모수에 대해 랜덤하게 결정된 3개의 값을 조합하여 만든 27(3x3x3)개의 초모수 조합값 (coeflearn, maxdepth, mfinal)에 대한 정확도를 보여주며, (coeflearn = “Freund”, maxdepth = 3, mfinal = 50)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (coeflearn = “Freund”, maxdepth = 3, mfinal = 50) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행한다.\n\ncustomGrid &lt;- expand.grid(coeflearn = \"Freund\", \n                          maxdepth = seq(2, 4, by = 1),    # maxdepth의 탐색 범위 / 만약 stump를 생성하고 싶으면 maxdepth = 1 입력 \n                          mfinal = seq(49, 51, by = 1))    # mfinal의 탐색 범위          \n\nset.seed(200)                                              # For CV\nada.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                      trControl = fitControl ,\n                      method = \"AdaBoost.M1\",\n                      tuneGrid = customGrid)\n\nada.tune.fit\n\nAdaBoost.M1 \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  maxdepth  mfinal  Accuracy  Kappa    \n  2         49      0.8000    0.5689567\n  2         50      0.7904    0.5457561\n  2         51      0.7984    0.5649820\n  3         49      0.8080    0.5879492\n  3         50      0.8128    0.6011374\n  3         51      0.8096    0.5923669\n  4         49      0.8000    0.5745881\n  4         50      0.7936    0.5611124\n  4         51      0.7920    0.5579438\n\nTuning parameter 'coeflearn' was held constant at a value of Freund\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mfinal = 50, maxdepth = 3 and coeflearn = Freund.\n\nplot(ada.tune.fit)                                         # Plot\n\n\n\n\n\n\n\nada.tune.fit$bestTune                                      # 최적의 초모수 조합값\n\n  mfinal maxdepth coeflearn\n5     50        3    Freund\n\n\nResult! (coeflearn = “Freund”, maxdepth = 3, mfinal = 50)일 때 정확도가 가장 높은 것을 알 수 있으며, (coeflearn = “Freund”, maxdepth = 3, mfinal = 50)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\n\n# 변수 중요도\nada.tune.fit$finalModel$importance\n\n      Age   FamSize      Fare   Pclass2   Pclass3   Sexmale \n23.088304  4.696113 23.358169  1.856474 19.436316 27.564623 \n\n# 변수 중요도 plot\nimp &lt;- data.frame(Importance = ada.tune.fit$finalModel$importance)\nimp$varnames &lt;- rownames(imp) \nrownames(imp) &lt;- NULL\n\nggplot(imp, aes(x = reorder(varnames, Importance), y = Importance)) +\n  geom_point() +\n  geom_segment(aes(x = varnames, xend = varnames,\n                   y = 0, yend = Importance)) +\n  ylab(\"Importance\") +\n  xlab(\"\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\nResult! 변수 Sexmale이 Target Survived을 분류하는 데 있어 중요하다.\n\n# 각 트리의 모형 가중치\nada.tune.fit$finalModel$weights         \n\n [1] 1.510936806 0.927010924 0.421573923 0.453298659 0.304418547 0.300688915 0.296799204 0.132083742 0.004000005 1.489478597 0.983663916 0.460815378 0.430311487 0.355253100 0.311800382 0.272773792\n[17] 0.235732120 0.289359269 0.102136346 0.340542363 0.133443213 0.268383487 0.070086602 0.209382223 0.293918097 0.263682187 0.136913237 0.360633482 0.154103963 0.137707883 0.283144053 0.254716524\n[33] 0.164776159 0.202683578 0.288598981 0.149596731 0.199384149 0.248666076 0.298677097 0.099505631 0.217216130 0.308277823 0.121786218 0.234721421 0.119798469 0.154637590 0.157083936 0.099016414\n[49] 0.313723910 0.305686218\n\n\nResult! 모형 가중치는 해당 예측 모형이 얼마나 정확한지에 따라 결정되며, 정확도가 높을수록 높은 가중치가 부여된다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "AdaBoost.html#모형-평가",
    "href": "AdaBoost.html#모형-평가",
    "title": "11  AdaBoost",
    "section": "11.7 모형 평가",
    "text": "11.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성 \ntest.ada.class &lt;- predict(ada.tune.fit,\n                          newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수  \n\ntest.ada.class\n\n  [1] yes no  no  yes no  no  yes no  no  yes no  no  yes yes no  yes no  no  yes no  no  no  no  no  no  no  no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  no  yes no  no  no  no \n [49] yes no  no  no  no  no  no  no  no  no  no  yes no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  yes no  no  yes yes no  no  no  no  no  no  yes no  no  no  no  no  yes yes yes\n [97] yes yes no  no  no  yes yes no  no  yes no  yes no  yes yes no  yes no  no  no  yes no  no  no  yes no  yes yes no  no  yes yes no  yes no  yes no  no  yes yes no  no  no  no  yes no  no  no \n[145] yes yes no  no  no  no  no  no  no  no  no  no  no  no  no  no  yes no  yes yes no  yes no  no  no  no  no  no  yes yes yes no  yes no  no  no  no  no  no  yes no  no  no  yes no  yes no  no \n[193] no  no  no  no  no  yes no  no  no  yes no  no  no  no  no  no  yes no  no  yes yes no  no  no  yes yes no  no  yes no  no  yes yes yes no  no  no  no  yes no  no  no  yes no  yes yes no  no \n[241] no  yes no  no  no  yes no  yes no  no  yes no  no  no  no  yes yes yes no  yes no  yes yes no  no  no \nLevels: no yes\n\n\n\n\n11.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.ada.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")       # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  154  30\n       yes  10  72\n                                          \n               Accuracy : 0.8496          \n                 95% CI : (0.8009, 0.8903)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6697          \n                                          \n Mcnemar's Test P-Value : 0.002663        \n                                          \n            Sensitivity : 0.7059          \n            Specificity : 0.9390          \n         Pos Pred Value : 0.8780          \n         Neg Pred Value : 0.8370          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2707          \n   Detection Prevalence : 0.3083          \n      Balanced Accuracy : 0.8225          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n11.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.ada.prob &lt;- predict(ada.tune.fit, \n                         newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수  \n                         type = \"prob\")                 # 예측 확률 생성     \n\ntest.ada.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n      no   yes\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 0.251 0.749\n 2 0.535 0.465\n 3 0.690 0.310\n 4 0.462 0.538\n 5 0.579 0.421\n 6 0.597 0.403\n 7 0.482 0.518\n 8 0.780 0.220\n 9 0.667 0.333\n10 0.274 0.726\n# ℹ 256 more rows\n\ntest.ada.prob &lt;- test.ada.prob[,2]                      # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                         # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.ada.prob)                        # 예측 확률을 수치형으로 변환\n\n\n11.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nada.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")        # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(ada.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(ada.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(ada.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n11.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n11.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nada.pred &lt;- prediction(pp, ac)                         # prediction(예측 확률, 실제 class) \n\nada.perf &lt;- performance(ada.pred, \"tpr\", \"fpr\")        # performance(, \"민감도\", \"1-특이도\")                      \nplot(ada.perf, col = \"gray\")                           # ROC Curve\n\nperf.auc   &lt;- performance(ada.pred, \"auc\")             # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n11.7.3 향상 차트\n\n11.7.3.1 Package “ROCR”\n\nada.perf &lt;- performance(ada.pred, \"lift\", \"rpp\")       # Lift Chart                      \nplot(ada.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AdaBoost</span>"
    ]
  },
  {
    "objectID": "GBM.html",
    "href": "GBM.html",
    "title": "12  Gradient Boosting",
    "section": "",
    "text": "12.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정   \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#데이터-전처리-i",
    "href": "GBM.html#데이터-전처리-i",
    "title": "12  Gradient Boosting",
    "section": "12.2 데이터 전처리 I",
    "text": "12.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  dplyr::select(Survived, Pclass, Sex, Age, Fare, FamSize)              # 분석에 사용할 변수 선택\n\nglimpse(titanic1)                                                       # 데이터 구조 확인\n\nRows: 891\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#데이터-탐색",
    "href": "GBM.html#데이터-탐색",
    "title": "12  Gradient Boosting",
    "section": "12.3 데이터 탐색",
    "text": "12.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"purple\",\"cyan4\")) +     # 특정 색깔 지정\n  scale_fill_manual(values = c(\"purple\",\"cyan4\")) +       # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#데이터-분할",
    "href": "GBM.html#데이터-분할",
    "title": "12  Gradient Boosting",
    "section": "12.4 데이터 분할",
    "text": "12.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#데이터-전처리-ii",
    "href": "GBM.html#데이터-전처리-ii",
    "title": "12  Gradient Boosting",
    "section": "12.5 데이터 전처리 II",
    "text": "12.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#모형-훈련",
    "href": "GBM.html#모형-훈련",
    "title": "12  Gradient Boosting",
    "section": "12.6 모형 훈련",
    "text": "12.6 모형 훈련\nBoosting은 다수의 약한 학습자(간단하면서 성능이 낮은 예측 모형)을 순차적으로 학습하는 앙상블 기법이다. Boosting의 특징은 이전 모형의 오차를 반영하여 다음 모형을 생성하며, 오차를 개선하는 방향으로 학습을 수행한다.\n\n\n\n\nGradient Boosting은 손실함수를 이용하여 손실함수가 작아지는 방향으로 예측값을 업데이트하며 이전 모형의 오차를 기반으로 다음 모형을 생성한다.\n\n\n\n\n\n\n\n\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 shrinkage(학습률), interaction.depth(트리 최대 깊이), n.minobsinnode(터미널 노드의 최소 case 개수), n.trees(트리 개수)의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5, # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)      # 병렬 처리\n\nset.seed(200)                                         # For CV\ngbm.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                 trControl = fitControl ,\n                 method = \"gbm\")                \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2571             nan     0.1000    0.0343\n     2        1.2009             nan     0.1000    0.0300\n     3        1.1562             nan     0.1000    0.0203\n     4        1.1123             nan     0.1000    0.0209\n     5        1.0733             nan     0.1000    0.0163\n     6        1.0433             nan     0.1000    0.0146\n     7        1.0197             nan     0.1000    0.0109\n     8        0.9942             nan     0.1000    0.0107\n     9        0.9759             nan     0.1000    0.0094\n    10        0.9591             nan     0.1000    0.0072\n    20        0.8547             nan     0.1000    0.0010\n    40        0.7710             nan     0.1000   -0.0003\n    60        0.7328             nan     0.1000   -0.0007\n    80        0.7089             nan     0.1000   -0.0018\n   100        0.6875             nan     0.1000   -0.0014\n   120        0.6671             nan     0.1000   -0.0010\n   140        0.6472             nan     0.1000   -0.0016\n   150        0.6410             nan     0.1000   -0.0008\n\n\nCaution! Package \"caret\"을 통해 \"gbm\"를 수행하는 경우, 함수 train(Target ~ 예측 변수, data)를 사용하면 범주형 예측 변수는 자동적으로 더미 변환이 된다. 범주형 예측 변수에 대해 더미 변환을 수행하고 싶지 않다면 함수 train(x = 예측 변수만 포함하는 데이터셋, y = Target만 포함하는 데이터셋)를 사용한다.\n\ngbm.fit\n\nStochastic Gradient Boosting \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy  Kappa    \n  1                   50      0.8032    0.5687775\n  1                  100      0.7952    0.5619060\n  1                  150      0.7984    0.5674442\n  2                   50      0.8000    0.5696634\n  2                  100      0.7984    0.5665672\n  2                  150      0.8016    0.5732542\n  3                   50      0.8000    0.5628963\n  3                  100      0.8048    0.5781083\n  3                  150      0.8192    0.6138743\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.\n\nplot(gbm.fit)                                         # Plot \n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 3개의 초모수 interaction.depth, n.trees 값과 1개의 초모수 shrinkage, n.minobsinnode 값을 조합하여 만든 9개의 초모수 조합값 (shrinkage, interaction.depth, n.minobsinnode, n.trees)에 대한 정확도를 보여주며, (shrinkage = 0.1, interaction.depth = 3, n.minobsinnode = 10, n.trees = 150)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (shrinkage = 0.1, interaction.depth = 3, n.minobsinnode = 10, n.trees = 150) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(shrinkage = seq(0.08, 0.12, by = 0.01),  # shrinkage의 탐색 범위\n                          interaction.depth = seq(2, 4, by = 1),   # interaction.depth의 탐색 범위\n                          n.minobsinnode = seq(9, 11, by = 1),     # n.minobsinnode의 탐색 범위\n                          n.trees = seq(149, 151, by = 1))         # n.trees의 탐색 범위\n\nset.seed(200)                                                      # For CV\ngbm.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                      trControl = fitControl ,\n                      method = \"gbm\",\n                      tuneGrid = customGrid)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2624             nan     0.0900    0.0373\n     2        1.2035             nan     0.0900    0.0265\n     3        1.1574             nan     0.0900    0.0222\n     4        1.1146             nan     0.0900    0.0216\n     5        1.0759             nan     0.0900    0.0156\n     6        1.0451             nan     0.0900    0.0158\n     7        1.0172             nan     0.0900    0.0118\n     8        0.9941             nan     0.0900    0.0100\n     9        0.9726             nan     0.0900    0.0101\n    10        0.9574             nan     0.0900    0.0057\n    20        0.8417             nan     0.0900    0.0025\n    40        0.7541             nan     0.0900   -0.0006\n    60        0.7059             nan     0.0900   -0.0006\n    80        0.6752             nan     0.0900   -0.0012\n   100        0.6570             nan     0.0900   -0.0021\n   120        0.6309             nan     0.0900   -0.0012\n   140        0.6089             nan     0.0900   -0.0005\n   150        0.5981             nan     0.0900   -0.0013\n\ngbm.tune.fit\n\nStochastic Gradient Boosting \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  shrinkage  interaction.depth  n.minobsinnode  n.trees  Accuracy  Kappa    \n  0.08       2                   9              149      0.7952    0.5591812\n  0.08       2                   9              150      0.7952    0.5591812\n  0.08       2                   9              151      0.7952    0.5593732\n  0.08       2                  10              149      0.8016    0.5734456\n  0.08       2                  10              150      0.7984    0.5656913\n  0.08       2                  10              151      0.7968    0.5627190\n  0.08       2                  11              149      0.7984    0.5632410\n  0.08       2                  11              150      0.7952    0.5564770\n  0.08       2                  11              151      0.7952    0.5573275\n  0.08       3                   9              149      0.8208    0.6144630\n  0.08       3                   9              150      0.8192    0.6115500\n  0.08       3                   9              151      0.8224    0.6171112\n  0.08       3                  10              149      0.8144    0.5997213\n  0.08       3                  10              150      0.8128    0.5968501\n  0.08       3                  10              151      0.8128    0.5985971\n  0.08       3                  11              149      0.8080    0.5870642\n  0.08       3                  11              150      0.8080    0.5863755\n  0.08       3                  11              151      0.8096    0.5901226\n  0.08       4                   9              149      0.8176    0.6062555\n  0.08       4                   9              150      0.8176    0.6070349\n  0.08       4                   9              151      0.8128    0.5962777\n  0.08       4                  10              149      0.8128    0.5972400\n  0.08       4                  10              150      0.8080    0.5871991\n  0.08       4                  10              151      0.8096    0.5909302\n  0.08       4                  11              149      0.8144    0.6012404\n  0.08       4                  11              150      0.8160    0.6045500\n  0.08       4                  11              151      0.8176    0.6071129\n  0.09       2                   9              149      0.8064    0.5845136\n  0.09       2                   9              150      0.8096    0.5909709\n  0.09       2                   9              151      0.8080    0.5877708\n  0.09       2                  10              149      0.8128    0.5946760\n  0.09       2                  10              150      0.8128    0.5940690\n  0.09       2                  10              151      0.8176    0.6040105\n  0.09       2                  11              149      0.8064    0.5817456\n  0.09       2                  11              150      0.8032    0.5744295\n  0.09       2                  11              151      0.8048    0.5779296\n  0.09       3                   9              149      0.8176    0.6050779\n  0.09       3                   9              150      0.8176    0.6056842\n  0.09       3                   9              151      0.8160    0.6018222\n  0.09       3                  10              149      0.8176    0.6042407\n  0.09       3                  10              150      0.8160    0.6011866\n  0.09       3                  10              151      0.8208    0.6117746\n  0.09       3                  11              149      0.8192    0.6114804\n  0.09       3                  11              150      0.8176    0.6082544\n  0.09       3                  11              151      0.8208    0.6152589\n  0.09       4                   9              149      0.8192    0.6153226\n  0.09       4                   9              150      0.8192    0.6152896\n  0.09       4                   9              151      0.8176    0.6101179\n  0.09       4                  10              149      0.8240    0.6223660\n  0.09       4                  10              150      0.8176    0.6095809\n  0.09       4                  10              151      0.8192    0.6126612\n  0.09       4                  11              149      0.8272    0.6264772\n  0.09       4                  11              150      0.8288    0.6304270\n  0.09       4                  11              151      0.8256    0.6228076\n  0.10       2                   9              149      0.8048    0.5811582\n  0.10       2                   9              150      0.8048    0.5818748\n  0.10       2                   9              151      0.8032    0.5781233\n  0.10       2                  10              149      0.8016    0.5739252\n  0.10       2                  10              150      0.8000    0.5688869\n  0.10       2                  10              151      0.8016    0.5720188\n  0.10       2                  11              149      0.8080    0.5859403\n  0.10       2                  11              150      0.8048    0.5799433\n  0.10       2                  11              151      0.8064    0.5829712\n  0.10       3                   9              149      0.8160    0.6022133\n  0.10       3                   9              150      0.8176    0.6052717\n  0.10       3                   9              151      0.8208    0.6126740\n  0.10       3                  10              149      0.8096    0.5958014\n  0.10       3                  10              150      0.8064    0.5882751\n  0.10       3                  10              151      0.8096    0.5950331\n  0.10       3                  11              149      0.8208    0.6162759\n  0.10       3                  11              150      0.8176    0.6098820\n  0.10       3                  11              151      0.8144    0.6029499\n  0.10       4                   9              149      0.8128    0.5999040\n  0.10       4                   9              150      0.8048    0.5826735\n  0.10       4                   9              151      0.8080    0.5883021\n  0.10       4                  10              149      0.8224    0.6178314\n  0.10       4                  10              150      0.8240    0.6222348\n  0.10       4                  10              151      0.8256    0.6253934\n  0.10       4                  11              149      0.8224    0.6196381\n  0.10       4                  11              150      0.8224    0.6197772\n  0.10       4                  11              151      0.8208    0.6160745\n  0.11       2                   9              149      0.8096    0.5937889\n  0.11       2                   9              150      0.8112    0.5968405\n  0.11       2                   9              151      0.8112    0.5968405\n  0.11       2                  10              149      0.8048    0.5786148\n  0.11       2                  10              150      0.8064    0.5816521\n  0.11       2                  10              151      0.8032    0.5748027\n  0.11       2                  11              149      0.8096    0.5878849\n  0.11       2                  11              150      0.8128    0.5956412\n  0.11       2                  11              151      0.8080    0.5839207\n  0.11       3                   9              149      0.8192    0.6137125\n  0.11       3                   9              150      0.8144    0.6023771\n  0.11       3                   9              151      0.8192    0.6144973\n  0.11       3                  10              149      0.8144    0.5991862\n  0.11       3                  10              150      0.8128    0.5954511\n  0.11       3                  10              151      0.8144    0.5999162\n  0.11       3                  11              149      0.8096    0.5904760\n  0.11       3                  11              150      0.8048    0.5800811\n  0.11       3                  11              151      0.8080    0.5875993\n  0.11       4                   9              149      0.8096    0.5906864\n  0.11       4                   9              150      0.8064    0.5842011\n  0.11       4                   9              151      0.8096    0.5915463\n  0.11       4                  10              149      0.8144    0.5995659\n  0.11       4                  10              150      0.8176    0.6061575\n  0.11       4                  10              151      0.8176    0.6061295\n  0.11       4                  11              149      0.8256    0.6249168\n  0.11       4                  11              150      0.8256    0.6254403\n  0.11       4                  11              151      0.8208    0.6143930\n  0.12       2                   9              149      0.8016    0.5722035\n  0.12       2                   9              150      0.8000    0.5693195\n  0.12       2                   9              151      0.8000    0.5711099\n  0.12       2                  10              149      0.8128    0.5970680\n  0.12       2                  10              150      0.8096    0.5887563\n  0.12       2                  10              151      0.8144    0.5992358\n  0.12       2                  11              149      0.8096    0.5888815\n  0.12       2                  11              150      0.8096    0.5880570\n  0.12       2                  11              151      0.8048    0.5781171\n  0.12       3                   9              149      0.8176    0.6072785\n  0.12       3                   9              150      0.8224    0.6176963\n  0.12       3                   9              151      0.8144    0.6003575\n  0.12       3                  10              149      0.8144    0.5984722\n  0.12       3                  10              150      0.8160    0.6015585\n  0.12       3                  10              151      0.8144    0.5985069\n  0.12       3                  11              149      0.8192    0.6106743\n  0.12       3                  11              150      0.8144    0.5993903\n  0.12       3                  11              151      0.8096    0.5893437\n  0.12       4                   9              149      0.8080    0.5875854\n  0.12       4                   9              150      0.8096    0.5923086\n  0.12       4                   9              151      0.8128    0.5989533\n  0.12       4                  10              149      0.8144    0.5981933\n  0.12       4                  10              150      0.8112    0.5920172\n  0.12       4                  10              151      0.8128    0.5957502\n  0.12       4                  11              149      0.8128    0.6003945\n  0.12       4                  11              150      0.8176    0.6096983\n  0.12       4                  11              151      0.8128    0.5989083\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 150, interaction.depth = 4, shrinkage = 0.09 and n.minobsinnode = 11.\n\nplot(gbm.tune.fit)                                                 # Plot\n\n\n\n\n\n\n\ngbm.tune.fit$bestTune                                              # 최적의 초모수 조합값\n\n   n.trees interaction.depth shrinkage n.minobsinnode\n53     150                 4      0.09             11\n\n\nResult! (shrinkage = 0.09, interaction.depth = 4, n.minobsinnode = 11, n.trees = 150)일 때 정확도가 가장 높은 것을 알 수 있으며, (shrinkage = 0.09, interaction.depth = 4, n.minobsinnode = 11, n.trees = 150)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\n# 변수 중요도\nsummary(gbm.tune.fit$finalModel, las = 2)  \n\n\n\n\n\n\n\n\n            var    rel.inf\nSexmale Sexmale 29.9231815\nFare       Fare 27.9787356\nAge         Age 21.9393972\nPclass3 Pclass3 10.6516698\nFamSize FamSize  8.7607672\nPclass2 Pclass2  0.7462487\n\n\nResult! 변수 Sexmale이 Target Survived을 분류하는 데 있어 중요하다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "GBM.html#모형-평가",
    "href": "GBM.html#모형-평가",
    "title": "12  Gradient Boosting",
    "section": "12.7 모형 평가",
    "text": "12.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성 \ntest.gbm.class &lt;- predict(gbm.tune.fit,\n                          newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수 \n\ntest.gbm.class\n\n  [1] yes no  no  yes no  no  no  no  no  yes no  no  yes yes no  yes no  no  yes no  no  no  no  no  no  no  no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  no  yes no  no  no  no \n [49] yes no  no  no  no  no  no  yes no  no  no  yes no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  no  no  no  yes yes no  yes no  no  no  no  no  no  no  no  no  no  yes yes yes\n [97] yes yes no  no  no  yes yes no  no  yes no  yes no  yes yes no  yes no  yes no  yes no  no  no  yes no  no  yes no  no  yes yes no  yes no  yes no  no  yes yes no  no  no  no  yes no  no  no \n[145] yes no  no  no  no  no  no  yes no  no  no  no  no  no  no  no  yes no  yes yes no  yes yes no  no  no  no  no  yes yes yes no  yes no  no  no  no  no  no  yes no  no  no  yes no  yes no  no \n[193] no  no  no  yes no  no  no  no  no  yes no  no  no  no  no  no  yes no  no  yes yes no  no  no  yes yes no  no  no  no  no  yes yes yes no  no  no  no  no  no  no  no  yes no  no  yes no  no \n[241] no  yes no  no  no  yes no  yes no  no  yes no  no  no  no  yes yes yes no  yes no  yes yes no  no  no \nLevels: no yes\n\n\n\n\n12.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.gbm.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")       # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  155  32\n       yes   9  70\n                                          \n               Accuracy : 0.8459          \n                 95% CI : (0.7968, 0.8871)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6595          \n                                          \n Mcnemar's Test P-Value : 0.0005908       \n                                          \n            Sensitivity : 0.6863          \n            Specificity : 0.9451          \n         Pos Pred Value : 0.8861          \n         Neg Pred Value : 0.8289          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2632          \n   Detection Prevalence : 0.2970          \n      Balanced Accuracy : 0.8157          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n12.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.gbm.prob &lt;- predict(gbm.tune.fit, \n                         newdata = titanic.ted.Imp[,-1],# Test Dataset including Only 예측 변수  \n                         type = \"prob\")                 # 예측 확률 생성     \n\ntest.gbm.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no    yes\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.0427 0.957 \n 2 0.764  0.236 \n 3 0.922  0.0776\n 4 0.125  0.875 \n 5 0.649  0.351 \n 6 0.842  0.158 \n 7 0.510  0.490 \n 8 0.943  0.0569\n 9 0.944  0.0555\n10 0.0294 0.971 \n# ℹ 256 more rows\n\ntest.gbm.prob &lt;- test.gbm.prob[,2]                      # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                         # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.gbm.prob)                        # 예측 확률을 수치형으로 변환\n\n\n12.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\ngbm.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")        # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(gbm.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(gbm.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(gbm.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n12.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n12.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\ngbm.pred &lt;- prediction(pp, ac)                         # prediction(예측 확률, 실제 class) \n\ngbm.perf &lt;- performance(gbm.pred, \"tpr\", \"fpr\")        # performance(, \"민감도\", \"1-특이도\")                      \nplot(gbm.perf, col = \"gray\")                           # ROC Curve\n\nperf.auc   &lt;- performance(gbm.pred, \"auc\")             # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n12.7.3 향상 차트\n\n12.7.3.1 Package “ROCR”\n\ngbm.perf &lt;- performance(gbm.pred, \"lift\", \"rpp\")       # Lift Chart                      \nplot(gbm.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "XGBoost.html",
    "href": "XGBoost.html",
    "title": "13  XGBoost",
    "section": "",
    "text": "13.1 데이터 불러오기\npacman::p_load(\"data.table\", \n               \"tidyverse\", \n               \"dplyr\", \"tidyr\",\n               \"ggplot2\", \"GGally\",\n               \"caret\",\n               \"xgboost\",                                               # For xgb.importance\n               \"doParallel\", \"parallel\")                                # For 병렬 처리\n\nregisterDoParallel(cores=detectCores())                                 # 사용할 Core 개수 지정  \n\ntitanic &lt;- fread(\"../Titanic.csv\")                                      # 데이터 불러오기\n\ntitanic %&gt;%\n  as_tibble\n# A tibble: 891 × 11\n   Survived Pclass Name                                                Sex      Age SibSp Parch Ticket            Fare Cabin  Embarked\n      &lt;int&gt;  &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1        0      3 Braund, Mr. Owen Harris                             male      22     1     0 A/5 21171         7.25 \"\"     S       \n 2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female    38     1     0 PC 17599         71.3  \"C85\"  C       \n 3        1      3 Heikkinen, Miss. Laina                              female    26     0     0 STON/O2. 3101282  7.92 \"\"     S       \n 4        1      1 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female    35     1     0 113803           53.1  \"C123\" S       \n 5        0      3 Allen, Mr. William Henry                            male      35     0     0 373450            8.05 \"\"     S       \n 6        0      3 Moran, Mr. James                                    male      NA     0     0 330877            8.46 \"\"     Q       \n 7        0      1 McCarthy, Mr. Timothy J                             male      54     0     0 17463            51.9  \"E46\"  S       \n 8        0      3 Palsson, Master. Gosta Leonard                      male       2     3     1 349909           21.1  \"\"     S       \n 9        1      3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   female    27     0     2 347742           11.1  \"\"     S       \n10        1      2 Nasser, Mrs. Nicholas (Adele Achem)                 female    14     1     0 237736           30.1  \"\"     C       \n# ℹ 881 more rows",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#데이터-전처리-i",
    "href": "XGBoost.html#데이터-전처리-i",
    "title": "13  XGBoost",
    "section": "13.2 데이터 전처리 I",
    "text": "13.2 데이터 전처리 I\n\ntitanic %&lt;&gt;%\n  data.frame() %&gt;%                                                      # Data Frame 형태로 변환 \n  mutate(Survived = ifelse(Survived == 1, \"yes\", \"no\"))                 # Target을 문자형 변수로 변환\n\n# 1. Convert to Factor\nfac.col &lt;- c(\"Pclass\", \"Sex\",\n             # Target\n             \"Survived\")\n\ntitanic &lt;- titanic %&gt;% \n  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 11\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n\n# 2. Generate New Variable\ntitanic &lt;- titanic %&gt;%\n  mutate(FamSize = SibSp + Parch)                                       # \"FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수\"로 가족 수를 의미하는 새로운 변수\n\nglimpse(titanic)                                                        # 데이터 구조 확인\n\nRows: 891\nColumns: 12\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Name     &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Allen, Mr. William Henry…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ SibSp    &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0…\n$ Parch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0…\n$ Ticket   &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"373450\", \"330877\", \"17463\", \"349909\", \"347742\", \"237736\", \"PP 9549\", \"113783\", \"A/5. 2151\", \"347082\", \"350406\", \"248706\", \"38…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ Cabin    &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C103\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"D56\", \"\", \"A6\", \"\", \"\", \"\", \"C23 C25 C27\", \"\", \"\", \"\", \"B78\", \"\", \"\", \"\", \"\", \"\"…\n$ Embarked &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"C\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"Q\", \"S\", \"C\", \"C\", \"Q\", \"S\", \"C\", \"S\", \"…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…\n\n# 3. Select Variables used for Analysis\ntitanic1 &lt;- titanic %&gt;% \n  dplyr::select(Survived, Pclass, Sex, Age, Fare, FamSize)              # 분석에 사용할 변수 선택\n\nglimpse(titanic1)                                                       # 데이터 구조 확인\n\nRows: 891\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, yes, no, no, no, no, yes, yes, yes, yes, no, no, no, yes, no, yes, no, yes, no, yes, yes, yes, no, yes, no, no, yes, no, no, yes, yes, no, no, no, yes, no, no, yes, no…\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3…\n$ Sex      &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male, female, female, male, male, female, female, male, male, female, male, female, femal…\n$ Age      &lt;dbl&gt; 22.0, 38.0, 26.0, 35.0, 35.0, NA, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, NA, 31.0, NA, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0, NA, 19.0, NA, NA, 40.0, NA, NA, 66.…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 8.0500, 31.2750, 7.8542, 16.0000, 29.1250, 13.0000, 18.0000, 7.2250, 26.0000,…\n$ FamSize  &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 4, 2, 1, 2, 0, 0, 6, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 6, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 3, 0, 0, 1, 0, 2, 1, 5, 0, 1, 1, 1, 0, 0, 0, 3, 7, 0…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#데이터-탐색",
    "href": "XGBoost.html#데이터-탐색",
    "title": "13  XGBoost",
    "section": "13.3 데이터 탐색",
    "text": "13.3 데이터 탐색\n\nggpairs(titanic1,                                        \n        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현\n  theme_bw()\n\n\n\n\n\n\n\nggpairs(titanic1,                                     \n        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현\n  scale_colour_manual(values = c(\"purple\",\"cyan4\")) +     # 특정 색깔 지정\n  scale_fill_manual(values = c(\"purple\",\"cyan4\")) +       # 특정 색깔 지정\n  theme_bw()",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#데이터-분할",
    "href": "XGBoost.html#데이터-분할",
    "title": "13  XGBoost",
    "section": "13.4 데이터 분할",
    "text": "13.4 데이터 분할\n\n# Partition (Training Dataset : Test Dataset = 7:3)\ny      &lt;- titanic1$Survived                           # Target\n\nset.seed(200)\nind    &lt;- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할\ntitanic.trd &lt;- titanic1[ind$Resample1,]               # Training Dataset\ntitanic.ted &lt;- titanic1[-ind$Resample1,]              # Test Dataset",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#데이터-전처리-ii",
    "href": "XGBoost.html#데이터-전처리-ii",
    "title": "13  XGBoost",
    "section": "13.5 데이터 전처리 II",
    "text": "13.5 데이터 전처리 II\n\n# Imputation\ntitanic.trd.Imp &lt;- titanic.trd %&gt;% \n  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체\n\ntitanic.ted.Imp &lt;- titanic.ted %&gt;% \n  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체\n\nglimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인\n\nRows: 625\nColumns: 6\n$ Survived &lt;fct&gt; no, yes, yes, no, no, no, yes, yes, yes, yes, no, no, yes, no, yes, no, yes, no, no, no, yes, no, no, yes, yes, no, no, no, no, no, yes, no, no, no, yes, no, yes, no, no, no, yes, n…\n$ Pclass   &lt;fct&gt; 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3…\n$ Sex      &lt;fct&gt; male, female, female, male, male, male, female, female, female, female, male, female, male, female, female, male, male, female, male, male, female, male, male, female, female, male,…\n$ Age      &lt;dbl&gt; 22.00000, 26.00000, 35.00000, 35.00000, 29.93737, 2.00000, 27.00000, 14.00000, 4.00000, 58.00000, 39.00000, 14.00000, 29.93737, 31.00000, 29.93737, 35.00000, 28.00000, 8.00000, 29.9…\n$ Fare     &lt;dbl&gt; 7.2500, 7.9250, 53.1000, 8.0500, 8.4583, 21.0750, 11.1333, 30.0708, 16.7000, 26.5500, 31.2750, 7.8542, 13.0000, 18.0000, 7.2250, 26.0000, 35.5000, 21.0750, 7.2250, 263.0000, 7.8792,…\n$ FamSize  &lt;int&gt; 1, 0, 1, 0, 0, 4, 2, 1, 2, 0, 6, 0, 0, 1, 0, 0, 0, 4, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 5, 1, 1, 0, 7, 0, 0, 5, 0, 2, 7, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3…\n\nglimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인\n\nRows: 266\nColumns: 6\n$ Survived &lt;fct&gt; yes, no, no, yes, no, yes, yes, yes, yes, yes, no, no, yes, yes, no, yes, no, yes, yes, no, yes, no, no, no, no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no, yes, no, n…\n$ Pclass   &lt;fct&gt; 1, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3…\n$ Sex      &lt;fct&gt; female, male, male, female, male, male, female, female, male, female, male, male, female, female, male, female, male, male, female, male, female, male, male, male, male, male, male,…\n$ Age      &lt;dbl&gt; 38.00000, 54.00000, 20.00000, 55.00000, 2.00000, 34.00000, 15.00000, 38.00000, 29.93737, 3.00000, 29.93737, 21.00000, 29.00000, 21.00000, 28.50000, 5.00000, 45.00000, 29.93737, 29.0…\n$ Fare     &lt;dbl&gt; 71.2833, 51.8625, 8.0500, 16.0000, 29.1250, 13.0000, 8.0292, 31.3875, 7.2292, 41.5792, 8.0500, 7.8000, 26.0000, 10.5000, 7.2292, 27.7500, 83.4750, 15.2458, 10.5000, 8.1583, 7.9250, …\n$ FamSize  &lt;int&gt; 1, 0, 0, 0, 5, 0, 0, 6, 0, 3, 0, 0, 1, 0, 0, 3, 1, 2, 0, 0, 6, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 5, 2, 5, 0, 5, 0, 4, 0, 6…",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#모형-훈련",
    "href": "XGBoost.html#모형-훈련",
    "title": "13  XGBoost",
    "section": "13.6 모형 훈련",
    "text": "13.6 모형 훈련\nBoosting은 다수의 약한 학습자(간단하면서 성능이 낮은 예측 모형)을 순차적으로 학습하는 앙상블 기법이다. Boosting의 특징은 이전 모형의 오차를 반영하여 다음 모형을 생성하며, 오차를 개선하는 방향으로 학습을 수행한다.\n\n\n\n\nXGBoost는 Extreme Gradient Boosting의 약어로 Gradient Boosting의 단점을 해결하기 위해 제안되었다.\nPackage \"caret\"은 통합 API를 통해 R로 기계 학습을 실행할 수 있는 매우 실용적인 방법을 제공한다. Package \"caret\"에서는 초모수의 최적의 조합을 찾는 방법으로 그리드 검색(Grid Search), 랜덤 검색(Random Search), 직접 탐색 범위 설정이 있다. 여기서는 초모수 eta, max_depth, gamma, colsample_bytree, min_child_weight, subsample, nrounds의 최적의 조합값을 찾기 위해 그리드 검색을 수행하였고, 이를 기반으로 직접 탐색 범위를 설정하였다. 아래는 그리드 검색을 수행하였을 때 결과이다.\n\n\n\n\n\nfitControl &lt;- trainControl(method = \"cv\", number = 5, # 5-Fold Cross Validation (5-Fold CV)\n                           allowParallel = TRUE)      # 병렬 처리\n\nset.seed(200)                                         # For CV\nxgb.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                 trControl = fitControl ,\n                 method = \"xgbTree\")                \n\nCaution! Package \"caret\"을 통해 \"xgbTree\"를 수행하는 경우, 함수 train(Target ~ 예측 변수, data)를 사용하면 범주형 예측 변수는 자동적으로 더미 변환이 된다. 범주형 예측 변수에 대해 더미 변환을 수행하고 싶지 않다면 함수 train(x = 예측 변수만 포함하는 데이터셋, y = Target만 포함하는 데이터셋)를 사용한다.\n\nxgb.fit\n\neXtreme Gradient Boosting \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy  Kappa    \n  0.3  1          0.6               0.50        50      0.7936    0.5532288\n  0.3  1          0.6               0.50       100      0.7984    0.5670283\n  0.3  1          0.6               0.50       150      0.8016    0.5750364\n  0.3  1          0.6               0.75        50      0.8000    0.5683679\n  0.3  1          0.6               0.75       100      0.7920    0.5530621\n  0.3  1          0.6               0.75       150      0.7952    0.5595167\n  0.3  1          0.6               1.00        50      0.8048    0.5824528\n  0.3  1          0.6               1.00       100      0.7936    0.5567690\n  0.3  1          0.6               1.00       150      0.7936    0.5583194\n  0.3  1          0.8               0.50        50      0.7952    0.5588203\n  0.3  1          0.8               0.50       100      0.7952    0.5606791\n  0.3  1          0.8               0.50       150      0.7984    0.5678404\n  0.3  1          0.8               0.75        50      0.8064    0.5838573\n  0.3  1          0.8               0.75       100      0.7952    0.5598180\n  0.3  1          0.8               0.75       150      0.7984    0.5680711\n  0.3  1          0.8               1.00        50      0.8032    0.5776070\n  0.3  1          0.8               1.00       100      0.7936    0.5567448\n  0.3  1          0.8               1.00       150      0.7952    0.5608127\n  0.3  2          0.6               0.50        50      0.7984    0.5659624\n  0.3  2          0.6               0.50       100      0.8000    0.5681795\n  0.3  2          0.6               0.50       150      0.7984    0.5655375\n  0.3  2          0.6               0.75        50      0.8080    0.5840128\n  0.3  2          0.6               0.75       100      0.8160    0.6017267\n  0.3  2          0.6               0.75       150      0.8208    0.6148647\n  0.3  2          0.6               1.00        50      0.7984    0.5642618\n  0.3  2          0.6               1.00       100      0.8160    0.6050330\n  0.3  2          0.6               1.00       150      0.8144    0.6035449\n  0.3  2          0.8               0.50        50      0.8112    0.5928300\n  0.3  2          0.8               0.50       100      0.8224    0.6163725\n  0.3  2          0.8               0.50       150      0.8192    0.6108362\n  0.3  2          0.8               0.75        50      0.8144    0.5988434\n  0.3  2          0.8               0.75       100      0.8064    0.5841983\n  0.3  2          0.8               0.75       150      0.8064    0.5869076\n  0.3  2          0.8               1.00        50      0.8048    0.5784530\n  0.3  2          0.8               1.00       100      0.8080    0.5916713\n  0.3  2          0.8               1.00       150      0.8160    0.6074775\n  0.3  3          0.6               0.50        50      0.8080    0.5855842\n  0.3  3          0.6               0.50       100      0.7984    0.5677822\n  0.3  3          0.6               0.50       150      0.7936    0.5622879\n  0.3  3          0.6               0.75        50      0.8000    0.5690285\n  0.3  3          0.6               0.75       100      0.8176    0.6067952\n  0.3  3          0.6               0.75       150      0.8032    0.5768480\n  0.3  3          0.6               1.00        50      0.8064    0.5846185\n  0.3  3          0.6               1.00       100      0.8176    0.6114240\n  0.3  3          0.6               1.00       150      0.8000    0.5742391\n  0.3  3          0.8               0.50        50      0.7856    0.5416109\n  0.3  3          0.8               0.50       100      0.8080    0.5909694\n  0.3  3          0.8               0.50       150      0.8048    0.5833752\n  0.3  3          0.8               0.75        50      0.8080    0.5848742\n  0.3  3          0.8               0.75       100      0.8096    0.5922924\n  0.3  3          0.8               0.75       150      0.8096    0.5944031\n  0.3  3          0.8               1.00        50      0.8112    0.5955304\n  0.3  3          0.8               1.00       100      0.8160    0.6095351\n  0.3  3          0.8               1.00       150      0.8080    0.5923806\n  0.4  1          0.6               0.50        50      0.7984    0.5654428\n  0.4  1          0.6               0.50       100      0.7856    0.5417550\n  0.4  1          0.6               0.50       150      0.7904    0.5519082\n  0.4  1          0.6               0.75        50      0.7904    0.5527988\n  0.4  1          0.6               0.75       100      0.7936    0.5588007\n  0.4  1          0.6               0.75       150      0.8048    0.5850564\n  0.4  1          0.6               1.00        50      0.8000    0.5708883\n  0.4  1          0.6               1.00       100      0.7904    0.5508317\n  0.4  1          0.6               1.00       150      0.7952    0.5638414\n  0.4  1          0.8               0.50        50      0.7952    0.5596631\n  0.4  1          0.8               0.50       100      0.7904    0.5501620\n  0.4  1          0.8               0.50       150      0.7984    0.5681207\n  0.4  1          0.8               0.75        50      0.7920    0.5567217\n  0.4  1          0.8               0.75       100      0.8016    0.5713368\n  0.4  1          0.8               0.75       150      0.7984    0.5700493\n  0.4  1          0.8               1.00        50      0.8000    0.5706676\n  0.4  1          0.8               1.00       100      0.7952    0.5627966\n  0.4  1          0.8               1.00       150      0.7872    0.5463291\n  0.4  2          0.6               0.50        50      0.8000    0.5686933\n  0.4  2          0.6               0.50       100      0.8304    0.6363294\n  0.4  2          0.6               0.50       150      0.8240    0.6250907\n  0.4  2          0.6               0.75        50      0.8096    0.5875135\n  0.4  2          0.6               0.75       100      0.8064    0.5837448\n  0.4  2          0.6               0.75       150      0.8064    0.5859264\n  0.4  2          0.6               1.00        50      0.7952    0.5598551\n  0.4  2          0.6               1.00       100      0.8160    0.6065912\n  0.4  2          0.6               1.00       150      0.8048    0.5836471\n  0.4  2          0.8               0.50        50      0.7840    0.5373904\n  0.4  2          0.8               0.50       100      0.8080    0.5909284\n  0.4  2          0.8               0.50       150      0.7904    0.5520807\n  0.4  2          0.8               0.75        50      0.8176    0.6082601\n  0.4  2          0.8               0.75       100      0.8240    0.6230251\n  0.4  2          0.8               0.75       150      0.8096    0.5938767\n  0.4  2          0.8               1.00        50      0.8064    0.5852386\n  0.4  2          0.8               1.00       100      0.8160    0.6091437\n  0.4  2          0.8               1.00       150      0.8064    0.5882986\n  0.4  3          0.6               0.50        50      0.8016    0.5800843\n  0.4  3          0.6               0.50       100      0.8160    0.6076108\n  0.4  3          0.6               0.50       150      0.7952    0.5644753\n  0.4  3          0.6               0.75        50      0.8064    0.5851191\n  0.4  3          0.6               0.75       100      0.8144    0.6048904\n  0.4  3          0.6               0.75       150      0.8096    0.5969159\n  0.4  3          0.6               1.00        50      0.8112    0.5967624\n  0.4  3          0.6               1.00       100      0.8096    0.5948556\n  0.4  3          0.6               1.00       150      0.8080    0.5925659\n  0.4  3          0.8               0.50        50      0.8080    0.5919065\n  0.4  3          0.8               0.50       100      0.7984    0.5685959\n  0.4  3          0.8               0.50       150      0.7904    0.5575082\n  0.4  3          0.8               0.75        50      0.8016    0.5746439\n  0.4  3          0.8               0.75       100      0.8176    0.6118326\n  0.4  3          0.8               0.75       150      0.8176    0.6124456\n  0.4  3          0.8               1.00        50      0.8144    0.6005889\n  0.4  3          0.8               1.00       100      0.7936    0.5584222\n  0.4  3          0.8               1.00       150      0.7872    0.5478133\n\nTuning parameter 'gamma' was held constant at a value of 0\nTuning parameter 'min_child_weight' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 100, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.5.\n\nplot(xgb.fit)                                         # Plot \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResult! 랜덤하게 결정된 초모수 값을 조합하여 만든 108개의 초모수 조합값 (eta, max_depth, gamma, colsample_bytree, min_child_weight, subsample, nrounds)에 대한 정확도를 보여주며, (eta = 0.4, max_depth = 2, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.5, nrounds = 100)일 때 정확도가 가장 높은 것을 알 수 있다. 따라서 그리드 검색을 통해 찾은 최적의 초모수 조합값 (eta = 0.4, max_depth = 2, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.5, nrounds = 100) 근처의 값들을 탐색 범위로 설정하여 훈련을 다시 수행할 수 있다.\n\ncustomGrid &lt;- expand.grid(eta = seq(0.3, 0.5, by = 0.1),              # eta의 탐색 범위\n                          max_depth = seq(1, 3, by = 1),              # max_depth의 탐색 범위\n                          gamma = seq(0.1, 1, by = 0.5),              # gamma의 탐색 범위\n                          colsample_bytree = seq(0.5, 0.7, by = 0.1), # colsample_bytree의 탐색 범위\n                          min_child_weight = seq(1, 2, by = 1),       # min_child_weight의 탐색 범위\n                          subsample = seq(0.45, 0.55, by = 0.1),      # subsample의 탐색 범위\n                          nrounds = seq(99, 101, by = 1))             # nrounds의 탐색 범위\n\nset.seed(200)                                                         # For CV\nxgb.tune.fit &lt;- train(Survived ~ ., data = titanic.trd.Imp, \n                      trControl = fitControl ,\n                      method = \"xgbTree\",\n                      tuneGrid = customGrid)\n\nxgb.tune.fit\n\neXtreme Gradient Boosting \n\n625 samples\n  5 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 500, 500, 500, 500, 500 \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  colsample_bytree  min_child_weight  subsample  nrounds  Accuracy  Kappa    \n  0.3  1          0.1    0.5               1                 0.45        99      0.8064    0.5838129\n  0.3  1          0.1    0.5               1                 0.45       100      0.8048    0.5814521\n  0.3  1          0.1    0.5               1                 0.45       101      0.8032    0.5774806\n  0.3  1          0.1    0.5               1                 0.55        99      0.8032    0.5775456\n  0.3  1          0.1    0.5               1                 0.55       100      0.8080    0.5881485\n  0.3  1          0.1    0.5               1                 0.55       101      0.8048    0.5801318\n  0.3  1          0.1    0.5               2                 0.45        99      0.8080    0.5877026\n  0.3  1          0.1    0.5               2                 0.45       100      0.8064    0.5837763\n  0.3  1          0.1    0.5               2                 0.45       101      0.8128    0.5982911\n  0.3  1          0.1    0.5               2                 0.55        99      0.8000    0.5711714\n  0.3  1          0.1    0.5               2                 0.55       100      0.8016    0.5743475\n  0.3  1          0.1    0.5               2                 0.55       101      0.7968    0.5657900\n  0.3  1          0.1    0.6               1                 0.45        99      0.8048    0.5834690\n  0.3  1          0.1    0.6               1                 0.45       100      0.8032    0.5813071\n  0.3  1          0.1    0.6               1                 0.45       101      0.8016    0.5772513\n  0.3  1          0.1    0.6               1                 0.55        99      0.8048    0.5792279\n  0.3  1          0.1    0.6               1                 0.55       100      0.8080    0.5846818\n  0.3  1          0.1    0.6               1                 0.55       101      0.8048    0.5800795\n  0.3  1          0.1    0.6               2                 0.45        99      0.7952    0.5604895\n  0.3  1          0.1    0.6               2                 0.45       100      0.7936    0.5553880\n  0.3  1          0.1    0.6               2                 0.45       101      0.7920    0.5536102\n  0.3  1          0.1    0.6               2                 0.55        99      0.7920    0.5529005\n  0.3  1          0.1    0.6               2                 0.55       100      0.7888    0.5460309\n  0.3  1          0.1    0.6               2                 0.55       101      0.7872    0.5438934\n  0.3  1          0.1    0.7               1                 0.45        99      0.8032    0.5753638\n  0.3  1          0.1    0.7               1                 0.45       100      0.8000    0.5679317\n  0.3  1          0.1    0.7               1                 0.45       101      0.8032    0.5744805\n  0.3  1          0.1    0.7               1                 0.55        99      0.7968    0.5628348\n  0.3  1          0.1    0.7               1                 0.55       100      0.7952    0.5588770\n  0.3  1          0.1    0.7               1                 0.55       101      0.7968    0.5619139\n  0.3  1          0.1    0.7               2                 0.45        99      0.7920    0.5530767\n  0.3  1          0.1    0.7               2                 0.45       100      0.7920    0.5527731\n  0.3  1          0.1    0.7               2                 0.45       101      0.7920    0.5525992\n  0.3  1          0.1    0.7               2                 0.55        99      0.7936    0.5540954\n  0.3  1          0.1    0.7               2                 0.55       100      0.8000    0.5666202\n  0.3  1          0.1    0.7               2                 0.55       101      0.8032    0.5739548\n  0.3  1          0.6    0.5               1                 0.45        99      0.7936    0.5564740\n  0.3  1          0.6    0.5               1                 0.45       100      0.7952    0.5581319\n  0.3  1          0.6    0.5               1                 0.45       101      0.7968    0.5605810\n  0.3  1          0.6    0.5               1                 0.55        99      0.7968    0.5633042\n  0.3  1          0.6    0.5               1                 0.55       100      0.8000    0.5719791\n  0.3  1          0.6    0.5               1                 0.55       101      0.7968    0.5641198\n  0.3  1          0.6    0.5               2                 0.45        99      0.8016    0.5737968\n  0.3  1          0.6    0.5               2                 0.45       100      0.7952    0.5585167\n  0.3  1          0.6    0.5               2                 0.45       101      0.7920    0.5504370\n  0.3  1          0.6    0.5               2                 0.55        99      0.8048    0.5790185\n  0.3  1          0.6    0.5               2                 0.55       100      0.8080    0.5852328\n  0.3  1          0.6    0.5               2                 0.55       101      0.8000    0.5681826\n  0.3  1          0.6    0.6               1                 0.45        99      0.7936    0.5563138\n  0.3  1          0.6    0.6               1                 0.45       100      0.7936    0.5569457\n  0.3  1          0.6    0.6               1                 0.45       101      0.7904    0.5508814\n  0.3  1          0.6    0.6               1                 0.55        99      0.7824    0.5305113\n  0.3  1          0.6    0.6               1                 0.55       100      0.7856    0.5374017\n  0.3  1          0.6    0.6               1                 0.55       101      0.7904    0.5474332\n  0.3  1          0.6    0.6               2                 0.45        99      0.8032    0.5756352\n  0.3  1          0.6    0.6               2                 0.45       100      0.8080    0.5844591\n  0.3  1          0.6    0.6               2                 0.45       101      0.8032    0.5747264\n  0.3  1          0.6    0.6               2                 0.55        99      0.7904    0.5503540\n  0.3  1          0.6    0.6               2                 0.55       100      0.7920    0.5527563\n  0.3  1          0.6    0.6               2                 0.55       101      0.7920    0.5511523\n  0.3  1          0.6    0.7               1                 0.45        99      0.7984    0.5650223\n  0.3  1          0.6    0.7               1                 0.45       100      0.8048    0.5805166\n  0.3  1          0.6    0.7               1                 0.45       101      0.8016    0.5747020\n  0.3  1          0.6    0.7               1                 0.55        99      0.7936    0.5594105\n  0.3  1          0.6    0.7               1                 0.55       100      0.7984    0.5674121\n  0.3  1          0.6    0.7               1                 0.55       101      0.7952    0.5596154\n  0.3  1          0.6    0.7               2                 0.45        99      0.8064    0.5827225\n  0.3  1          0.6    0.7               2                 0.45       100      0.8032    0.5757635\n  0.3  1          0.6    0.7               2                 0.45       101      0.7968    0.5641691\n  0.3  1          0.6    0.7               2                 0.55        99      0.8032    0.5760319\n  0.3  1          0.6    0.7               2                 0.55       100      0.8032    0.5759375\n  0.3  1          0.6    0.7               2                 0.55       101      0.8064    0.5836489\n  0.3  2          0.1    0.5               1                 0.45        99      0.7984    0.5659478\n  0.3  2          0.1    0.5               1                 0.45       100      0.8000    0.5700399\n  0.3  2          0.1    0.5               1                 0.45       101      0.7984    0.5646069\n  0.3  2          0.1    0.5               1                 0.55        99      0.8032    0.5767185\n  0.3  2          0.1    0.5               1                 0.55       100      0.8048    0.5810878\n  0.3  2          0.1    0.5               1                 0.55       101      0.8016    0.5721704\n  0.3  2          0.1    0.5               2                 0.45        99      0.8128    0.5953302\n  0.3  2          0.1    0.5               2                 0.45       100      0.8176    0.6047191\n  0.3  2          0.1    0.5               2                 0.45       101      0.8144    0.5969698\n  0.3  2          0.1    0.5               2                 0.55        99      0.8016    0.5733767\n  0.3  2          0.1    0.5               2                 0.55       100      0.8112    0.5946178\n  0.3  2          0.1    0.5               2                 0.55       101      0.8096    0.5907259\n  0.3  2          0.1    0.6               1                 0.45        99      0.8096    0.5936793\n  0.3  2          0.1    0.6               1                 0.45       100      0.8032    0.5783268\n  0.3  2          0.1    0.6               1                 0.45       101      0.8048    0.5820366\n  0.3  2          0.1    0.6               1                 0.55        99      0.8144    0.5984934\n  0.3  2          0.1    0.6               1                 0.55       100      0.8080    0.5845359\n  0.3  2          0.1    0.6               1                 0.55       101      0.8144    0.5978417\n  0.3  2          0.1    0.6               2                 0.45        99      0.8096    0.5899417\n  0.3  2          0.1    0.6               2                 0.45       100      0.8048    0.5774758\n  0.3  2          0.1    0.6               2                 0.45       101      0.8064    0.5823657\n  0.3  2          0.1    0.6               2                 0.55        99      0.8032    0.5752657\n  0.3  2          0.1    0.6               2                 0.55       100      0.8032    0.5759264\n  0.3  2          0.1    0.6               2                 0.55       101      0.8080    0.5847341\n  0.3  2          0.1    0.7               1                 0.45        99      0.8064    0.5824642\n  0.3  2          0.1    0.7               1                 0.45       100      0.8016    0.5732246\n  0.3  2          0.1    0.7               1                 0.45       101      0.8048    0.5791015\n  0.3  2          0.1    0.7               1                 0.55        99      0.8144    0.6017616\n  0.3  2          0.1    0.7               1                 0.55       100      0.8096    0.5906118\n  0.3  2          0.1    0.7               1                 0.55       101      0.8112    0.5930176\n  0.3  2          0.1    0.7               2                 0.45        99      0.8016    0.5705171\n  0.3  2          0.1    0.7               2                 0.45       100      0.8016    0.5693442\n  0.3  2          0.1    0.7               2                 0.45       101      0.7984    0.5639016\n  0.3  2          0.1    0.7               2                 0.55        99      0.8032    0.5785788\n  0.3  2          0.1    0.7               2                 0.55       100      0.8048    0.5823144\n  0.3  2          0.1    0.7               2                 0.55       101      0.8112    0.5972521\n  0.3  2          0.6    0.5               1                 0.45        99      0.8112    0.5959995\n  0.3  2          0.6    0.5               1                 0.45       100      0.8016    0.5752524\n  0.3  2          0.6    0.5               1                 0.45       101      0.8048    0.5823796\n  0.3  2          0.6    0.5               1                 0.55        99      0.8048    0.5797549\n  0.3  2          0.6    0.5               1                 0.55       100      0.8064    0.5841294\n  0.3  2          0.6    0.5               1                 0.55       101      0.8032    0.5767271\n  0.3  2          0.6    0.5               2                 0.45        99      0.8048    0.5787174\n  0.3  2          0.6    0.5               2                 0.45       100      0.8064    0.5823871\n  0.3  2          0.6    0.5               2                 0.45       101      0.8144    0.6004399\n  0.3  2          0.6    0.5               2                 0.55        99      0.8224    0.6144061\n  0.3  2          0.6    0.5               2                 0.55       100      0.8176    0.6050864\n  0.3  2          0.6    0.5               2                 0.55       101      0.8144    0.5971932\n  0.3  2          0.6    0.6               1                 0.45        99      0.8080    0.5874888\n  0.3  2          0.6    0.6               1                 0.45       100      0.8048    0.5812470\n  0.3  2          0.6    0.6               1                 0.45       101      0.8048    0.5812451\n  0.3  2          0.6    0.6               1                 0.55        99      0.8112    0.5957263\n  0.3  2          0.6    0.6               1                 0.55       100      0.8064    0.5839784\n  0.3  2          0.6    0.6               1                 0.55       101      0.8048    0.5799662\n  0.3  2          0.6    0.6               2                 0.45        99      0.8080    0.5881105\n  0.3  2          0.6    0.6               2                 0.45       100      0.8160    0.6046260\n  0.3  2          0.6    0.6               2                 0.45       101      0.8192    0.6111008\n  0.3  2          0.6    0.6               2                 0.55        99      0.8160    0.6028799\n  0.3  2          0.6    0.6               2                 0.55       100      0.8160    0.6018224\n  0.3  2          0.6    0.6               2                 0.55       101      0.8048    0.5770912\n  0.3  2          0.6    0.7               1                 0.45        99      0.8096    0.5903734\n  0.3  2          0.6    0.7               1                 0.45       100      0.8096    0.5895973\n  0.3  2          0.6    0.7               1                 0.45       101      0.8080    0.5859832\n  0.3  2          0.6    0.7               1                 0.55        99      0.8000    0.5726832\n  0.3  2          0.6    0.7               1                 0.55       100      0.8016    0.5755417\n  0.3  2          0.6    0.7               1                 0.55       101      0.8016    0.5761515\n  0.3  2          0.6    0.7               2                 0.45        99      0.8192    0.6111638\n  0.3  2          0.6    0.7               2                 0.45       100      0.8240    0.6204912\n  0.3  2          0.6    0.7               2                 0.45       101      0.8128    0.5987658\n  0.3  2          0.6    0.7               2                 0.55        99      0.8128    0.5956023\n  0.3  2          0.6    0.7               2                 0.55       100      0.8096    0.5894324\n  0.3  2          0.6    0.7               2                 0.55       101      0.8160    0.6027735\n  0.3  3          0.1    0.5               1                 0.45        99      0.8048    0.5839084\n  0.3  3          0.1    0.5               1                 0.45       100      0.8112    0.5978148\n  0.3  3          0.1    0.5               1                 0.45       101      0.8096    0.5960775\n  0.3  3          0.1    0.5               1                 0.55        99      0.8096    0.5926541\n  0.3  3          0.1    0.5               1                 0.55       100      0.8096    0.5950085\n  0.3  3          0.1    0.5               1                 0.55       101      0.8128    0.6005444\n  0.3  3          0.1    0.5               2                 0.45        99      0.8208    0.6162723\n  0.3  3          0.1    0.5               2                 0.45       100      0.8080    0.5888067\n  0.3  3          0.1    0.5               2                 0.45       101      0.8096    0.5919595\n  0.3  3          0.1    0.5               2                 0.55        99      0.8144    0.6041075\n  0.3  3          0.1    0.5               2                 0.55       100      0.8176    0.6115186\n  0.3  3          0.1    0.5               2                 0.55       101      0.8160    0.6075260\n  0.3  3          0.1    0.6               1                 0.45        99      0.8112    0.5932933\n  0.3  3          0.1    0.6               1                 0.45       100      0.8144    0.5997478\n  0.3  3          0.1    0.6               1                 0.45       101      0.8080    0.5858388\n  0.3  3          0.1    0.6               1                 0.55        99      0.8160    0.6042108\n  0.3  3          0.1    0.6               1                 0.55       100      0.8224    0.6173801\n  0.3  3          0.1    0.6               1                 0.55       101      0.8208    0.6146349\n  0.3  3          0.1    0.6               2                 0.45        99      0.8096    0.5937302\n  0.3  3          0.1    0.6               2                 0.45       100      0.8064    0.5869546\n  0.3  3          0.1    0.6               2                 0.45       101      0.8032    0.5790146\n  0.3  3          0.1    0.6               2                 0.55        99      0.8208    0.6175102\n  0.3  3          0.1    0.6               2                 0.55       100      0.8192    0.6143484\n  0.3  3          0.1    0.6               2                 0.55       101      0.8208    0.6182049\n  0.3  3          0.1    0.7               1                 0.45        99      0.8144    0.6017376\n  0.3  3          0.1    0.7               1                 0.45       100      0.8096    0.5918062\n  0.3  3          0.1    0.7               1                 0.45       101      0.8096    0.5915797\n  0.3  3          0.1    0.7               1                 0.55        99      0.8048    0.5841298\n  0.3  3          0.1    0.7               1                 0.55       100      0.8080    0.5911333\n  0.3  3          0.1    0.7               1                 0.55       101      0.8032    0.5807817\n  0.3  3          0.1    0.7               2                 0.45        99      0.8048    0.5795917\n  0.3  3          0.1    0.7               2                 0.45       100      0.8048    0.5816915\n  0.3  3          0.1    0.7               2                 0.45       101      0.8064    0.5855839\n  0.3  3          0.1    0.7               2                 0.55        99      0.8064    0.5860909\n  0.3  3          0.1    0.7               2                 0.55       100      0.8080    0.5893588\n  0.3  3          0.1    0.7               2                 0.55       101      0.8080    0.5898876\n  0.3  3          0.6    0.5               1                 0.45        99      0.8112    0.5979589\n  0.3  3          0.6    0.5               1                 0.45       100      0.8112    0.5987583\n  0.3  3          0.6    0.5               1                 0.45       101      0.8080    0.5910979\n  0.3  3          0.6    0.5               1                 0.55        99      0.8016    0.5724942\n  0.3  3          0.6    0.5               1                 0.55       100      0.8144    0.6017634\n  0.3  3          0.6    0.5               1                 0.55       101      0.8112    0.5947893\n  0.3  3          0.6    0.5               2                 0.45        99      0.8224    0.6170823\n  0.3  3          0.6    0.5               2                 0.45       100      0.8272    0.6281251\n  0.3  3          0.6    0.5               2                 0.45       101      0.8256    0.6256626\n  0.3  3          0.6    0.5               2                 0.55        99      0.8144    0.6007998\n  0.3  3          0.6    0.5               2                 0.55       100      0.8064    0.5833309\n  0.3  3          0.6    0.5               2                 0.55       101      0.7984    0.5666751\n  0.3  3          0.6    0.6               1                 0.45        99      0.8048    0.5810873\n  0.3  3          0.6    0.6               1                 0.45       100      0.8080    0.5917270\n  0.3  3          0.6    0.6               1                 0.45       101      0.8112    0.5953732\n  0.3  3          0.6    0.6               1                 0.55        99      0.8032    0.5777434\n  0.3  3          0.6    0.6               1                 0.55       100      0.8032    0.5800519\n  0.3  3          0.6    0.6               1                 0.55       101      0.8048    0.5843351\n  0.3  3          0.6    0.6               2                 0.45        99      0.8160    0.6065282\n  0.3  3          0.6    0.6               2                 0.45       100      0.8208    0.6165114\n  0.3  3          0.6    0.6               2                 0.45       101      0.8160    0.6058498\n  0.3  3          0.6    0.6               2                 0.55        99      0.8048    0.5828182\n  0.3  3          0.6    0.6               2                 0.55       100      0.8160    0.6055006\n  0.3  3          0.6    0.6               2                 0.55       101      0.8080    0.5882109\n  0.3  3          0.6    0.7               1                 0.45        99      0.8080    0.5888975\n  0.3  3          0.6    0.7               1                 0.45       100      0.8016    0.5762706\n  0.3  3          0.6    0.7               1                 0.45       101      0.8032    0.5794309\n  0.3  3          0.6    0.7               1                 0.55        99      0.8048    0.5821583\n  0.3  3          0.6    0.7               1                 0.55       100      0.8048    0.5811598\n  0.3  3          0.6    0.7               1                 0.55       101      0.8112    0.5944848\n  0.3  3          0.6    0.7               2                 0.45        99      0.8064    0.5824673\n  0.3  3          0.6    0.7               2                 0.45       100      0.8096    0.5918120\n  0.3  3          0.6    0.7               2                 0.45       101      0.8080    0.5913199\n  0.3  3          0.6    0.7               2                 0.55        99      0.8080    0.5889894\n  0.3  3          0.6    0.7               2                 0.55       100      0.8016    0.5747475\n  0.3  3          0.6    0.7               2                 0.55       101      0.8048    0.5802493\n  0.4  1          0.1    0.5               1                 0.45        99      0.7968    0.5648428\n  0.4  1          0.1    0.5               1                 0.45       100      0.7968    0.5665363\n  0.4  1          0.1    0.5               1                 0.45       101      0.8016    0.5755251\n  0.4  1          0.1    0.5               1                 0.55        99      0.8000    0.5712934\n  0.4  1          0.1    0.5               1                 0.55       100      0.7968    0.5633200\n  0.4  1          0.1    0.5               1                 0.55       101      0.7984    0.5667448\n  0.4  1          0.1    0.5               2                 0.45        99      0.8064    0.5874486\n  0.4  1          0.1    0.5               2                 0.45       100      0.8112    0.5960756\n  0.4  1          0.1    0.5               2                 0.45       101      0.8160    0.6047659\n  0.4  1          0.1    0.5               2                 0.55        99      0.7888    0.5478920\n  0.4  1          0.1    0.5               2                 0.55       100      0.7936    0.5580187\n  0.4  1          0.1    0.5               2                 0.55       101      0.7888    0.5488473\n  0.4  1          0.1    0.6               1                 0.45        99      0.7984    0.5661084\n  0.4  1          0.1    0.6               1                 0.45       100      0.8032    0.5749515\n  0.4  1          0.1    0.6               1                 0.45       101      0.7968    0.5617222\n  0.4  1          0.1    0.6               1                 0.55        99      0.7984    0.5681274\n  0.4  1          0.1    0.6               1                 0.55       100      0.8000    0.5734568\n  0.4  1          0.1    0.6               1                 0.55       101      0.8032    0.5793127\n  0.4  1          0.1    0.6               2                 0.45        99      0.7952    0.5575754\n  0.4  1          0.1    0.6               2                 0.45       100      0.7856    0.5373404\n  0.4  1          0.1    0.6               2                 0.45       101      0.7840    0.5339368\n  0.4  1          0.1    0.6               2                 0.55        99      0.7888    0.5471573\n  0.4  1          0.1    0.6               2                 0.55       100      0.7888    0.5465656\n  0.4  1          0.1    0.6               2                 0.55       101      0.7904    0.5513458\n  0.4  1          0.1    0.7               1                 0.45        99      0.7888    0.5451314\n  0.4  1          0.1    0.7               1                 0.45       100      0.7888    0.5477034\n  0.4  1          0.1    0.7               1                 0.45       101      0.7936    0.5564137\n  0.4  1          0.1    0.7               1                 0.55        99      0.7888    0.5441428\n  0.4  1          0.1    0.7               1                 0.55       100      0.7888    0.5430693\n  0.4  1          0.1    0.7               1                 0.55       101      0.7952    0.5585235\n  0.4  1          0.1    0.7               2                 0.45        99      0.8096    0.5879311\n  0.4  1          0.1    0.7               2                 0.45       100      0.8080    0.5836482\n  0.4  1          0.1    0.7               2                 0.45       101      0.8064    0.5812550\n  0.4  1          0.1    0.7               2                 0.55        99      0.8048    0.5825363\n  0.4  1          0.1    0.7               2                 0.55       100      0.8096    0.5932939\n  0.4  1          0.1    0.7               2                 0.55       101      0.8112    0.5960429\n  0.4  1          0.6    0.5               1                 0.45        99      0.8032    0.5791064\n  0.4  1          0.6    0.5               1                 0.45       100      0.8064    0.5855774\n  0.4  1          0.6    0.5               1                 0.45       101      0.8000    0.5717159\n  0.4  1          0.6    0.5               1                 0.55        99      0.8016    0.5710198\n  0.4  1          0.6    0.5               1                 0.55       100      0.7920    0.5480072\n  0.4  1          0.6    0.5               1                 0.55       101      0.7920    0.5502826\n  0.4  1          0.6    0.5               2                 0.45        99      0.7936    0.5640744\n  0.4  1          0.6    0.5               2                 0.45       100      0.7968    0.5681072\n  0.4  1          0.6    0.5               2                 0.45       101      0.7968    0.5704825\n  0.4  1          0.6    0.5               2                 0.55        99      0.8096    0.5930286\n  0.4  1          0.6    0.5               2                 0.55       100      0.8080    0.5897734\n  0.4  1          0.6    0.5               2                 0.55       101      0.8064    0.5879978\n  0.4  1          0.6    0.6               1                 0.45        99      0.8064    0.5861577\n  0.4  1          0.6    0.6               1                 0.45       100      0.8048    0.5815993\n  0.4  1          0.6    0.6               1                 0.45       101      0.7984    0.5657947\n  0.4  1          0.6    0.6               1                 0.55        99      0.7968    0.5624708\n  0.4  1          0.6    0.6               1                 0.55       100      0.7984    0.5658045\n  0.4  1          0.6    0.6               1                 0.55       101      0.7936    0.5576578\n  0.4  1          0.6    0.6               2                 0.45        99      0.8032    0.5763327\n  0.4  1          0.6    0.6               2                 0.45       100      0.7968    0.5628419\n  0.4  1          0.6    0.6               2                 0.45       101      0.7936    0.5553905\n  0.4  1          0.6    0.6               2                 0.55        99      0.8112    0.5930763\n  0.4  1          0.6    0.6               2                 0.55       100      0.8048    0.5764562\n  0.4  1          0.6    0.6               2                 0.55       101      0.8080    0.5844435\n  0.4  1          0.6    0.7               1                 0.45        99      0.7872    0.5397521\n  0.4  1          0.6    0.7               1                 0.45       100      0.7872    0.5404324\n  0.4  1          0.6    0.7               1                 0.45       101      0.7936    0.5536214\n  0.4  1          0.6    0.7               1                 0.55        99      0.7936    0.5583798\n  0.4  1          0.6    0.7               1                 0.55       100      0.7920    0.5563395\n  0.4  1          0.6    0.7               1                 0.55       101      0.8000    0.5725743\n  0.4  1          0.6    0.7               2                 0.45        99      0.8048    0.5785592\n  0.4  1          0.6    0.7               2                 0.45       100      0.8000    0.5696905\n  0.4  1          0.6    0.7               2                 0.45       101      0.8000    0.5692470\n  0.4  1          0.6    0.7               2                 0.55        99      0.8032    0.5770293\n  0.4  1          0.6    0.7               2                 0.55       100      0.7984    0.5663165\n  0.4  1          0.6    0.7               2                 0.55       101      0.8000    0.5709361\n  0.4  2          0.1    0.5               1                 0.45        99      0.8144    0.5982400\n  0.4  2          0.1    0.5               1                 0.45       100      0.8208    0.6150196\n  0.4  2          0.1    0.5               1                 0.45       101      0.8256    0.6255417\n  0.4  2          0.1    0.5               1                 0.55        99      0.8240    0.6220474\n  0.4  2          0.1    0.5               1                 0.55       100      0.8256    0.6249393\n  0.4  2          0.1    0.5               1                 0.55       101      0.8224    0.6173992\n  0.4  2          0.1    0.5               2                 0.45        99      0.8208    0.6127408\n  0.4  2          0.1    0.5               2                 0.45       100      0.8160    0.6039160\n  0.4  2          0.1    0.5               2                 0.45       101      0.8096    0.5890128\n  0.4  2          0.1    0.5               2                 0.55        99      0.8048    0.5773155\n  0.4  2          0.1    0.5               2                 0.55       100      0.8208    0.6129043\n  0.4  2          0.1    0.5               2                 0.55       101      0.8160    0.6023447\n  0.4  2          0.1    0.6               1                 0.45        99      0.8080    0.5899733\n  0.4  2          0.1    0.6               1                 0.45       100      0.8048    0.5817091\n  0.4  2          0.1    0.6               1                 0.45       101      0.8016    0.5742258\n  0.4  2          0.1    0.6               1                 0.55        99      0.8144    0.6012797\n  0.4  2          0.1    0.6               1                 0.55       100      0.8192    0.6116334\n  0.4  2          0.1    0.6               1                 0.55       101      0.8128    0.5997723\n  0.4  2          0.1    0.6               2                 0.45        99      0.8096    0.5919074\n  0.4  2          0.1    0.6               2                 0.45       100      0.8112    0.5950910\n  0.4  2          0.1    0.6               2                 0.45       101      0.8128    0.5988078\n  0.4  2          0.1    0.6               2                 0.55        99      0.8032    0.5767340\n  0.4  2          0.1    0.6               2                 0.55       100      0.8096    0.5904993\n  0.4  2          0.1    0.6               2                 0.55       101      0.8128    0.5967108\n  0.4  2          0.1    0.7               1                 0.45        99      0.7984    0.5715282\n  0.4  2          0.1    0.7               1                 0.45       100      0.7936    0.5603396\n  0.4  2          0.1    0.7               1                 0.45       101      0.8000    0.5733208\n  0.4  2          0.1    0.7               1                 0.55        99      0.8080    0.5900799\n  0.4  2          0.1    0.7               1                 0.55       100      0.8032    0.5788321\n  0.4  2          0.1    0.7               1                 0.55       101      0.8048    0.5818428\n  0.4  2          0.1    0.7               2                 0.45        99      0.7936    0.5570264\n  0.4  2          0.1    0.7               2                 0.45       100      0.7968    0.5604493\n  0.4  2          0.1    0.7               2                 0.45       101      0.7952    0.5579014\n  0.4  2          0.1    0.7               2                 0.55        99      0.8112    0.5972013\n  0.4  2          0.1    0.7               2                 0.55       100      0.8096    0.5934287\n  0.4  2          0.1    0.7               2                 0.55       101      0.8032    0.5803223\n  0.4  2          0.6    0.5               1                 0.45        99      0.7888    0.5469905\n  0.4  2          0.6    0.5               1                 0.45       100      0.7872    0.5441030\n  0.4  2          0.6    0.5               1                 0.45       101      0.7904    0.5501000\n  0.4  2          0.6    0.5               1                 0.55        99      0.8112    0.5916252\n  0.4  2          0.6    0.5               1                 0.55       100      0.8128    0.5968697\n  0.4  2          0.6    0.5               1                 0.55       101      0.8112    0.5917534\n  0.4  2          0.6    0.5               2                 0.45        99      0.8032    0.5797886\n  0.4  2          0.6    0.5               2                 0.45       100      0.8160    0.6050679\n  0.4  2          0.6    0.5               2                 0.45       101      0.8112    0.5937308\n  0.4  2          0.6    0.5               2                 0.55        99      0.8144    0.6012854\n  0.4  2          0.6    0.5               2                 0.55       100      0.8144    0.5998335\n  0.4  2          0.6    0.5               2                 0.55       101      0.8112    0.5938388\n  0.4  2          0.6    0.6               1                 0.45        99      0.8000    0.5725380\n  0.4  2          0.6    0.6               1                 0.45       100      0.8048    0.5831854\n  0.4  2          0.6    0.6               1                 0.45       101      0.8048    0.5830149\n  0.4  2          0.6    0.6               1                 0.55        99      0.8032    0.5765950\n  0.4  2          0.6    0.6               1                 0.55       100      0.7968    0.5649878\n  0.4  2          0.6    0.6               1                 0.55       101      0.8000    0.5707581\n  0.4  2          0.6    0.6               2                 0.45        99      0.8144    0.5992945\n  0.4  2          0.6    0.6               2                 0.45       100      0.8112    0.5918309\n  0.4  2          0.6    0.6               2                 0.45       101      0.8128    0.5972461\n  0.4  2          0.6    0.6               2                 0.55        99      0.8048    0.5762582\n  0.4  2          0.6    0.6               2                 0.55       100      0.8144    0.5979076\n  0.4  2          0.6    0.6               2                 0.55       101      0.8224    0.6136496\n  0.4  2          0.6    0.7               1                 0.45        99      0.8128    0.5978036\n  0.4  2          0.6    0.7               1                 0.45       100      0.7984    0.5679823\n  0.4  2          0.6    0.7               1                 0.45       101      0.8016    0.5727478\n  0.4  2          0.6    0.7               1                 0.55        99      0.8160    0.6089290\n  0.4  2          0.6    0.7               1                 0.55       100      0.8160    0.6083179\n  0.4  2          0.6    0.7               1                 0.55       101      0.8064    0.5864786\n  0.4  2          0.6    0.7               2                 0.45        99      0.8080    0.5879538\n  0.4  2          0.6    0.7               2                 0.45       100      0.8160    0.6046354\n  0.4  2          0.6    0.7               2                 0.45       101      0.8144    0.6030495\n  0.4  2          0.6    0.7               2                 0.55        99      0.8256    0.6236890\n  0.4  2          0.6    0.7               2                 0.55       100      0.8256    0.6254991\n  0.4  2          0.6    0.7               2                 0.55       101      0.8304    0.6359992\n  0.4  3          0.1    0.5               1                 0.45        99      0.8000    0.5739537\n  0.4  3          0.1    0.5               1                 0.45       100      0.7952    0.5667440\n  0.4  3          0.1    0.5               1                 0.45       101      0.8048    0.5855250\n  0.4  3          0.1    0.5               1                 0.55        99      0.8144    0.6046439\n  0.4  3          0.1    0.5               1                 0.55       100      0.8128    0.6012938\n  0.4  3          0.1    0.5               1                 0.55       101      0.8064    0.5870635\n  0.4  3          0.1    0.5               2                 0.45        99      0.8224    0.6157011\n  0.4  3          0.1    0.5               2                 0.45       100      0.8192    0.6088574\n  0.4  3          0.1    0.5               2                 0.45       101      0.8256    0.6234331\n  0.4  3          0.1    0.5               2                 0.55        99      0.8112    0.5966845\n  0.4  3          0.1    0.5               2                 0.55       100      0.8032    0.5825946\n  0.4  3          0.1    0.5               2                 0.55       101      0.8080    0.5945848\n  0.4  3          0.1    0.6               1                 0.45        99      0.7984    0.5700555\n  0.4  3          0.1    0.6               1                 0.45       100      0.7984    0.5691527\n  0.4  3          0.1    0.6               1                 0.45       101      0.7984    0.5711980\n  0.4  3          0.1    0.6               1                 0.55        99      0.7936    0.5576469\n  0.4  3          0.1    0.6               1                 0.55       100      0.7984    0.5700712\n  0.4  3          0.1    0.6               1                 0.55       101      0.7984    0.5697263\n  0.4  3          0.1    0.6               2                 0.45        99      0.7952    0.5613850\n  0.4  3          0.1    0.6               2                 0.45       100      0.7984    0.5695710\n  0.4  3          0.1    0.6               2                 0.45       101      0.8080    0.5884539\n  0.4  3          0.1    0.6               2                 0.55        99      0.8000    0.5756429\n  0.4  3          0.1    0.6               2                 0.55       100      0.7920    0.5581939\n  0.4  3          0.1    0.6               2                 0.55       101      0.7952    0.5640903\n  0.4  3          0.1    0.7               1                 0.45        99      0.8080    0.5867722\n  0.4  3          0.1    0.7               1                 0.45       100      0.8096    0.5901813\n  0.4  3          0.1    0.7               1                 0.45       101      0.8080    0.5894819\n  0.4  3          0.1    0.7               1                 0.55        99      0.7952    0.5616378\n  0.4  3          0.1    0.7               1                 0.55       100      0.7984    0.5702454\n  0.4  3          0.1    0.7               1                 0.55       101      0.7984    0.5692306\n  0.4  3          0.1    0.7               2                 0.45        99      0.8192    0.6144148\n  0.4  3          0.1    0.7               2                 0.45       100      0.8144    0.6045889\n  0.4  3          0.1    0.7               2                 0.45       101      0.8176    0.6116084\n  0.4  3          0.1    0.7               2                 0.55        99      0.8016    0.5733535\n  0.4  3          0.1    0.7               2                 0.55       100      0.7968    0.5634136\n  0.4  3          0.1    0.7               2                 0.55       101      0.7984    0.5674154\n  0.4  3          0.6    0.5               1                 0.45        99      0.7920    0.5543971\n  0.4  3          0.6    0.5               1                 0.45       100      0.7920    0.5539615\n  0.4  3          0.6    0.5               1                 0.45       101      0.8064    0.5862700\n  0.4  3          0.6    0.5               1                 0.55        99      0.7984    0.5698990\n  0.4  3          0.6    0.5               1                 0.55       100      0.7968    0.5668926\n  0.4  3          0.6    0.5               1                 0.55       101      0.8000    0.5728400\n  0.4  3          0.6    0.5               2                 0.45        99      0.8080    0.5864349\n  0.4  3          0.6    0.5               2                 0.45       100      0.8048    0.5801866\n  0.4  3          0.6    0.5               2                 0.45       101      0.7920    0.5536591\n  0.4  3          0.6    0.5               2                 0.55        99      0.8000    0.5712036\n  0.4  3          0.6    0.5               2                 0.55       100      0.8080    0.5865977\n  0.4  3          0.6    0.5               2                 0.55       101      0.8064    0.5840325\n  0.4  3          0.6    0.6               1                 0.45        99      0.7904    0.5543360\n  0.4  3          0.6    0.6               1                 0.45       100      0.7888    0.5471223\n  0.4  3          0.6    0.6               1                 0.45       101      0.8016    0.5767752\n  0.4  3          0.6    0.6               1                 0.55        99      0.8000    0.5701150\n  0.4  3          0.6    0.6               1                 0.55       100      0.7968    0.5624274\n  0.4  3          0.6    0.6               1                 0.55       101      0.7984    0.5650349\n  0.4  3          0.6    0.6               2                 0.45        99      0.7984    0.5649638\n  0.4  3          0.6    0.6               2                 0.45       100      0.7936    0.5564593\n  0.4  3          0.6    0.6               2                 0.45       101      0.8000    0.5711640\n  0.4  3          0.6    0.6               2                 0.55        99      0.8048    0.5835703\n  0.4  3          0.6    0.6               2                 0.55       100      0.8144    0.6044045\n  0.4  3          0.6    0.6               2                 0.55       101      0.8096    0.5950353\n  0.4  3          0.6    0.7               1                 0.45        99      0.8016    0.5772039\n  0.4  3          0.6    0.7               1                 0.45       100      0.8048    0.5851602\n  0.4  3          0.6    0.7               1                 0.45       101      0.7936    0.5619635\n  0.4  3          0.6    0.7               1                 0.55        99      0.8064    0.5895548\n  0.4  3          0.6    0.7               1                 0.55       100      0.8096    0.5966714\n  0.4  3          0.6    0.7               1                 0.55       101      0.8096    0.5970981\n  0.4  3          0.6    0.7               2                 0.45        99      0.8000    0.5720680\n  0.4  3          0.6    0.7               2                 0.45       100      0.7952    0.5613857\n  0.4  3          0.6    0.7               2                 0.45       101      0.8000    0.5705116\n  0.4  3          0.6    0.7               2                 0.55        99      0.8000    0.5739845\n  0.4  3          0.6    0.7               2                 0.55       100      0.7936    0.5613344\n  0.4  3          0.6    0.7               2                 0.55       101      0.7968    0.5670451\n  0.5  1          0.1    0.5               1                 0.45        99      0.8032    0.5763495\n  0.5  1          0.1    0.5               1                 0.45       100      0.7936    0.5551323\n  0.5  1          0.1    0.5               1                 0.45       101      0.7840    0.5338263\n  0.5  1          0.1    0.5               1                 0.55        99      0.7968    0.5661053\n  0.5  1          0.1    0.5               1                 0.55       100      0.7968    0.5662083\n  0.5  1          0.1    0.5               1                 0.55       101      0.8048    0.5832938\n  0.5  1          0.1    0.5               2                 0.45        99      0.7936    0.5567226\n  0.5  1          0.1    0.5               2                 0.45       100      0.7952    0.5629058\n  0.5  1          0.1    0.5               2                 0.45       101      0.7968    0.5654417\n  0.5  1          0.1    0.5               2                 0.55        99      0.8064    0.5850504\n  0.5  1          0.1    0.5               2                 0.55       100      0.8080    0.5890217\n  0.5  1          0.1    0.5               2                 0.55       101      0.7984    0.5692429\n  0.5  1          0.1    0.6               1                 0.45        99      0.7968    0.5628308\n  0.5  1          0.1    0.6               1                 0.45       100      0.7968    0.5635390\n  0.5  1          0.1    0.6               1                 0.45       101      0.7904    0.5492666\n  0.5  1          0.1    0.6               1                 0.55        99      0.8064    0.5848651\n  0.5  1          0.1    0.6               1                 0.55       100      0.8096    0.5908818\n  0.5  1          0.1    0.6               1                 0.55       101      0.8096    0.5909583\n  0.5  1          0.1    0.6               2                 0.45        99      0.8144    0.5998746\n  0.5  1          0.1    0.6               2                 0.45       100      0.8112    0.5942978\n  0.5  1          0.1    0.6               2                 0.45       101      0.8128    0.5990446\n  0.5  1          0.1    0.6               2                 0.55        99      0.7904    0.5435422\n  0.5  1          0.1    0.6               2                 0.55       100      0.7936    0.5547012\n  0.5  1          0.1    0.6               2                 0.55       101      0.7952    0.5572543\n  0.5  1          0.1    0.7               1                 0.45        99      0.8048    0.5867834\n  0.5  1          0.1    0.7               1                 0.45       100      0.8032    0.5816360\n  0.5  1          0.1    0.7               1                 0.45       101      0.8032    0.5814908\n  0.5  1          0.1    0.7               1                 0.55        99      0.7856    0.5393567\n  0.5  1          0.1    0.7               1                 0.55       100      0.7792    0.5255469\n  0.5  1          0.1    0.7               1                 0.55       101      0.7808    0.5293043\n  0.5  1          0.1    0.7               2                 0.45        99      0.8128    0.5976354\n  0.5  1          0.1    0.7               2                 0.45       100      0.8144    0.5986222\n  0.5  1          0.1    0.7               2                 0.45       101      0.8144    0.6011537\n  0.5  1          0.1    0.7               2                 0.55        99      0.8032    0.5783321\n  0.5  1          0.1    0.7               2                 0.55       100      0.7968    0.5642749\n  0.5  1          0.1    0.7               2                 0.55       101      0.7952    0.5634869\n  0.5  1          0.6    0.5               1                 0.45        99      0.8000    0.5725969\n  0.5  1          0.6    0.5               1                 0.45       100      0.8000    0.5744279\n  0.5  1          0.6    0.5               1                 0.45       101      0.8032    0.5826629\n  0.5  1          0.6    0.5               1                 0.55        99      0.7840    0.5400600\n  0.5  1          0.6    0.5               1                 0.55       100      0.7888    0.5497872\n  0.5  1          0.6    0.5               1                 0.55       101      0.7872    0.5444276\n  0.5  1          0.6    0.5               2                 0.45        99      0.7984    0.5672880\n  0.5  1          0.6    0.5               2                 0.45       100      0.7968    0.5626342\n  0.5  1          0.6    0.5               2                 0.45       101      0.8000    0.5714996\n  0.5  1          0.6    0.5               2                 0.55        99      0.7920    0.5531878\n  0.5  1          0.6    0.5               2                 0.55       100      0.7984    0.5700373\n  0.5  1          0.6    0.5               2                 0.55       101      0.7904    0.5533853\n  0.5  1          0.6    0.6               1                 0.45        99      0.8000    0.5698651\n  0.5  1          0.6    0.6               1                 0.45       100      0.7872    0.5435998\n  0.5  1          0.6    0.6               1                 0.45       101      0.8064    0.5832235\n  0.5  1          0.6    0.6               1                 0.55        99      0.7968    0.5675827\n  0.5  1          0.6    0.6               1                 0.55       100      0.8000    0.5731900\n  0.5  1          0.6    0.6               1                 0.55       101      0.8016    0.5752530\n  0.5  1          0.6    0.6               2                 0.45        99      0.8048    0.5859595\n  0.5  1          0.6    0.6               2                 0.45       100      0.8048    0.5871449\n  0.5  1          0.6    0.6               2                 0.45       101      0.8080    0.5920804\n  0.5  1          0.6    0.6               2                 0.55        99      0.8096    0.5921209\n  0.5  1          0.6    0.6               2                 0.55       100      0.8064    0.5822818\n  0.5  1          0.6    0.6               2                 0.55       101      0.8080    0.5858828\n  0.5  1          0.6    0.7               1                 0.45        99      0.7920    0.5532564\n  0.5  1          0.6    0.7               1                 0.45       100      0.7952    0.5614666\n  0.5  1          0.6    0.7               1                 0.45       101      0.8000    0.5733456\n  0.5  1          0.6    0.7               1                 0.55        99      0.8192    0.6140360\n  0.5  1          0.6    0.7               1                 0.55       100      0.8144    0.6036613\n  0.5  1          0.6    0.7               1                 0.55       101      0.8192    0.6146661\n  0.5  1          0.6    0.7               2                 0.45        99      0.8048    0.5786991\n  0.5  1          0.6    0.7               2                 0.45       100      0.8080    0.5849150\n  0.5  1          0.6    0.7               2                 0.45       101      0.7952    0.5598728\n  0.5  1          0.6    0.7               2                 0.55        99      0.8016    0.5749519\n  0.5  1          0.6    0.7               2                 0.55       100      0.7968    0.5653279\n  0.5  1          0.6    0.7               2                 0.55       101      0.7968    0.5645514\n  0.5  2          0.1    0.5               1                 0.45        99      0.8128    0.5989748\n  0.5  2          0.1    0.5               1                 0.45       100      0.8096    0.5935491\n  0.5  2          0.1    0.5               1                 0.45       101      0.8144    0.6031455\n  0.5  2          0.1    0.5               1                 0.55        99      0.8144    0.6027193\n  0.5  2          0.1    0.5               1                 0.55       100      0.8064    0.5853639\n  0.5  2          0.1    0.5               1                 0.55       101      0.8112    0.5950285\n  0.5  2          0.1    0.5               2                 0.45        99      0.8048    0.5845094\n  0.5  2          0.1    0.5               2                 0.45       100      0.7984    0.5713613\n  0.5  2          0.1    0.5               2                 0.45       101      0.8096    0.5956802\n  0.5  2          0.1    0.5               2                 0.55        99      0.8048    0.5799867\n  0.5  2          0.1    0.5               2                 0.55       100      0.8048    0.5800400\n  0.5  2          0.1    0.5               2                 0.55       101      0.8032    0.5769708\n  0.5  2          0.1    0.6               1                 0.45        99      0.7968    0.5632408\n  0.5  2          0.1    0.6               1                 0.45       100      0.8016    0.5732302\n  0.5  2          0.1    0.6               1                 0.45       101      0.8096    0.5909158\n  0.5  2          0.1    0.6               1                 0.55        99      0.7968    0.5649963\n  0.5  2          0.1    0.6               1                 0.55       100      0.8032    0.5814555\n  0.5  2          0.1    0.6               1                 0.55       101      0.8096    0.5939723\n  0.5  2          0.1    0.6               2                 0.45        99      0.8000    0.5687592\n  0.5  2          0.1    0.6               2                 0.45       100      0.8000    0.5683392\n  0.5  2          0.1    0.6               2                 0.45       101      0.8032    0.5736847\n  0.5  2          0.1    0.6               2                 0.55        99      0.8176    0.6083516\n  0.5  2          0.1    0.6               2                 0.55       100      0.8176    0.6090571\n  0.5  2          0.1    0.6               2                 0.55       101      0.8192    0.6131141\n  0.5  2          0.1    0.7               1                 0.45        99      0.8080    0.5868709\n  0.5  2          0.1    0.7               1                 0.45       100      0.8032    0.5758353\n  0.5  2          0.1    0.7               1                 0.45       101      0.8032    0.5771500\n  0.5  2          0.1    0.7               1                 0.55        99      0.8048    0.5822481\n  0.5  2          0.1    0.7               1                 0.55       100      0.8032    0.5804910\n  0.5  2          0.1    0.7               1                 0.55       101      0.8000    0.5730032\n  0.5  2          0.1    0.7               2                 0.45        99      0.7984    0.5707673\n  0.5  2          0.1    0.7               2                 0.45       100      0.7952    0.5629595\n  0.5  2          0.1    0.7               2                 0.45       101      0.7920    0.5529894\n  0.5  2          0.1    0.7               2                 0.55        99      0.8080    0.5901246\n  0.5  2          0.1    0.7               2                 0.55       100      0.8064    0.5860440\n  0.5  2          0.1    0.7               2                 0.55       101      0.8000    0.5730932\n  0.5  2          0.6    0.5               1                 0.45        99      0.7968    0.5673774\n  0.5  2          0.6    0.5               1                 0.45       100      0.7968    0.5681916\n  0.5  2          0.6    0.5               1                 0.45       101      0.7984    0.5725797\n  0.5  2          0.6    0.5               1                 0.55        99      0.7952    0.5640566\n  0.5  2          0.6    0.5               1                 0.55       100      0.7936    0.5585354\n  0.5  2          0.6    0.5               1                 0.55       101      0.7888    0.5467925\n  0.5  2          0.6    0.5               2                 0.45        99      0.7936    0.5542431\n  0.5  2          0.6    0.5               2                 0.45       100      0.7952    0.5608133\n  0.5  2          0.6    0.5               2                 0.45       101      0.7952    0.5575078\n  0.5  2          0.6    0.5               2                 0.55        99      0.8160    0.6039908\n  0.5  2          0.6    0.5               2                 0.55       100      0.8096    0.5920139\n  0.5  2          0.6    0.5               2                 0.55       101      0.8208    0.6171648\n  0.5  2          0.6    0.6               1                 0.45        99      0.8112    0.5949845\n  0.5  2          0.6    0.6               1                 0.45       100      0.8000    0.5693836\n  0.5  2          0.6    0.6               1                 0.45       101      0.8064    0.5845865\n  0.5  2          0.6    0.6               1                 0.55        99      0.8016    0.5750297\n  0.5  2          0.6    0.6               1                 0.55       100      0.8000    0.5733590\n  0.5  2          0.6    0.6               1                 0.55       101      0.8032    0.5804208\n  0.5  2          0.6    0.6               2                 0.45        99      0.8032    0.5786274\n  0.5  2          0.6    0.6               2                 0.45       100      0.8096    0.5930928\n  0.5  2          0.6    0.6               2                 0.45       101      0.8096    0.5933705\n  0.5  2          0.6    0.6               2                 0.55        99      0.8000    0.5716383\n  0.5  2          0.6    0.6               2                 0.55       100      0.7968    0.5658482\n  0.5  2          0.6    0.6               2                 0.55       101      0.7952    0.5640643\n  0.5  2          0.6    0.7               1                 0.45        99      0.7952    0.5666841\n  0.5  2          0.6    0.7               1                 0.45       100      0.7936    0.5628605\n  0.5  2          0.6    0.7               1                 0.45       101      0.7888    0.5512151\n  0.5  2          0.6    0.7               1                 0.55        99      0.8064    0.5841222\n  0.5  2          0.6    0.7               1                 0.55       100      0.8096    0.5910420\n  0.5  2          0.6    0.7               1                 0.55       101      0.8032    0.5792084\n  0.5  2          0.6    0.7               2                 0.45        99      0.8160    0.6048872\n  0.5  2          0.6    0.7               2                 0.45       100      0.8144    0.5980613\n  0.5  2          0.6    0.7               2                 0.45       101      0.8176    0.6065533\n  0.5  2          0.6    0.7               2                 0.55        99      0.8176    0.6101146\n  0.5  2          0.6    0.7               2                 0.55       100      0.8048    0.5840427\n  0.5  2          0.6    0.7               2                 0.55       101      0.8080    0.5914939\n  0.5  3          0.1    0.5               1                 0.45        99      0.8128    0.5978386\n  0.5  3          0.1    0.5               1                 0.45       100      0.8160    0.6035074\n  0.5  3          0.1    0.5               1                 0.45       101      0.8224    0.6181951\n  0.5  3          0.1    0.5               1                 0.55        99      0.7888    0.5512073\n  0.5  3          0.1    0.5               1                 0.55       100      0.7888    0.5504228\n  0.5  3          0.1    0.5               1                 0.55       101      0.7872    0.5471532\n  0.5  3          0.1    0.5               2                 0.45        99      0.8064    0.5876166\n  0.5  3          0.1    0.5               2                 0.45       100      0.8160    0.6055529\n  0.5  3          0.1    0.5               2                 0.45       101      0.8080    0.5872798\n  0.5  3          0.1    0.5               2                 0.55        99      0.7936    0.5607528\n  0.5  3          0.1    0.5               2                 0.55       100      0.7984    0.5723670\n  0.5  3          0.1    0.5               2                 0.55       101      0.7888    0.5500899\n  0.5  3          0.1    0.6               1                 0.45        99      0.8016    0.5758417\n  0.5  3          0.1    0.6               1                 0.45       100      0.8032    0.5784385\n  0.5  3          0.1    0.6               1                 0.45       101      0.8000    0.5717967\n  0.5  3          0.1    0.6               1                 0.55        99      0.8064    0.5873364\n  0.5  3          0.1    0.6               1                 0.55       100      0.7984    0.5706933\n  0.5  3          0.1    0.6               1                 0.55       101      0.7984    0.5695325\n  0.5  3          0.1    0.6               2                 0.45        99      0.7952    0.5638030\n  0.5  3          0.1    0.6               2                 0.45       100      0.8032    0.5799274\n  0.5  3          0.1    0.6               2                 0.45       101      0.8016    0.5775043\n  0.5  3          0.1    0.6               2                 0.55        99      0.8000    0.5715343\n  0.5  3          0.1    0.6               2                 0.55       100      0.8000    0.5710062\n  0.5  3          0.1    0.6               2                 0.55       101      0.7936    0.5599672\n  0.5  3          0.1    0.7               1                 0.45        99      0.7920    0.5584895\n  0.5  3          0.1    0.7               1                 0.45       100      0.7904    0.5534373\n  0.5  3          0.1    0.7               1                 0.45       101      0.7888    0.5496257\n  0.5  3          0.1    0.7               1                 0.55        99      0.8080    0.5961297\n  0.5  3          0.1    0.7               1                 0.55       100      0.8128    0.6040817\n  0.5  3          0.1    0.7               1                 0.55       101      0.8128    0.6031082\n  0.5  3          0.1    0.7               2                 0.45        99      0.7952    0.5607323\n  0.5  3          0.1    0.7               2                 0.45       100      0.8016    0.5749702\n  0.5  3          0.1    0.7               2                 0.45       101      0.7936    0.5572030\n  0.5  3          0.1    0.7               2                 0.55        99      0.8080    0.5865582\n  0.5  3          0.1    0.7               2                 0.55       100      0.8112    0.5932810\n  0.5  3          0.1    0.7               2                 0.55       101      0.8144    0.5996192\n  0.5  3          0.6    0.5               1                 0.45        99      0.8048    0.5821933\n  0.5  3          0.6    0.5               1                 0.45       100      0.8000    0.5697399\n  0.5  3          0.6    0.5               1                 0.45       101      0.8064    0.5828413\n  0.5  3          0.6    0.5               1                 0.55        99      0.8096    0.5928874\n  0.5  3          0.6    0.5               1                 0.55       100      0.8128    0.6006056\n  0.5  3          0.6    0.5               1                 0.55       101      0.8096    0.5915135\n  0.5  3          0.6    0.5               2                 0.45        99      0.8048    0.5814272\n  0.5  3          0.6    0.5               2                 0.45       100      0.8016    0.5748128\n  0.5  3          0.6    0.5               2                 0.45       101      0.8096    0.5905967\n  0.5  3          0.6    0.5               2                 0.55        99      0.8000    0.5719155\n  0.5  3          0.6    0.5               2                 0.55       100      0.8048    0.5844191\n  0.5  3          0.6    0.5               2                 0.55       101      0.8016    0.5773891\n  0.5  3          0.6    0.6               1                 0.45        99      0.7904    0.5521494\n  0.5  3          0.6    0.6               1                 0.45       100      0.7984    0.5687816\n  0.5  3          0.6    0.6               1                 0.45       101      0.8048    0.5799408\n  0.5  3          0.6    0.6               1                 0.55        99      0.7952    0.5695903\n  0.5  3          0.6    0.6               1                 0.55       100      0.8016    0.5817920\n  0.5  3          0.6    0.6               1                 0.55       101      0.8000    0.5775279\n  0.5  3          0.6    0.6               2                 0.45        99      0.7968    0.5603817\n  0.5  3          0.6    0.6               2                 0.45       100      0.7968    0.5612484\n  0.5  3          0.6    0.6               2                 0.45       101      0.7936    0.5558168\n  0.5  3          0.6    0.6               2                 0.55        99      0.8096    0.5919859\n  0.5  3          0.6    0.6               2                 0.55       100      0.8144    0.6024666\n  0.5  3          0.6    0.6               2                 0.55       101      0.8128    0.5991701\n  0.5  3          0.6    0.7               1                 0.45        99      0.7968    0.5695434\n  0.5  3          0.6    0.7               1                 0.45       100      0.7936    0.5622803\n  0.5  3          0.6    0.7               1                 0.45       101      0.8016    0.5788004\n  0.5  3          0.6    0.7               1                 0.55        99      0.7984    0.5742120\n  0.5  3          0.6    0.7               1                 0.55       100      0.7968    0.5697453\n  0.5  3          0.6    0.7               1                 0.55       101      0.7968    0.5705984\n  0.5  3          0.6    0.7               2                 0.45        99      0.8032    0.5769559\n  0.5  3          0.6    0.7               2                 0.45       100      0.8064    0.5854369\n  0.5  3          0.6    0.7               2                 0.45       101      0.7984    0.5684251\n  0.5  3          0.6    0.7               2                 0.55        99      0.8144    0.6060192\n  0.5  3          0.6    0.7               2                 0.55       100      0.8128    0.6012620\n  0.5  3          0.6    0.7               2                 0.55       101      0.8096    0.5951375\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 101, max_depth = 2, eta = 0.4, gamma = 0.6, colsample_bytree = 0.7, min_child_weight = 2 and subsample = 0.55.\n\nxgb.tune.fit$bestTune                                                 # 최적의 초모수 조합값\n\n    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample\n360     101         2 0.4   0.6              0.7                2      0.55\n\n\nResult! (eta = 0.4, max_depth = 2, gamma = 0.6, colsample_bytree = 0.7, min_child_weight = 2, subsample = 0.55, nrounds = 101)일 때 정확도가 가장 높은 것을 알 수 있으며, (eta = 0.4, max_depth = 2, gamma = 0.6, colsample_bytree = 0.7, min_child_weight = 2, subsample = 0.55, nrounds = 101)를 가지는 모형을 최적의 훈련된 모형으로 선택한다.\n\n# 변수 중요도\nimportance &lt;- xgboost::xgb.importance(model =  xgb.tune.fit$finalModel)\n\nimportance\n\n   Feature       Gain      Cover  Frequency\n    &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n1:    Fare 0.32508276 0.39859069 0.40370370\n2:     Age 0.24472964 0.31555757 0.31851852\n3: Sexmale 0.22943973 0.09741137 0.08888889\n4: FamSize 0.10861218 0.09341485 0.10740741\n5: Pclass3 0.07158805 0.05498747 0.04444444\n6: Pclass2 0.02054765 0.04003806 0.03703704\n\n# 변수 중요도 plot\nxgboost::xgb.plot.importance(importance_matrix = importance) \n\n\n\n\n\n\n\n\nResult! 변수 Fare이 Target Survived을 분류하는 데 있어 중요하다.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#모형-평가",
    "href": "XGBoost.html#모형-평가",
    "title": "13  XGBoost",
    "section": "13.7 모형 평가",
    "text": "13.7 모형 평가\nCaution! 모형 평가를 위해 Test Dataset에 대한 예측 class/확률 이 필요하며, 함수 predict()를 이용하여 생성한다.\n\n# 예측 class 생성 \ntest.xgb.class &lt;- predict(xgb.tune.fit,\n                          newdata = titanic.ted.Imp[,-1]) # Test Dataset including Only 예측 변수 \n\ntest.xgb.class\n\n  [1] yes no  no  yes no  no  yes no  no  yes no  no  yes yes no  yes no  no  yes no  no  no  no  no  no  no  no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  no  yes no  no  no  no \n [49] yes no  no  no  no  no  no  yes no  no  no  yes no  no  yes no  yes no  no  no  no  no  no  no  no  yes no  no  no  no  no  no  yes no  yes no  no  no  no  no  no  no  no  no  no  yes yes yes\n [97] no  yes no  no  no  yes yes no  yes yes no  yes no  yes yes no  yes no  yes no  yes no  no  no  yes no  no  yes no  yes no  yes no  yes no  yes no  no  yes yes no  no  no  no  yes no  no  no \n[145] yes no  no  no  no  no  no  yes no  no  no  no  no  no  no  no  yes no  yes yes no  yes yes no  no  no  no  no  yes yes yes no  yes no  no  no  no  no  no  yes no  no  no  yes no  yes no  no \n[193] no  no  no  no  no  yes no  no  no  no  no  no  no  no  no  no  yes no  no  yes yes no  no  no  yes yes no  no  no  no  no  yes yes yes no  no  no  no  no  no  no  no  yes no  no  yes no  no \n[241] no  yes no  yes no  yes no  yes no  no  yes no  no  no  no  yes yes yes no  yes no  yes yes no  no  no \nLevels: no yes\n\n\n\n\n13.7.1 ConfusionMatrix\n\nCM   &lt;- caret::confusionMatrix(test.xgb.class, titanic.ted.Imp$Survived, \n                               positive = \"yes\")       # confusionMatrix(예측 class, 실제 class, positive = \"관심 class\")\nCM\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  153  34\n       yes  11  68\n                                          \n               Accuracy : 0.8308          \n                 95% CI : (0.7803, 0.8738)\n    No Information Rate : 0.6165          \n    P-Value [Acc &gt; NIR] : 2.211e-14       \n                                          \n                  Kappa : 0.6263          \n                                          \n Mcnemar's Test P-Value : 0.00104         \n                                          \n            Sensitivity : 0.6667          \n            Specificity : 0.9329          \n         Pos Pred Value : 0.8608          \n         Neg Pred Value : 0.8182          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2556          \n   Detection Prevalence : 0.2970          \n      Balanced Accuracy : 0.7998          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\n\n\n\n13.7.2 ROC 곡선\n\n# 예측 확률 생성\ntest.xgb.prob &lt;- predict(xgb.tune.fit, \n                         newdata = titanic.ted.Imp[,-1], # Test Dataset including Only 예측 변수  \n                         type = \"prob\")                  # 예측 확률 생성     \n\ntest.xgb.prob %&gt;%\n  as_tibble\n\n# A tibble: 266 × 2\n       no    yes\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.0600 0.940 \n 2 0.757  0.243 \n 3 0.960  0.0397\n 4 0.172  0.828 \n 5 0.642  0.358 \n 6 0.860  0.140 \n 7 0.453  0.547 \n 8 0.978  0.0223\n 9 0.957  0.0431\n10 0.0349 0.965 \n# ℹ 256 more rows\n\ntest.xgb.prob &lt;- test.xgb.prob[,2]                       # \"Survived = yes\"에 대한 예측 확률\n\nac  &lt;- titanic.ted.Imp$Survived                          # Test Dataset의 실제 class \npp  &lt;- as.numeric(test.xgb.prob)                         # 예측 확률을 수치형으로 변환\n\n\n13.7.2.1 Package “pROC”\n\npacman::p_load(\"pROC\")\n\nxgb.roc  &lt;- roc(ac, pp, plot = T, col = \"gray\")        # roc(실제 class, 예측 확률)\nauc      &lt;- round(auc(xgb.roc), 3)\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\nCaution! Package \"pROC\"를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.\n\n# 함수 plot.roc() 이용\nplot.roc(xgb.roc,   \n         col=\"gray\",                                   # Line Color\n         print.auc = TRUE,                             # AUC 출력 여부\n         print.auc.col = \"red\",                        # AUC 글씨 색깔\n         print.thres = TRUE,                           # Cutoff Value 출력 여부\n         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양\n         print.thres.col = \"red\",                      # Cutoff Value를 표시하는 도형의 색깔\n         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부\n         auc.polygon.col = \"gray90\")                   # 곡선 아래 면적의 색깔\n\n\n\n\n\n\n\n\n\n# 함수 ggroc() 이용\nggroc(xgb.roc) +\nannotate(geom = \"text\", x = 0.9, y = 1.0,\nlabel = paste(\"AUC = \", auc),\nsize = 5,\ncolor=\"red\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n13.7.2.2 Package “Epi”\n\npacman::p_load(\"Epi\")       \n# install_version(\"etm\", version = \"1.1\", repos = \"http://cran.us.r-project.org\")\n\nROC(pp, ac, plot = \"ROC\")                              # ROC(예측 확률, 실제 class)  \n\n\n\n\n\n\n\n\n\n\n13.7.2.3 Package “ROCR”\n\npacman::p_load(\"ROCR\")\n\nxgb.pred &lt;- prediction(pp, ac)                         # prediction(예측 확률, 실제 class) \n\nxgb.perf &lt;- performance(xgb.pred, \"tpr\", \"fpr\")        # performance(, \"민감도\", \"1-특이도\")                      \nplot(xgb.perf, col = \"gray\")                           # ROC Curve\n\nperf.auc   &lt;- performance(xgb.pred, \"auc\")             # AUC\nauc        &lt;- attributes(perf.auc)$y.values\nlegend(\"bottomright\", legend = auc, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n13.7.3 향상 차트\n\n13.7.3.1 Package “ROCR”\n\nxgb.perf &lt;- performance(xgb.pred, \"lift\", \"rpp\")       # Lift Chart                      \nplot(xgb.perf, main = \"lift curve\",\n     colorize = T,                                     # Coloring according to cutoff \n     lwd = 2)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  }
]